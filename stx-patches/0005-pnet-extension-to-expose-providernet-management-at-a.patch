From 021ae1ac80ca7cbde9903c83b540ca775b05a94f Mon Sep 17 00:00:00 2001
From: Allain Legacy <allain.legacy@windriver.com>
Date: Thu, 26 May 2016 18:59:17 -0400
Subject: [PATCH 005/155] pnet: extension to expose providernet management at
 api

This extension exposes the management of provider networks to the api
user.  Previously, provider networks were nothing more than an entry in
a config file that was statically defined.  Now the user can management
then as they would tenant networks.

Conflicts:
	etc/policy.json
	neutron/common/constants.py
	neutron/common/exceptions.py
	neutron/db/l3_db.py
	neutron/db/migration/alembic_migrations/versions/HEAD
	neutron/db/migration/models/frozen.py
	neutron/db/migration/models/head.py
	neutron/plugins/ml2/drivers/type_local.py
	neutron/plugins/ml2/managers.py
	neutron/plugins/ml2/plugin.py
	neutron/tests/etc/policy.json
	setup.cfg

Conflicts:
	neutron/db/hosts_db.py

CGTS-7284: db: only start new session if non exists

There are sections of code that instantiate a new DB session regardless if one
already exists for the current session.  This seems harmless because can
actually result in long delays in servicing API requests or even outright DB
exceptions due to timeouts acquiring a new connection.

The reason for this is that there are a finite set of DB pool connections.
Once a worker process goes over the limit then any greenthread requesting a new
session will block.  Under normal circumstances we are always under the pool
limit so a greenthread requesting a second session does not wait and no issue
is noticed.

Under heavy load we see situations were individual worker processes are above
the DB pool limit.  Again, this is not necessarily a problem but if all
previous threads were also waiting for a second session then a sort-of deadlock
condition happens where none of them make any progress until one of them
experiences a DB PoolLimit timeout exception.

Given then high number of workers that exist you would think that it would be
unlikely that we'd have that many DB connections open, but because the workload
is not at all balanced across the worker processes it is possible that most of
the workers are idle while only a small subset have completely exhausted their
connection pool.

To avoid this scenario this commit changes callers of "db_api.get_session()" to
call a new API named "db_api.get_current_session()" which will default to
re-using the existing DB session handle rather than starting a new one.  This
will have the benefit of speeding up the operation that wanted to do its work
on a new session and also avoid the long API requests that we have been seeing.

The issue is easily reproduced by creating 32 ports (1 at a time), and then
deleting them all at the same time (in parallel).  On a system with 2 API
workers, a DB PoolLimit of 10 and an overflow of 1, a pool timeout of 30 this
results in some of the delete operations getting a 500 result while those after
it experience > 30 sec response time.

CGTS-7752: PCI PT provider network shows down and logs error

This update is to suppress the provider network alarm if the provider network
does not have any data interfaces attached to it.

Conflicts:
	neutron/common/constants.py
	neutron/db/api.py
	neutron/db/l3_hamode_db.py
	neutron/db/segments_db.py
	neutron/plugins/ml2/driver_api.py
	neutron/plugins/ml2/drivers/helpers.py
	neutron/plugins/ml2/drivers/type_flat.py
	neutron/plugins/ml2/drivers/type_local.py
	neutron/plugins/ml2/drivers/type_tunnel.py
	neutron/plugins/ml2/drivers/type_vlan.py
	neutron/plugins/ml2/managers.py
	neutron/plugins/ml2/plugin.py
	neutron/plugins/ml2/rpc.py
	neutron/scheduler/dhcp_host_agent_scheduler.py
	neutron/scheduler/l3_host_agent_scheduler.py
---
 etc/policy.json                                    |   13 +
 neutron/agent/rpc.py                               |   11 +
 neutron/common/constants.py                        |   55 +-
 neutron/common/exceptions.py                       |   31 +
 neutron/db/api.py                                  |   20 +
 neutron/db/hosts_db.py                             |   88 ++
 neutron/db/l3_db.py                                |   11 +
 .../expand/3c52bf0d97f3_vxlan_provider_networks.py |   55 +
 .../wrs_kilo_shipped/expand/wrs_kilo_shipped.py    |    2 +-
 .../alembic_migrations/vswitch_init_ops.py         |   33 +
 neutron/db/migration/models/head.py                |    8 +
 neutron/db/providernet_db.py                       |  698 ++++++++++++
 neutron/db/segments_db.py                          |   17 +
 neutron/drivers/host.py                            |    4 +-
 neutron/extensions/wrs_provider.py                 |  472 +++++++++
 neutron/plugins/common/constants.py                |    1 +
 neutron/plugins/ml2/driver_api.py                  |   33 +-
 neutron/plugins/ml2/drivers/helpers.py             |   58 +-
 neutron/plugins/ml2/drivers/type_flat.py           |   18 +-
 neutron/plugins/ml2/drivers/type_gre.py            |    7 +
 neutron/plugins/ml2/drivers/type_local.py          |   17 +-
 neutron/plugins/ml2/drivers/type_tunnel.py         |   30 +-
 neutron/plugins/ml2/drivers/type_vlan.py           |   22 +-
 neutron/plugins/ml2/drivers/type_vxlan.py          |    7 +
 neutron/plugins/ml2/managers.py                    |   64 +-
 neutron/plugins/ml2/plugin.py                      |  393 ++++++-
 neutron/plugins/ml2/rpc.py                         |   21 +
 neutron/plugins/wrs/drivers/type_generic.py        |  358 +++++++
 neutron/plugins/wrs/drivers/type_managed_flat.py   |   70 ++
 neutron/plugins/wrs/drivers/type_managed_vlan.py   |   71 ++
 neutron/plugins/wrs/drivers/type_managed_vxlan.py  |   92 ++
 neutron/scheduler/dhcp_agent_scheduler.py          |    5 +-
 neutron/scheduler/dhcp_host_agent_scheduler.py     |   59 +-
 neutron/scheduler/l3_host_agent_scheduler.py       |  158 ++-
 neutron/tests/etc/policy.json                      |   13 +
 neutron/tests/unit/db/test_db_base_plugin_v2.py    |    3 +-
 neutron/tests/unit/plugins/ml2/_test_mech_agent.py |   25 +
 .../unit/plugins/ml2/drivers/base_type_tunnel.py   |    1 +
 neutron/tests/unit/plugins/ml2/test_plugin.py      |   11 +-
 .../plugins/wrs/drivers/test_type_managed_flat.py  |  125 +++
 .../plugins/wrs/drivers/test_type_managed_vlan.py  |  215 ++++
 .../plugins/wrs/drivers/test_type_managed_vxlan.py |   68 ++
 .../tests/unit/plugins/wrs/test_agent_scheduler.py |  563 ++++++++++
 .../tests/unit/plugins/wrs/test_extension_host.py  |  193 ++++
 .../unit/plugins/wrs/test_extension_interface.py   |  498 +++++++++
 .../unit/plugins/wrs/test_extension_manager.py     |   43 +
 .../tests/unit/plugins/wrs/test_extension_pnet.py  | 1111 ++++++++++++++++++++
 setup.cfg                                          |    3 +
 48 files changed, 5785 insertions(+), 89 deletions(-)
 create mode 100644 neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/3c52bf0d97f3_vxlan_provider_networks.py
 create mode 100644 neutron/db/providernet_db.py
 create mode 100644 neutron/extensions/wrs_provider.py
 create mode 100644 neutron/plugins/wrs/drivers/type_generic.py
 create mode 100644 neutron/plugins/wrs/drivers/type_managed_flat.py
 create mode 100644 neutron/plugins/wrs/drivers/type_managed_vlan.py
 create mode 100644 neutron/plugins/wrs/drivers/type_managed_vxlan.py
 create mode 100644 neutron/tests/unit/plugins/wrs/drivers/test_type_managed_flat.py
 create mode 100644 neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vlan.py
 create mode 100644 neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vxlan.py
 create mode 100644 neutron/tests/unit/plugins/wrs/test_agent_scheduler.py
 create mode 100644 neutron/tests/unit/plugins/wrs/test_extension_host.py
 create mode 100644 neutron/tests/unit/plugins/wrs/test_extension_interface.py
 create mode 100644 neutron/tests/unit/plugins/wrs/test_extension_manager.py
 create mode 100644 neutron/tests/unit/plugins/wrs/test_extension_pnet.py

diff --git a/etc/policy.json b/etc/policy.json
index 792c43a..744ca82 100644
--- a/etc/policy.json
+++ b/etc/policy.json
@@ -69,6 +69,19 @@
     "update_segment": "rule:admin_only",
     "delete_segment": "rule:admin_only",
 
+    "get_providernet": "rule:admin_only",
+    "get_providernets": "rule:admin_only",
+    "create_providernet": "rule:admin_only",
+    "update_providernet": "rule:admin_only",
+    "delete_providernet": "rule:admin_only",
+    "get_providernet_range": "rule:admin_only",
+    "get_providernet_ranges": "rule:admin_only",
+    "create_providernet_range": "rule:admin_only",
+    "update_providernet_range": "rule:admin_only",
+    "delete_providernet_range": "rule:admin_only",
+    "get_providernet_types": "rule:admin_only",
+    "get_providernet-bindings": "rule:admin_only",
+
     "network_device": "field:port:device_owner=~^network:",
     "create_port": "",
     "create_port:device_owner": "not rule:network_device or rule:context_is_advsvc or rule:admin_or_network_owner",
diff --git a/neutron/agent/rpc.py b/neutron/agent/rpc.py
index c1cec85..31f17a6 100644
--- a/neutron/agent/rpc.py
+++ b/neutron/agent/rpc.py
@@ -128,6 +128,17 @@ class PluginApi(object):
                           host=host,
                           agent_id=agent_id)
 
+    def get_provider_details(self, context, host, agent_id,
+                             network_type, physical_network, segmentation_id):
+        cctxt = self.client.prepare()
+        return cctxt.call(context,
+                          'get_provider_details',
+                          host=host,
+                          agent_id=agent_id,
+                          network_type=network_type,
+                          physical_network=physical_network,
+                          segmentation_id=segmentation_id)
+
     def get_devices_details_list(self, context, devices, agent_id, host=None):
         cctxt = self.client.prepare(version='1.3')
         return cctxt.call(context, 'get_devices_details_list',
diff --git a/neutron/common/constants.py b/neutron/common/constants.py
index cab0f70..c89c9a1 100644
--- a/neutron/common/constants.py
+++ b/neutron/common/constants.py
@@ -13,7 +13,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-# Copyright (c) 2013-2014 Wind River Systems, Inc.
+# Copyright (c) 2013-2016 Wind River Systems, Inc.
 #
 # The right to copy, distribute, modify, or otherwise make use
 # of this software may be licensed only pursuant to the terms
@@ -196,3 +196,56 @@ HOST_DOWN = 'down'
 # Possible types of values (e.g. in QoS rule types)
 VALUES_TYPE_CHOICES = "choices"
 VALUES_TYPE_RANGE = "range"
+
+# Provider Networks
+MIN_VLAN_TAG = 1
+MAX_VLAN_TAG = 4094
+
+MAX_VXLAN_VNI = 2 ** 24 - 1
+MIN_VXLAN_VNI = 1
+
+PROVIDERNET_FLAT = 'flat'
+PROVIDERNET_VLAN = 'vlan'
+PROVIDERNET_VXLAN = 'vxlan'
+PROVIDERNET_GRE = 'gre'
+
+PROVIDERNET_ACTIVE = 'ACTIVE'
+PROVIDERNET_DOWN = 'DOWN'
+PROVIDERNET_ERROR = 'ERROR'
+
+DEFAULT_MTU = 1500
+MINIMUM_MTU = 576
+MAXIMUM_MTU = 9216
+VALID_MTU_RANGE = [MINIMUM_MTU, MAXIMUM_MTU]
+
+MINIMUM_TTL = 1
+MAXIMUM_TTL = 255
+VALID_TTL_RANGE = [MINIMUM_TTL, MAXIMUM_TTL]
+
+DEFAULT_VXLAN_GROUP = '239.0.0.1'
+DEFAULT_VXLAN_TTL = 1
+DEFAULT_VXLAN_UDP_PORT = 4789
+VALID_VXLAN_UDP_PORTS = [4789, 8472]
+
+# Represents the number of bytes added to a tenant packet when it is carried
+# by a VXLAN based provider network.  We start by assuming a tenant network
+# with an MTU of 1500 bytes.  This means that at the host vswitch the
+# ethernet frame will be 1514 bytes (+4 if VLAN tagged) not including the FCS
+# trailer.   To get this packet on to the provider network it must be
+# encapsulated as-is with a {IPv4|IPv6}+UDP+VXLAN headers.  The ETH+VLAN
+# headers are not included because they themselves are not included in the
+# provider network MTU (i.e., the VXLAN packet must fit within the ethernet
+# payload of the provider interface).
+# Therefore the maximum overhead, assuming a VLAN tagged provider network, is:
+#
+#  IPv4 = 20 + 8 + 8 = 36
+#  IPv6 = 40 + 8 + 8 = 56
+#
+# This brings the maximum tenant packet size to:
+#  IPv4 = 36 + 1518 = 1554
+#  IPv6 = 56 + 1518 = 1574
+#
+# Therefore to support an tenant MTU of 1500 the underlying physical
+# interface must support an MTU of 1574 bytes.
+#
+VXLAN_MTU_OVERHEAD = 74
diff --git a/neutron/common/exceptions.py b/neutron/common/exceptions.py
index 9b8e5f3..cb05759 100644
--- a/neutron/common/exceptions.py
+++ b/neutron/common/exceptions.py
@@ -12,6 +12,14 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
 
 from neutron_lib import exceptions as e
 
@@ -103,6 +111,12 @@ class DNSNameServersExhausted(e.BadRequest):
                 "The number of DNS nameservers exceeds the limit %(quota)s.")
 
 
+class SegmentationIdInUse(e.InUse):
+    message = _("Unable to create the network. "
+                "The segment %(id)s on physical network "
+                "%(physical_network)s is in use.")
+
+
 class FlatNetworkInUse(e.InUse):
     message = _("Unable to create the flat network. "
                 "Physical network %(physical_network)s is in use.")
@@ -119,6 +133,23 @@ class NoNetworkFoundInMaximumAllowedAttempts(e.ServiceUnavailable):
                 "No available network found in maximum allowed attempts.")
 
 
+class NetworkNotPredefined(e.BadRequest):
+    message = _("Unable to create the network. "
+                "The %(type)s %(id)s on physical network "
+                "%(physical_network)s is not predefined as "
+                "a provider network.")
+
+    def __init__(self, **kwargs):
+        # Identify the id nature, is it a vlan or vxlan
+        if 'vxlan_vni' in kwargs:
+            kwargs['id'] = kwargs['vxlan_vni']
+            kwargs['type'] = 'vxlan_vni'
+        else:
+            kwargs['id'] = kwargs['vlan_id']
+            kwargs['type'] = 'vlan_id'
+        super(NetworkNotPredefined, self).__init__(**kwargs)
+
+
 class MalformedRequestBody(e.BadRequest):
     message = _("Malformed request body: %(reason)s.")
 
diff --git a/neutron/db/api.py b/neutron/db/api.py
index c69fe45..6967ed5 100644
--- a/neutron/db/api.py
+++ b/neutron/db/api.py
@@ -21,6 +21,7 @@ from debtcollector import removals
 from neutron_lib.db import api
 from neutron_lib import exceptions
 from oslo_config import cfg
+from oslo_context import context as common_context
 from oslo_db import api as oslo_db_api
 from oslo_db import exception as db_exc
 from oslo_log import log as logging
@@ -209,6 +210,25 @@ def get_writer_session():
     return context_manager.writer.get_sessionmaker()()
 
 
+def get_current_session(autocommit=True, expire_on_commit=False,
+                        use_slave=False, **kwargs):
+    """Helper method to acquire a session if one isn't already present on the
+    current context.  We do not want to get another session if one already
+    exists because if we exceed the connection pool size then it is possible
+    that we deadlock while waiting for a connection to become available (if all
+    other threads are doing the same thing and also acquiring a second
+    session).
+    """
+    kwargs.update({'autocommit': autocommit,
+                   'expire_on_commit': expire_on_commit,
+                   'use_slave': use_slave})
+    context = common_context.get_current()
+    if context and hasattr(context, 'session'):
+        if context.session:
+            return context.session
+    return get_session(**kwargs)
+
+
 @contextlib.contextmanager
 def autonested_transaction(sess):
     """This is a convenience method to not bother with 'nested' parameter."""
diff --git a/neutron/db/hosts_db.py b/neutron/db/hosts_db.py
index 8c8d201..533a2ff 100644
--- a/neutron/db/hosts_db.py
+++ b/neutron/db/hosts_db.py
@@ -32,11 +32,13 @@ from oslo_utils import timeutils
 from oslo_utils import uuidutils
 import sqlalchemy as sa
 from sqlalchemy.orm import exc
+from sqlalchemy import and_, or_, func
 
 from neutron.common import constants
 from neutron.common import topics
 from neutron.common import utils
 from neutron.db import api as db_api
+from neutron.db import providernet_db
 from neutron.drivers import fm
 from neutron.drivers import host
 from neutron.extensions import host as ext_host
@@ -46,8 +48,13 @@ from neutron_lib.plugins import directory
 LOG = logging.getLogger(__name__)
 
 DATA_NETWORK = "data"
+
+# data interface types to be considered when scheduling resources
+DATA_NETWORK_TYPES = [DATA_NETWORK]
+
 PCI_PASSTHROUGH = "pci-passthrough"
 PCI_SRIOV_PASSTHROUGH = "pci-sriov"
+PCI_NETWORK_TYPES = [PCI_PASSTHROUGH, PCI_SRIOV_PASSTHROUGH]
 
 
 class Host(model_base.BASEV2):
@@ -204,6 +211,85 @@ class HostDbMixin(ext_host.HostPluginBase):
                          interface_uuid)))
         return [binding.providernet_id for binding in query.all()]
 
+    def _get_providernet_networktype(self, context, networktypes):
+        """
+        Returns the list of inactive provider networks that are attached
+        to the interface of the specified network types on the
+        given host.
+        """
+        providernets = (
+            context.session.query(providernet_db.ProviderNet)
+            .join(HostInterfaceProviderNetBinding,
+                  HostInterfaceProviderNetBinding.providernet_id ==
+                  providernet_db.ProviderNet.id)
+            .join(HostInterface,
+                  and_((HostInterface.id ==
+                        HostInterfaceProviderNetBinding.interface_id),
+                       (HostInterface.network_type.
+                        in_(networktypes))))
+            .join(Host,
+                  Host.id == HostInterface.host_id)
+            .filter(Host.availability == constants.HOST_UP)
+            .filter(providernet_db.ProviderNet.status !=
+                    constants.PROVIDERNET_ACTIVE)
+            .group_by(providernet_db.ProviderNet.id)
+            .all())
+        return providernets
+
+    def _update_providernet_states(self, context):
+        with context.session.begin(subtransactions=True):
+            # Find all providernets that are currently marked as DOWN and are
+            # bound to a host that is UP.
+            providernets = self._get_providernet_networktype(
+                context, DATA_NETWORK_TYPES)
+            data = {'status': constants.PROVIDERNET_ACTIVE}
+            LOG.debug("updating {} providernets to ACTIVE".format(
+                    len(providernets)))
+            for providernet in providernets:
+                providernet.update(data)
+                self._clear_providernet_fault(providernet)
+
+            # Clear the provider network alarm without changing the status
+            # only for 'pci-passthrough' and 'pci-sriov' networktypes.
+            # Find all providernets that are currently marked as DOWN and are
+            # bound to a host that is UP.
+            providernets = self._get_providernet_networktype(
+                context, PCI_NETWORK_TYPES)
+            for providernet in providernets:
+                LOG.info(("clearing alarm for providernet {}").format(
+                    providernet))
+                self._clear_providernet_fault(providernet)
+
+            # Update providernet states for providernets without at least one
+            # binding to a host that is UP.
+            providernets = (
+                context.session.query(providernet_db.ProviderNet)
+                .outerjoin(HostInterfaceProviderNetBinding,
+                      HostInterfaceProviderNetBinding.providernet_id ==
+                      providernet_db.ProviderNet.id)
+                .outerjoin(HostInterface,
+                           and_((HostInterface.network_type == DATA_NETWORK),
+                                (HostInterface.id ==
+                                 HostInterfaceProviderNetBinding.
+                                 interface_id)))
+                .outerjoin(Host,
+                           and_(Host.id == HostInterface.host_id,
+                                Host.availability == constants.HOST_UP))
+                .filter(providernet_db.ProviderNet.status ==
+                        constants.PROVIDERNET_ACTIVE)
+                .group_by(providernet_db.ProviderNet.id)
+                .having(or_(func.count(Host.id) == 0,
+                            func.count(HostInterfaceProviderNetBinding.
+                                       providernet_id)
+                            == 0))
+                .all())
+            data = {'status': constants.PROVIDERNET_DOWN}
+            LOG.debug("updating {} providernets to DOWN".format(
+                    len(providernets)))
+            for providernet in providernets:
+                providernet.update(data)
+                self._report_providernet_fault(providernet)
+
     def _update_interface_providernet_bindings(self, context, interface_uuid,
                                                current_bindings):
         previous_bindings = self._get_providernet_bindings(
@@ -289,6 +375,7 @@ class HostDbMixin(ext_host.HostPluginBase):
         host_data = host['host']
         id = host_data.get('id')
         host = self._create_or_update_host(context, id, host)
+        self._update_providernet_states(context)
         return host
 
     def _fix_host_data(self, context, id, host):
@@ -305,6 +392,7 @@ class HostDbMixin(ext_host.HostPluginBase):
     @db_api.retry_if_session_inactive()
     def update_host(self, context, id, host):
         host = self._create_or_update_host(context, id, host)
+        self._update_providernet_states(context)
         if host.get('availability') == constants.HOST_UP:
             host_state_up = True
         else:
diff --git a/neutron/db/l3_db.py b/neutron/db/l3_db.py
index 69bd689..081d012 100644
--- a/neutron/db/l3_db.py
+++ b/neutron/db/l3_db.py
@@ -1767,6 +1767,17 @@ class L3_NAT_dbonly_mixin(l3.RouterPluginBase,
         self._process_interfaces(routers_dict, interfaces)
         return list(routers_dict.values())
 
+    def get_sync_networks(self, context, router_ids,
+                          device_owner=DEVICE_OWNER_ROUTER_INTF):
+        """Query networks provided by the list of router_ids."""
+        if not router_ids:
+            return []
+        filters = {'device_id': router_ids}
+        if device_owner:
+            filters.update({'device_owner': [device_owner]})
+        interfaces = self._core_plugin.get_ports(context.elevated(), filters)
+        return [interface['network_id'] for interface in interfaces]
+
 
 @registry.has_registry_receivers
 class L3RpcNotifierMixin(object):
diff --git a/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/3c52bf0d97f3_vxlan_provider_networks.py b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/3c52bf0d97f3_vxlan_provider_networks.py
new file mode 100644
index 0000000..ce723e6
--- /dev/null
+++ b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/3c52bf0d97f3_vxlan_provider_networks.py
@@ -0,0 +1,55 @@
+# Copyright 2015 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+
+"""vxlan provider networks
+
+Revision ID: 3c52bf0d97f3
+Revises: 428d71c78e01
+Create Date: 2015-02-11 17:14:05.190769
+
+"""
+
+# revision identifiers, used by Alembic.
+revision = '3c52bf0d97f3'
+down_revision = '10b1502ffd1c'
+
+from alembic import op
+import sqlalchemy as sa
+
+
+def upgrade():
+    op.create_table(
+        'wrs_vxlan_allocations',
+        sa.Column('physical_network',
+                  sa.String(length=64), nullable=False),
+        sa.Column('vxlan_vni',
+                  sa.Integer(), autoincrement=False, nullable=False),
+        sa.Column('allocated',
+                  sa.Boolean(), server_default='false', nullable=False),
+        sa.PrimaryKeyConstraint('physical_network', 'vxlan_vni')
+    )
+    op.create_table(
+        'providernet_range_vxlans',
+        sa.Column('id', sa.String(length=36), nullable=False),
+        sa.Column('group', sa.String(length=64), nullable=False),
+        sa.Column('port', sa.Integer(), nullable=False),
+        sa.Column('ttl', sa.Integer(), nullable=False),
+        sa.Column('providernet_range_id',
+                  sa.String(length=36), nullable=True),
+        sa.ForeignKeyConstraint(['providernet_range_id'],
+                                ['providernet_ranges.id'], ),
+        sa.PrimaryKeyConstraint('id')
+    )
+    op.drop_table('providernet_vxlans')
diff --git a/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py
index 6a65ea1..c0e801c 100644
--- a/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py
+++ b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py
@@ -29,7 +29,7 @@ Create Date: 2016-05-25 00:00:01.000000
 
 # revision identifiers, used by Alembic.
 revision = 'wrs_kilo_shipped'
-down_revision = '10b1502ffd1c'
+down_revision = '3c52bf0d97f3'
 
 
 def upgrade():
diff --git a/neutron/db/migration/alembic_migrations/vswitch_init_ops.py b/neutron/db/migration/alembic_migrations/vswitch_init_ops.py
index 18a7f40..dfbe3bf 100644
--- a/neutron/db/migration/alembic_migrations/vswitch_init_ops.py
+++ b/neutron/db/migration/alembic_migrations/vswitch_init_ops.py
@@ -49,6 +49,39 @@ def upgrade():
         sa.UniqueConstraint('name'))
 
     op.create_table(
+        'providernets',
+        sa.Column('id', sa.String(length=36), nullable=False),
+        sa.Column('name', sa.String(length=255), nullable=False),
+        sa.Column('description', sa.String(length=255), nullable=True),
+        sa.Column('mtu', sa.Integer(), nullable=False),
+        sa.Column('status', sa.String(length=16), nullable=True),
+        sa.Column('type', providernet_type, nullable=False),
+        sa.PrimaryKeyConstraint('id'),
+        sa.UniqueConstraint('name'))
+
+    op.create_table(
+        'providernet_vxlans',
+        sa.Column('id', sa.String(length=36), nullable=False),
+        sa.Column('first_ip', sa.String(length=64), nullable=False),
+        sa.Column('last_ip', sa.String(length=64), nullable=False),
+        sa.Column('providernet_id', sa.String(length=36), nullable=True),
+        sa.ForeignKeyConstraint(['providernet_id'], ['providernets.id'], ),
+        sa.PrimaryKeyConstraint('id'))
+
+    op.create_table(
+        'providernet_ranges',
+        sa.Column('tenant_id', sa.String(length=255), nullable=True),
+        sa.Column('id', sa.String(length=36), nullable=False),
+        sa.Column('name', sa.String(length=255), nullable=True),
+        sa.Column('description', sa.String(length=255), nullable=True),
+        sa.Column('shared', sa.Boolean(), nullable=False),
+        sa.Column('minimum', sa.Integer(), nullable=True),
+        sa.Column('maximum', sa.Integer(), nullable=True),
+        sa.Column('providernet_id', sa.String(length=36), nullable=True),
+        sa.ForeignKeyConstraint(['providernet_id'], ['providernets.id'], ),
+        sa.PrimaryKeyConstraint('id'))
+
+    op.create_table(
         'hostprovidernetbindings',
         sa.Column('providernet_id', sa.String(length=36), nullable=False),
         sa.Column('host_id', sa.String(length=36), nullable=False),
diff --git a/neutron/db/migration/models/head.py b/neutron/db/migration/models/head.py
index 9e69ba2..842087f 100644
--- a/neutron/db/migration/models/head.py
+++ b/neutron/db/migration/models/head.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 """
 The module provides all database models at current HEAD.
@@ -39,6 +46,7 @@ from neutron.db.quota import models as quota_models  # noqa
 from neutron.db import rbac_db_models  # noqa
 from neutron.ipam.drivers.neutrondb_ipam import db_models  # noqa
 from neutron.plugins.ml2 import models as ml2_models  # noqa
+from neutron.plugins.wrs.drivers import type_managed_vxlan  # noqa
 from neutron.services.auto_allocate import models as aa_models  # noqa
 from neutron.services.trunk import models as trunk_models  # noqa
 
diff --git a/neutron/db/providernet_db.py b/neutron/db/providernet_db.py
new file mode 100644
index 0000000..72b5375
--- /dev/null
+++ b/neutron/db/providernet_db.py
@@ -0,0 +1,698 @@
+# Copyright (c) 2014 OpenStack Foundation.
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import six
+
+from neutron_lib.api import validators
+from neutron_lib.db import model_base
+from oslo_log import log as logging
+from oslo_utils import uuidutils
+import sqlalchemy as sa
+from sqlalchemy import and_
+from sqlalchemy import orm
+from sqlalchemy.orm import contains_eager
+from sqlalchemy.orm import exc
+from sqlalchemy.sql.expression import literal_column
+
+from neutron._i18n import _
+from neutron.common import constants
+from neutron.db import api as db_api
+from neutron.db.models import segment as segments_model
+from neutron.db import models_v2
+from neutron.db import segments_db
+from neutron.drivers import fm
+from neutron.extensions import wrs_provider as ext_providernet
+
+
+LOG = logging.getLogger(__name__)
+
+
+class ProviderNetRangeVxLan(model_base.BASEV2, model_base.HasId):
+    """Represents VXLAN specific data for a provider network."""
+    __tablename__ = 'providernet_range_vxlans'
+
+    # IP address of the multicast group
+    group = sa.Column(sa.String(64), nullable=False)
+
+    # Destination DP port value for all instances
+    port = sa.Column(sa.Integer, default=constants.DEFAULT_VXLAN_UDP_PORT,
+                     nullable=False)
+    # Time-to-live value for all instances
+    ttl = sa.Column(sa.Integer, default=constants.DEFAULT_VXLAN_TTL,
+                    nullable=False)
+
+    # 1-to-1 relationship back to provider network range table
+    providernet_range_id = sa.Column(sa.String(36),
+                                     sa.ForeignKey('providernet_ranges.id'))
+
+    def __init__(self, group, port, ttl, providernet_range_id=None):
+        self.group = group
+        self.port = port
+        self.ttl = ttl
+        self.providernet_range_id = providernet_range_id
+
+    def __repr__(self):
+        return "<ProviderNetRangeVxLan(%s,%s,%s,%s)>" % (
+            self.group, self.port, self.ttl, self.providernet_range_id)
+
+
+class ProviderNetRange(model_base.BASEV2, model_base.HasId,
+                       model_base.HasProject):
+    """Represents provider network segmentation id range data."""
+    __tablename__ = 'providernet_ranges'
+
+    # user-defined provider network segmentation id range name
+    name = sa.Column(sa.String(255), nullable=True)
+
+    # user-defined provider network segmentation id range description
+    description = sa.Column(sa.String(255), nullable=True)
+
+    # defines whether multiple tenants can use this provider network range
+    shared = sa.Column(sa.Boolean, default=True, nullable=False)
+
+    # minimum segmentation id value
+    minimum = sa.Column(sa.Integer, default=0)
+
+    # maximum segmentation id value
+    maximum = sa.Column(sa.Integer, default=0)
+
+    # n-to-1 relationship back to provider network table
+    providernet_id = sa.Column(sa.String(36), sa.ForeignKey('providernets.id'))
+
+    # 1-to-1 relationship to provider network VLAN per-range data table
+    vxlan = orm.relationship(
+        ProviderNetRangeVxLan,
+        uselist=False,
+        backref="providernet_range",
+        cascade="all, delete-orphan")
+
+    def __init__(self, id, name, description, shared, minimum, maximum,
+                 providernet_id, tenant_id=None):
+        self.id = id
+        self.name = name
+        self.description = description
+        self.minimum = minimum
+        self.maximum = maximum
+        self.providernet_id = providernet_id
+        self.shared = shared
+        if not self.shared:
+            self.tenant_id = tenant_id
+        else:
+            self.tenant_id = None
+
+    def __repr__(self):
+        return "<ProviderNetRange(%s,%s,%s,%s,%s,%s - %s,%s)>" % (
+            self.id, self.name, self.description, str(self.shared),
+            self.tenant_id, self.providernet_id, self.minimum, self.maximum)
+
+
+class ProviderNet(model_base.BASEV2, model_base.HasId):
+    """Represents provider network configuration data."""
+    __tablename__ = 'providernets'
+
+    # user-defined provider network name
+    name = sa.Column(sa.String(255), unique=True, nullable=False)
+
+    # user-defined provider network description
+    description = sa.Column(sa.String(255), nullable=True)
+
+    # defines the maximum transmit unit on this provider network
+    mtu = sa.Column(sa.Integer, default=constants.DEFAULT_MTU, nullable=False)
+
+    # defines the status of the providernet (i.e., whether it is connected to
+    # any nodes)
+    status = sa.Column(sa.String(16))
+
+    # provider network type
+    type = sa.Column(sa.Enum(constants.PROVIDERNET_FLAT,
+                             constants.PROVIDERNET_VLAN,
+                             constants.PROVIDERNET_VXLAN,
+                             constants.PROVIDERNET_GRE,
+                             name='providernet_types'),
+                     default=constants.PROVIDERNET_FLAT, nullable=False)
+
+    # 1-to-n relationship to provider network segmentation id range table
+    ranges = orm.relationship(
+        ProviderNetRange,
+        backref="providernet",
+        cascade="all, delete-orphan")
+
+    def __init__(self, id, name, description, status, type,
+                 mtu=constants.DEFAULT_MTU):
+        self.id = id
+        self.name = name
+        self.description = description
+        self.status = status
+        self.type = type
+        self.mtu = mtu
+
+    def __repr__(self):
+        return "<ProviderNet(%s,%s,%s,%s,%s,%s)>" % (
+            self.id, self.name, self.description,
+            str(self.status), self.type, self.mtu)
+
+
+class ProviderNetDbMixin(ext_providernet.ProviderNetPluginBase):
+    """
+    Mixin class to add the provider network extension to the db_plugin_base_v2.
+    """
+    fm_driver = fm.NoopFmDriver()
+
+    def _get_providernet_by_id(self, context, id):
+        try:
+            query = self._model_query(context, ProviderNet)
+            providernet = query.filter(ProviderNet.id == id).one()
+        except exc.NoResultFound:
+            raise ext_providernet.ProviderNetNotFoundById(id=id)
+        return providernet
+
+    def _get_providernet_by_name(self, context, name):
+        try:
+            query = self._model_query(context, ProviderNet)
+            providernet = query.filter(ProviderNet.name == name).one()
+        except exc.NoResultFound:
+            raise ext_providernet.ProviderNetNotFoundByName(name=name)
+        return providernet
+
+    def get_providernet_by_id(self, context, id):
+        try:
+            return self._make_providernet_dict(
+                self._get_providernet_by_id(context, id))
+        except ext_providernet.ProviderNetNotFoundById:
+            return None
+
+    def get_providernet_by_name(self, context, name):
+        try:
+            return self._make_providernet_dict(
+                self._get_providernet_by_name(context, name))
+        except ext_providernet.ProviderNetNotFoundByName:
+            return None
+
+    def _make_providernet_segment_dict(self, providernet, fields=None):
+        """
+        Return a dictionary of fields relevant for describing a provider
+        segment which is all of the provider network (and range) fields that
+        are needed to determine how to implement the actual segment on a
+        compute node.
+        """
+        res = {'id': providernet['id'],
+               'name': providernet['name'],
+               'description': providernet['description'],
+               'type': providernet['type'],
+               'status': providernet['status'],
+               'mtu': providernet['mtu']}
+        if providernet['type'] == constants.PROVIDERNET_VXLAN:
+            vxlan = providernet['ranges'][0]['vxlan']
+            res.update({'vxlan': {'group': vxlan['group'],
+                                  'port': vxlan['port'],
+                                  'ttl': vxlan['ttl']}})
+        return self._fields(res, fields)
+
+    def _get_flat_providernet_segment_details(self, context, type, name):
+        """
+        Find a providernet by name and type.
+        """
+        with context.session.begin(subtransactions=True):
+            query = self._model_query(context, ProviderNet)
+            query = (query.
+                     filter(ProviderNet.name == name).
+                     filter(ProviderNet.type == type))
+            providernet = query.one()
+            return self._make_providernet_segment_dict(providernet)
+
+    def _get_providernet_segment_details(self, context, type, name, id):
+        """
+        Find a providernet combination of provider+range+[vxlan] based on the
+        segmentation id provided.  This will remove all unrelated ranges.
+        """
+        with context.session.begin(subtransactions=True):
+            query = self._model_query(context, ProviderNet)
+            query = (query.
+                     join(ProviderNetRange,
+                          and_((ProviderNetRange.providernet_id ==
+                                ProviderNet.id),
+                               and_(ProviderNetRange.minimum <= id,
+                                    ProviderNetRange.maximum >= id))).
+                     outerjoin(ProviderNetRangeVxLan).
+                     filter(ProviderNet.name == name).
+                     filter(ProviderNet.type == type))
+            query = query.options(contains_eager(ProviderNet.ranges))
+            providernet = query.one()
+            return self._make_providernet_segment_dict(providernet)
+
+    def get_providernet_segment_details(self, context, type, name, id):
+        """
+        Find a combination of provider+[range+[vxlan]] based on the
+        segmentation id provided.  This will remove all unrelated ranges.
+        """
+        if not id:
+            return self._get_flat_providernet_segment_details(
+                context, type, name)
+        else:
+            return self._get_providernet_segment_details(
+                context, type, name, id)
+
+    def check_providernet_id_allowed(self, context, name, id):
+        try:
+            query = self._model_query(context, ProviderNetRange)
+            query = (query.
+                     filter(ProviderNet.name == name).
+                     filter(ProviderNetRange.providernet_id == ProviderNet.id).
+                     filter(ProviderNetRange.minimum <= id).
+                     filter(ProviderNetRange.maximum >= id))
+            query.one()
+        except exc.NoResultFound:
+            return False
+        return True
+
+    def _make_vxlan_dict(self, vxlan, fields=None):
+        res = {'group': vxlan['group'],
+               'port': vxlan['port'],
+               'ttl': vxlan['ttl']}
+        return self._fields(res, fields)
+
+    def _make_providernet_dict(self, providernet, fields=None):
+        res = {'id': providernet['id'],
+               'name': providernet['name'],
+               'description': providernet['description'],
+               'type': providernet['type'],
+               'status': providernet['status'],
+               'mtu': providernet['mtu']}
+        res['ranges'] = [self._make_providernet_range_dict(r)
+                         for r in providernet['ranges']]
+        # filter out redundant fields
+        for r in res['ranges']:
+            r.pop('providernet_id', None)
+            r.pop('providernet_name', None)
+            r.pop('providernet_type', None)
+        return self._fields(res, fields)
+
+    def _validate_providernet_exists(self, context, name):
+        try:
+            self._get_providernet_by_name(context, name)
+            raise ext_providernet.ProviderNetNameAlreadyExists(name=name)
+        except ext_providernet.ProviderNetNotFoundByName:
+            pass
+
+    def _validate_providernet(self, context, providernet):
+        providernet_data = providernet['providernet']
+        self._validate_providernet_exists(context, providernet_data['name'])
+
+    @db_api.retry_if_session_inactive()
+    def create_providernet(self, context, providernet):
+        providernet_data = providernet['providernet']
+        res = {
+            'id': providernet_data.get('id') or uuidutils.generate_uuid(),
+            'type': providernet_data['type'],
+            'name': providernet_data['name'],
+            'description': providernet_data['description'],
+            'mtu': providernet_data['mtu'],
+            'status': constants.PROVIDERNET_DOWN}
+        self._validate_providernet(context, providernet)
+        with context.session.begin(subtransactions=True):
+            providernet = ProviderNet(**res)
+            context.session.add(providernet)
+        self._report_providernet_fault(providernet)
+        return self._make_providernet_dict(providernet)
+
+    @db_api.retry_if_session_inactive()
+    def update_providernet(self, context, id, providernet):
+        providernet_data = providernet['providernet']
+        with context.session.begin(subtransactions=True):
+            providernet = self._get_providernet_by_id(context, id)
+            providernet.update(providernet_data)
+        return self._make_providernet_dict(providernet)
+
+    @db_api.retry_if_session_inactive()
+    def delete_providernet(self, context, id):
+        with context.session.begin(subtransactions=True):
+            providernet = self._get_providernet_by_id(context, id)
+            context.session.delete(providernet)
+            self._clear_providernet_fault(providernet)
+
+    @db_api.retry_if_session_inactive()
+    def get_providernet(self, context, id, fields=None):
+        providernet = self._get_providernet_by_id(context, id)
+        return self._make_providernet_dict(providernet, fields)
+
+    @db_api.retry_if_session_inactive()
+    def get_providernets(self, context, filters=None, fields=None,
+                         sorts=None, limit=None, marker=None,
+                         page_reverse=False):
+        marker_obj = self._get_marker_obj(context, 'providernet',
+                                          limit, marker)
+        return self._get_collection(
+            context, ProviderNet, self._make_providernet_dict,
+            filters=filters, fields=fields, sorts=sorts, limit=limit,
+            marker_obj=marker_obj, page_reverse=page_reverse)
+
+    def get_providernet_required_mtu(self, context, id, new_mtu=None):
+        """
+        Returns the MTU value required on a physical interface in order to
+        support the specified provider network MTU.  See notes regarding
+        VXLAN_MTU_OVERHEAD.
+        """
+        providernet = self.get_providernet(context, id)
+        if providernet['type'] != constants.PROVIDERNET_VXLAN:
+            # No additional overhead required for any other providernet
+            # network type.  VLAN is not considered here because most
+            # switches assume an extra 4 bytes are possible over the
+            # configured MTU size in order to supported tagged packets.
+            overhead = 0
+        else:
+            overhead = constants.VXLAN_MTU_OVERHEAD
+        return (overhead + (new_mtu or providernet['mtu']))
+
+    def get_providernet_types(self, context, filters=None, fields=None):
+        return [{'type': constants.PROVIDERNET_FLAT,
+                 'description':
+                 _('Ethernet network without additional encapsulation')},
+                {'type': constants.PROVIDERNET_VLAN,
+                 'description':
+                 _('802.1q encapsulated Ethernet network')},
+                {'type': constants.PROVIDERNET_VXLAN,
+                 'description':
+                _('Virtual Extensible LAN encapsulated network')},
+                ]
+
+    def _get_providernet_range_by_id(self, context, id):
+        try:
+            query = self._model_query(context, ProviderNetRange)
+            providernet_range = query.filter(ProviderNetRange.id == id).one()
+        except exc.NoResultFound:
+            raise ext_providernet.ProviderNetRangeNotFoundById(id=id)
+        return providernet_range
+
+    def get_providernet_range_by_id(self, context, id):
+        try:
+            return self._make_providernet_range_dict(
+                self._get_providernet_range_by_id(context, id))
+        except ext_providernet.ProviderNetRangeNotFoundById:
+            return None
+
+    def _make_providernet_range_dict(self, providernet_range, fields=None):
+        res = {'id': providernet_range['id'],
+               'name': providernet_range['name'],
+               'description': providernet_range['description'],
+               'shared': providernet_range['shared'],
+               'tenant_id': providernet_range['tenant_id'],
+               'minimum': providernet_range['minimum'],
+               'maximum': providernet_range['maximum'],
+               'providernet_name': providernet_range['providernet']['name'],
+               'providernet_id': providernet_range['providernet']['id'],
+               'providernet_type': providernet_range['providernet']['type']}
+        if providernet_range['vxlan']:
+            res['vxlan'] = self._make_vxlan_dict(
+                providernet_range['vxlan'], fields=None)
+        return self._fields(res, fields)
+
+    def _validate_providernet_range_overlap(self, context, range):
+        try:
+            query = self._model_query(context, ProviderNetRange)
+            query = (query.
+                     filter(ProviderNetRange.providernet_id ==
+                            range['providernet_id']).
+                     filter(ProviderNetRange.id != range.get('id', '0')).
+                     filter(and_(range['minimum'] <=
+                                 ProviderNetRange.maximum,
+                                 (range['maximum'] >=
+                                  ProviderNetRange.minimum))))
+            conflict = query.first()
+            if conflict:
+                raise ext_providernet.ProviderNetRangeOverlaps(
+                    id=conflict['id'])
+        except exc.NoResultFound:
+            return
+
+    def check_providernet_id(self, network_type, id):
+        if network_type == constants.PROVIDERNET_FLAT:
+            return False
+        if network_type == constants.PROVIDERNET_VLAN:
+            if (id < constants.MIN_VLAN_TAG or id > constants.MAX_VLAN_TAG):
+                return False
+            return True
+        if network_type == constants.PROVIDERNET_VXLAN:
+            if (id < constants.MIN_VXLAN_VNI or id > constants.MAX_VXLAN_VNI):
+                return False
+            return True
+        return False
+
+    def _validate_providernet_range_vlanid(self, context, range):
+        if (int(range['minimum']) < constants.MIN_VLAN_TAG or
+            int(range['maximum']) < constants.MIN_VLAN_TAG or
+            int(range['minimum']) > constants.MAX_VLAN_TAG or
+            int(range['maximum']) > constants.MAX_VLAN_TAG):
+            raise ext_providernet.ProviderNetVlanIdOutOfRange(
+                minimum=range['minimum'],
+                maximum=range['maximum'],
+                threshold=constants.MAX_VLAN_TAG)
+
+    def _validate_providernet_range_vxlanid(self, context, range):
+        if (int(range['minimum']) < constants.MIN_VXLAN_VNI or
+            int(range['maximum']) < constants.MIN_VXLAN_VNI or
+            int(range['minimum']) > constants.MAX_VXLAN_VNI or
+            int(range['maximum']) > constants.MAX_VXLAN_VNI):
+            raise ext_providernet.ProviderNetVxlanIdOutOfRange(
+                minimum=range['minimum'],
+                maximum=range['maximum'],
+                threshold=constants.MAX_VXLAN_VNI)
+
+    def _validate_providernet_range_order(self, context, range):
+        if (int(range['minimum']) > int(range['maximum'])):
+            raise ext_providernet.ProviderNetRangeOutOfOrder(
+                minimum=range['minimum'],
+                maximum=range['maximum'])
+
+    def _validate_providernet_range_attrs(self, context, providernet, range):
+        if providernet['type'] == constants.PROVIDERNET_VXLAN:
+            if not validators.is_attr_set(range.get('group')):
+                raise ext_providernet.ProviderNetWithoutMulticastGroup()
+            if not validators.is_attr_set(range.get('ttl')):
+                raise ext_providernet.ProviderNetWithoutTTL()
+            if not validators.is_attr_set(range.get('port')):
+                raise ext_providernet.ProviderNetWithoutPort()
+        else:
+            if validators.is_attr_set(range.get('group')):
+                raise ext_providernet.ProviderNetWithMulticastGroup()
+            if validators.is_attr_set(range.get('ttl')):
+                raise ext_providernet.ProviderNetWithTTL()
+
+    def _validate_providernet_range(self, context, providernet, range):
+        if providernet.type == constants.PROVIDERNET_FLAT:
+            raise ext_providernet.ProviderNetRangeNotAllowedOnFlatNet(
+                id=providernet.id)
+        self._validate_providernet_range_order(context, range)
+        if providernet.type == constants.PROVIDERNET_VLAN:
+            self._validate_providernet_range_vlanid(context, range)
+        if providernet.type == constants.PROVIDERNET_VXLAN:
+            self._validate_providernet_range_vxlanid(context, range)
+        self._validate_providernet_range_overlap(context, range)
+        self._validate_providernet_range_attrs(context, providernet, range)
+
+    def create_providernet_range(self, context, providernet_range):
+        range_data = providernet_range['providernet_range']
+        providernet = self._get_providernet_by_id(context,
+                                                  range_data['providernet_id'])
+        self._validate_providernet_range(context, providernet, range_data)
+        res = {'id': range_data.get('id') or uuidutils.generate_uuid(),
+               'name': range_data['name'],
+               'description': range_data['description'],
+               'shared': range_data['shared'],
+               'minimum': range_data['minimum'],
+               'maximum': range_data['maximum'],
+               'providernet_id': range_data['providernet_id']}
+        if not range_data['shared']:
+            res['tenant_id'] = range_data['tenant_id']
+        with context.session.begin(subtransactions=True):
+            providernet_range = ProviderNetRange(**res)
+            context.session.add(providernet_range)
+            if providernet['type'] == constants.PROVIDERNET_VXLAN:
+                res = {'group': range_data['group'],
+                       'port': range_data['port'],
+                       'ttl': range_data['ttl'],
+                       'providernet_range_id': providernet_range.id}
+                vxlan = ProviderNetRangeVxLan(**res)
+                context.session.add(vxlan)
+        return self._make_providernet_range_dict(providernet_range)
+
+    def _add_unchanged_range_attributes(self, updates, existing):
+        """
+        Adds data for unspecified fields on incoming update requests.  Since
+        incoming requests are flat the existing data is also flattened by
+        updating the topmost attributes with lower level attributes.
+        """
+        for key, value in six.iteritems(existing):
+            if isinstance(value, dict):
+                for subkey, subvalue in six.iteritems(value):
+                    updates.setdefault(subkey, subvalue)
+            else:
+                updates.setdefault(key, value)
+        return updates
+
+    @db_api.retry_if_session_inactive()
+    def update_providernet_range(self, context, id, providernet_range):
+        updated_data = providernet_range['providernet_range']
+        with context.session.begin(subtransactions=True):
+            providernet_range = self._get_providernet_range_by_id(context, id)
+            providernet = self._get_providernet_by_id(
+                context, providernet_range['providernet_id'])
+            old_data = self._make_providernet_range_dict(providernet_range)
+            new_data = self._add_unchanged_range_attributes(updated_data,
+                                                            old_data)
+            self._validate_providernet_range(context, providernet, new_data)
+            providernet_range.update(new_data)
+            if providernet_range.vxlan:
+                vxlan = {'group': new_data['group'],
+                         'port': new_data['port'],
+                         'ttl': new_data['ttl']}
+                providernet_range.vxlan.update(vxlan)
+        return self._make_providernet_range_dict(providernet_range)
+
+    @db_api.retry_if_session_inactive()
+    def delete_providernet_range(self, context, id):
+        with context.session.begin(subtransactions=True):
+            providernet_range = self._get_providernet_range_by_id(context, id)
+            context.session.delete(providernet_range)
+
+    @db_api.retry_if_session_inactive()
+    def get_providernet_range(self, context, id, fields=None):
+        providernet_range = self._get_providernet_range_by_id(context, id)
+        return self._make_providernet_range_dict(providernet_range, fields)
+
+    @db_api.retry_if_session_inactive()
+    def get_providernet_ranges(self, context, filters=None, fields=None,
+                     sorts=None, limit=None, marker=None, page_reverse=False):
+        marker_obj = self._get_marker_obj(context, 'providernet_range',
+                                          limit, marker)
+        return self._get_collection(
+            context, ProviderNetRange, self._make_providernet_range_dict,
+            filters=filters, fields=fields, sorts=sorts, limit=limit,
+            marker_obj=marker_obj, page_reverse=page_reverse)
+
+    def _list_networks_on_providernet(self, context, name,
+                                      filters=None, fields=None):
+        # Produce a list of bindings for networks
+        networks = (context.session.query(
+            models_v2.Network.id,
+            models_v2.Network.name,
+            literal_column("0").label("tenant_vlan_id"),
+            ProviderNet.type,
+            segments_model.NetworkSegment.segmentation_id,
+            ProviderNetRange))
+        networks = (networks
+            .select_from(models_v2.Network)
+            .join(segments_model.NetworkSegment,
+                  models_v2.Network.id ==
+                  segments_model.NetworkSegment.network_id)
+            .join(ProviderNet,
+                  segments_model.NetworkSegment
+                  .physical_network ==
+                  ProviderNet.name)
+            .join(ProviderNetRange,
+                  and_(ProviderNet.id ==
+                       ProviderNetRange.providernet_id,
+                       and_(segments_model.NetworkSegment.segmentation_id >=
+                            ProviderNetRange.minimum,
+                            segments_model.NetworkSegment.segmentation_id <=
+                            ProviderNetRange.maximum)))
+            .filter(segments_model.NetworkSegment.physical_network ==
+                    name))
+        # Produce a list of bindings for subnets
+        subnets = context.session.query(
+            models_v2.Network.id,
+            models_v2.Network.name,
+            models_v2.Subnet.vlan_id.label("tenant_vlan_id"),
+            ProviderNet.type,
+            segments_db.SubnetSegment.segmentation_id,
+            ProviderNetRange)
+        subnets = (subnets
+            .select_from(models_v2.Network)
+            .join(models_v2.Subnet,
+                  models_v2.Network.id ==
+                  models_v2.Subnet.network_id)
+            .join(segments_db.SubnetSegment,
+                  models_v2.Subnet.id ==
+                  segments_db.SubnetSegment.subnet_id)
+            .join(ProviderNet,
+                  (ProviderNet.name ==
+                   segments_db.SubnetSegment.physical_network))
+            .join(ProviderNetRange,
+                  and_(ProviderNet.id ==
+                       ProviderNetRange.providernet_id,
+                       and_(segments_db.SubnetSegment.segmentation_id >=
+                            ProviderNetRange.minimum,
+                            segments_db.SubnetSegment.segmentation_id <=
+                            ProviderNetRange.maximum)))
+            .filter(models_v2.Subnet.vlan_id != 0)
+            .filter(segments_db.SubnetSegment.physical_network ==
+                    name)
+            .distinct(models_v2.Subnet.vlan_id))
+        query = networks.union(subnets)
+        columns = ['id', 'name', 'vlan_id',
+                   'providernet_type', 'segmentation_id', 'range']
+        results = []
+        for entry in query.order_by(models_v2.Network.id,
+                                    "tenant_vlan_id").all():
+            range_data = entry[5]
+            res = dict((k, entry[i])
+                       for i, k in enumerate(columns) if k not in ['range'])
+            vxlan = range_data.get('vxlan')
+            attrs = self._make_vxlan_dict(vxlan) if vxlan else {}
+            res.update({'vxlan': attrs})
+            res = self._fields(res, fields)
+            results.append(res)
+        return results
+
+    @db_api.retry_if_session_inactive()
+    def list_networks_on_providernet(self, context, id,
+                                     filters=None, fields=None,
+                                     sorts=None, limit=None,
+                                     marker=None, page_reverse=False):
+        results = []
+        try:
+            # Determine if the id is a UUID or provider network name
+            providernet = self._get_providernet_by_id(context, id)
+        except ValueError:
+            # Query by name to determine if it exists
+            providernet = self._get_providernet_by_name(context, id)
+        try:
+            name = providernet.get("name")
+            results = self._list_networks_on_providernet(
+                context, name, filters, fields)
+        except exc.NoResultFound:
+            results = []
+        return {"networks": results}
+
+    def _report_providernet_fault(self, providernet):
+        """
+        Generate a fault management alarm condition for provider network status
+        """
+        LOG.debug("Report provider network fault: "
+                  "{}".format(providernet['id']))
+        self.fm_driver.report_providernet_fault(providernet['id'])
+
+    def _clear_providernet_fault(self, providernet):
+        """
+        Clear a fault management alarm condition for provider network status
+        """
+        LOG.debug("Clear provider network fault: {}".format(providernet['id']))
+        self.fm_driver.clear_providernet_fault(providernet['id'])
diff --git a/neutron/db/segments_db.py b/neutron/db/segments_db.py
index 405869f..d6aed7f 100644
--- a/neutron/db/segments_db.py
+++ b/neutron/db/segments_db.py
@@ -15,6 +15,7 @@ from neutron_lib.callbacks import registry
 from neutron_lib.callbacks import resources
 from oslo_log import log as logging
 from oslo_utils import uuidutils
+from sqlalchemy import and_
 
 from neutron.db import api as db_api
 from neutron.db.models import segment as segments_model
@@ -122,3 +123,19 @@ def delete_network_segment(context, segment_id):
     """Release a dynamic segment for the params provided if one exists."""
     with db_api.context_manager.writer.using(context):
         network_obj.NetworkSegment.delete_objects(context, id=segment_id)
+
+
+def network_segments_exist(session, network_type, physical_network,
+                           segment_range=None):
+    with session.begin(subtransactions=True):
+        query = (session.query(segments_model.NetworkSegment).
+                 filter_by(network_type=network_type,
+                           physical_network=physical_network))
+        if segment_range:
+            minimum_id = segment_range['minimum']
+            maximum_id = segment_range['maximum']
+            query = (query.filter(and_(
+                segments_model.NetworkSegment.segmentation_id >= minimum_id,
+                segments_model.NetworkSegment.segmentation_id <= maximum_id
+            )))
+        return bool(query.count() > 0)
diff --git a/neutron/drivers/host.py b/neutron/drivers/host.py
index 6fa98b8..856e54b 100644
--- a/neutron/drivers/host.py
+++ b/neutron/drivers/host.py
@@ -26,6 +26,7 @@ import uuid
 import six
 
 from neutron.common import constants
+from neutron.db import providernet_db
 
 
 @six.add_metaclass(abc.ABCMeta)
@@ -57,7 +58,8 @@ class NoopHostDriver(HostDriver):
 
     def get_host_providernets(self, context, host_uuid):
         interface_uuid = str(uuid.uuid5(uuid.UUID(host_uuid), "interface-0"))
-        providernets = []
+        query = context.session.query(providernet_db.ProviderNet)
+        providernets = [entry.id for entry in query.all()]
         return {interface_uuid: {'providernets': providernets}}
 
     def get_host_interfaces(self, context, host_uuid):
diff --git a/neutron/extensions/wrs_provider.py b/neutron/extensions/wrs_provider.py
new file mode 100644
index 0000000..6a6797e
--- /dev/null
+++ b/neutron/extensions/wrs_provider.py
@@ -0,0 +1,472 @@
+# Copyright (c) 2012 OpenStack Foundation.
+# All rights reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import abc
+
+import netaddr
+
+from neutron_lib.api import converters
+from neutron_lib.api import extensions as api_extensions
+from neutron_lib.api import validators
+from neutron_lib import constants
+from neutron_lib.db import constants as db_const
+from neutron_lib import exceptions as exc
+from neutron_lib.plugins import directory
+
+from neutron.common import constants as n_const
+from oslo_log import log as logging
+
+from neutron._i18n import _
+from neutron.api import extensions
+from neutron.api.v2 import attributes
+from neutron.api.v2 import base
+from neutron.api.v2 import resource
+from neutron import policy
+from neutron import wsgi
+
+LOG = logging.getLogger(__name__)
+
+
+def _validate_ip_mcast_address(data, valid_values=None):
+    """
+    Validates that an IP address is a multicast address.
+    """
+    if not netaddr.IPAddress(data).is_multicast():
+        msg = _("'%s' is not a valid multicast IP address") % data
+        LOG.debug(msg)
+        return msg
+
+
+validators.add_validator('type:ip_mcast_address', _validate_ip_mcast_address)
+
+NETWORK_TYPE = 'wrs-provider:network_type'
+PHYSICAL_NETWORK = 'wrs-provider:physical_network'
+SEGMENTATION_ID = 'wrs-provider:segmentation_id'
+MTU = 'wrs-provider:mtu'
+
+EXTENDED_ATTRIBUTES_2_0 = {
+    'subnets': {
+        NETWORK_TYPE: {'allow_post': False, 'allow_put': False,
+                       'enforce_policy': True,
+                       'is_visible': True},
+        PHYSICAL_NETWORK: {'allow_post': False, 'allow_put': False,
+                           'enforce_policy': True,
+                           'is_visible': True},
+        SEGMENTATION_ID: {'allow_post': False, 'allow_put': False,
+                          'convert_to': int,
+                          'enforce_policy': True,
+                          'default': constants.ATTR_NOT_SPECIFIED,
+                          'is_visible': True},
+    },
+}
+
+
+# Provider Network Attribute Map
+PROVIDERNET_TYPE_LIST = ['flat', 'vlan', 'vxlan', 'gre']
+PROVIDERNET_TYPE_NAME = 'providernet_type'
+PROVIDERNET_TYPE_ATTRIBUTES = {
+    PROVIDERNET_TYPE_NAME + 's': {
+        'type': {'allow_post': False, 'allow_put': False,
+                 'validate': {'type:values': PROVIDERNET_TYPE_LIST},
+                 'is_visible': True},
+        'description': {'allow_post': False, 'allow_put': False,
+                        'is_visible': True}
+    },
+}
+
+# Provider Network Attribute Map
+PROVIDERNET_NAME = 'providernet'
+PROVIDERNET_ATTRIBUTES = {
+    PROVIDERNET_NAME + 's': {
+        'id': {'allow_post': False, 'allow_put': False,
+               'is_visible': True},
+        'name': {'allow_post': True, 'allow_put': False,
+                 'is_visible': True,
+                 'validate': {'type:not_empty_string':
+                              db_const.NAME_FIELD_SIZE}},
+        'type': {'allow_post': True, 'allow_put': False,
+                 'validate': {'type:values': PROVIDERNET_TYPE_LIST},
+                 'is_visible': True},
+        'description': {'allow_post': True, 'allow_put': True,
+                        'is_visible': True,
+                        'default': None},
+        'status': {'allow_post': False, 'allow_put': False,
+                   'is_visible': True},
+        'mtu': {'allow_post': True, 'allow_put': True,
+                'convert_to': converters.convert_to_int,
+                'validate': {'type:range': n_const.VALID_MTU_RANGE},
+                'default': n_const.DEFAULT_MTU,
+                'is_visible': True},
+        'tenant_id': {'allow_post': True, 'allow_put': False,
+                      'is_visible': False,
+                      'default': None},
+        'ranges': {'allow_post': False, 'allow_put': False,
+                   'is_visible': True},
+    },
+}
+
+# Provider Network Attribute Map
+PROVIDERNET_RANGE_NAME = 'providernet_range'
+PROVIDERNET_RANGE_ATTRIBUTES = {
+    PROVIDERNET_RANGE_NAME + 's': {
+        'id': {'allow_post': False, 'allow_put': False,
+               'is_visible': True},
+        'name': {'allow_post': True, 'allow_put': False,
+                 'is_visible': True,
+                 'default': None},
+        'description': {'allow_post': True, 'allow_put': True,
+                        'is_visible': True,
+                        'default': None},
+        'shared': {'allow_post': True, 'allow_put': False,
+                   'convert_to': converters.convert_to_boolean,
+                   'is_visible': True,
+                   'default': True},
+        'minimum': {'allow_post': True, 'allow_put': True,
+                    'convert_to': converters.convert_to_int,
+                    'is_visible': True},
+        'maximum': {'allow_post': True, 'allow_put': True,
+                    'convert_to': converters.convert_to_int,
+                    'is_visible': True},
+        'providernet_id': {'allow_post': True, 'allow_put': False,
+                           'is_visible': True},
+        'providernet_name': {'allow_post': False, 'allow_put': False,
+                             'is_visible': True},
+        'providernet_type': {'allow_post': False, 'allow_put': False,
+                             'is_visible': True},
+        'tenant_id': {'allow_post': True, 'allow_put': False,
+                      'is_visible': True,
+                      'default': None},
+        'group': {'allow_post': True, 'allow_put': False,
+                  'is_visible': True,
+                  'validate': {'type:ip_mcast_address': None},
+                  'default': constants.ATTR_NOT_SPECIFIED},
+        'port': {'allow_post': True, 'allow_put': False,
+                 'convert_to': converters.convert_to_int,
+                 'validate': {'type:values': n_const.VALID_VXLAN_UDP_PORTS},
+                 'default': n_const.DEFAULT_VXLAN_UDP_PORT,
+                 'is_visible': True},
+        'ttl': {'allow_post': True, 'allow_put': False,
+                'convert_to': converters.convert_to_int,
+                'validate': {'type:range': n_const.VALID_TTL_RANGE},
+                'is_visible': True,
+                'default': constants.ATTR_NOT_SPECIFIED},
+        'vxlan': {'allow_post': False, 'allow_put': False,
+                  'is_visible': True},
+    },
+}
+
+RESOURCE_ATTRIBUTE_MAPS = dict(PROVIDERNET_TYPE_ATTRIBUTES.items() +
+                               PROVIDERNET_ATTRIBUTES.items() +
+                               PROVIDERNET_RANGE_ATTRIBUTES.items())
+
+
+class ProviderNetTypeNotSupported(exc.NeutronException):
+    message = _("Provider network %(type)s not supported")
+
+
+class ProviderNetNotFoundById(exc.NotFound):
+    message = _("Provider network %(id)s could not be found")
+
+
+class ProviderNetNotFoundByName(exc.NotFound):
+    message = _("Provider network %(name)s could not be found")
+
+
+class ProviderNetNameAlreadyExists(exc.NeutronException):
+    message = _("Provider network with name %(name)s already exists")
+
+
+class ProviderNetReferencedByTenant(exc.NeutronException):
+    message = _("Provider network %(name)s is referenced by one or "
+                "more tenant networks")
+
+
+class ProviderNetRangeReferencedByTenant(exc.NeutronException):
+    message = _("Provider network range %(name)s is referenced by one or "
+                "more tenant networks")
+
+
+class ProviderNetRangeReferencedBySystemVlans(exc.NeutronException):
+    message = _("Provider network range conflicts with system VLAN values "
+                "assigned to interface %(interface)s on host %(host)s")
+
+
+class ProviderNetRangeConflictsWithSystemVlans(exc.Conflict):
+    message = _("Provider network %(providernet)s range %(providernet_range)s "
+                "conflicts with system VLAN values: %(vlan_ids)s")
+
+
+class ProviderNetReferencedByComputeNode(exc.NeutronException):
+    message = _("Provider network %(name)s is referenced by one or "
+                "more compute nodes")
+
+
+class ProviderNetMtuExceedsInterfaceMtu(exc.NeutronException):
+    message = _("%(type)s provider network MTU %(value)s requires an "
+                "interface MTU of %(required)s which exceeds the "
+                "smallest configured MTU of any interface: %(minimum)s")
+
+
+class ProviderNetRequiresInterfaceMtu(exc.NeutronException):
+    message = _("Provider network %(providernet)s requires an interface "
+                "MTU value of at least %(mtu)s bytes")
+
+
+class ProviderNetRangeNotFoundById(exc.NotFound):
+    message = _("Provider network segmentation id range %(id)s "
+                "could not be found")
+
+
+class ProviderNetRangeNotAllowedOnFlatNet(exc.NeutronException):
+    message = _("Provider network segmentation id range not allowed "
+                "on flat network %(id)s")
+
+
+class ProviderNetRangeOverlaps(exc.Conflict):
+    message = _("Provider network segmentation id range overlaps "
+                "with range with id %(id)s")
+
+
+class ProviderNetExistingRangeOverlaps(exc.Conflict):
+    message = _("Provider network range %(first)s overlaps "
+                "with range %(second)s")
+
+
+class ProviderNetTypesIncompatible(exc.Conflict):
+    message = _("Provider network types cannot be assigned to the same "
+                "interface; types: %(types)s")
+
+
+class ProviderNetTypesIncompatibleWithPthru(exc.Conflict):
+    message = _("Provider network types cannot be assigned to a "
+                "PCI passthrough interface; types: %(types)s")
+
+
+class ProviderNetRangeOutOfOrder(exc.NeutronException):
+    message = _("Range minimum %(minimum)s is greater "
+                "than maximum %(maximum)s")
+
+
+class ProviderNetWithoutMulticastGroup(exc.NeutronException):
+    message = _("VXLAN multicast group attributes missing")
+
+
+class ProviderNetWithMulticastGroup(exc.NeutronException):
+    message = _("Multicast group attribute only valid for "
+                "VXLAN provider networks")
+
+
+class ProviderNetWithoutTTL(exc.NeutronException):
+    message = _("VXLAN time-to-live attributes missing")
+
+
+class ProviderNetWithTTL(exc.NeutronException):
+    message = _("Time-to-live attribute only valid for "
+                "VXLAN provider networks")
+
+
+class ProviderNetWithoutPort(exc.NeutronException):
+    message = _("UDP port attributes missing")
+
+
+class ProviderNetWithPort(exc.NeutronException):
+    message = _("UDP Port attribute only valid for "
+                "VXLAN provider networks")
+
+
+class ProviderNetWithInvalidTTL(exc.NeutronException):
+    message = _("Time-to-live %(ttl)s is out of range 1 to 255")
+
+
+class ProviderNetVlanIdOutOfRange(exc.NeutronException):
+    message = _("VLAN id range %(minimum)s to "
+                "%(maximum)s exceeds %(threshold)s")
+
+
+class ProviderNetVxlanIdOutOfRange(exc.NeutronException):
+    message = _("VXLAN id range %(minimum)s to "
+                "%(maximum)s exceeds %(threshold)s")
+
+
+def _raise_if_updates_provider_attributes(attrs):
+    """Raise exception if provider attributes are present.
+
+    This method is used for plugins that do not support
+    updating provider networks.
+    """
+    immutable = (NETWORK_TYPE, PHYSICAL_NETWORK, SEGMENTATION_ID)
+    if any(attributes.is_attr_set(attrs.get(a)) for a in immutable):
+        msg = _("Plugin does not support updating provider attributes")
+        raise exc.InvalidInput(error_message=msg)
+
+PNET_BINDING = 'providernet-binding'
+PNET_BINDINGS = PNET_BINDING + 's'
+
+
+class WrsProviderNetBindingsController(wsgi.Controller):
+    def index(self, request, **kwargs):
+        plugin = directory.get_plugin()
+        policy.enforce(request.context,
+                       "get_%s" % PNET_BINDINGS,
+                       {})
+        return plugin.list_networks_on_providernet(
+            request.context, kwargs['providernet_id'])
+
+
+class Wrs_provider(api_extensions.ExtensionDescriptor):
+    """Extension class supporting provider networks.
+
+    This class is used by neutron's extension framework to make
+    metadata about the provider network extension available to
+    clients. No new resources are defined by this extension. Instead,
+    the existing network resource's request and response messages are
+    extended with attributes in the provider namespace.
+
+    With admin rights, network dictionaries returned will also include
+    provider attributes.
+    """
+
+    @classmethod
+    def get_name(cls):
+        return "wrs-provider-network"
+
+    @classmethod
+    def get_alias(cls):
+        return "wrs-provider"
+
+    @classmethod
+    def get_description(cls):
+        return "WRS Provider Network Extensions."
+
+    @classmethod
+    def get_namespace(cls):
+        return "http://docs.windriver.org/tis/ext/wrs-provider/v1"
+
+    @classmethod
+    def get_updated(cls):
+        return "2014-10-01T12:00:00-00:00"
+
+    @classmethod
+    def get_resources(cls):
+        """Returns extension resources"""
+        ext_list = []
+        my_plurals = [(key, key[:-1])
+                      for key in RESOURCE_ATTRIBUTE_MAPS.keys()]
+        plugin = directory.get_plugin()
+        for plural, name in my_plurals:
+            params = RESOURCE_ATTRIBUTE_MAPS.get(plural)
+            plural = plural.replace('_', '-')
+            controller = base.create_resource(plural, name, plugin, params)
+            plural = "%s/%s" % (cls.get_alias(), plural)
+            ext = extensions.ResourceExtension(plural, controller)
+            ext_list.append(ext)
+
+        # Add an extra extension for the binding controller
+        controller = resource.Resource(WrsProviderNetBindingsController(),
+                                       base.FAULT_MAP)
+        ext_list.append(extensions.ResourceExtension(PNET_BINDINGS, controller,
+                        dict(member_name="providernet",
+                             collection_name="wrs-provider/providernets")))
+        return ext_list
+
+    def get_extended_resources(self, version):
+        if version == "2.0":
+            return EXTENDED_ATTRIBUTES_2_0
+        else:
+            return {}
+
+
+class ProviderNetPluginBase(object):
+    """REST API to manage provider networks.
+
+    All methods must be in an admin context.
+    """
+
+    @abc.abstractmethod
+    def get_providernet_types(self, context, filters=None, fields=None):
+        pass
+
+    @abc.abstractmethod
+    def create_providernet(self, context, providernet):
+        """
+        Create a provider network record for given network data.
+        """
+        pass
+
+    @abc.abstractmethod
+    def delete_providernet(self, context, id):
+        """
+        Delete a provider network by id.
+        """
+        pass
+
+    @abc.abstractmethod
+    def update_providernet(self, context, id, providernet):
+        """
+        Update a provider network with given data
+        @raise exc.BadRequest:
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_providernets(self, context, filters=None, fields=None):
+        pass
+
+    @abc.abstractmethod
+    def get_providernet(self, context, id, fields=None):
+        pass
+
+    @abc.abstractmethod
+    def create_providernet_range(self, context, range):
+        """
+        Create a provider network segmentation id range record for given
+        network data.
+        """
+        pass
+
+    @abc.abstractmethod
+    def delete_providernet_range(self, context, id):
+        """
+        Delete a provider network segmentation id range by id.
+        """
+        pass
+
+    @abc.abstractmethod
+    def update_providernet_range(self, context, id, range):
+        """
+        Update a provider network segmentation id range with given data
+        @raise exc.BadRequest:
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_providernet_ranges(self, context, filters=None, fields=None):
+        pass
+
+    @abc.abstractmethod
+    def get_providernet_range(self, context, id, fields=None):
+        pass
+
+    @abc.abstractmethod
+    def list_networks_on_providernet(self, context, id,
+                                     filters=None, fields=None):
+        pass
diff --git a/neutron/plugins/common/constants.py b/neutron/plugins/common/constants.py
index fa27871..54aa42c 100644
--- a/neutron/plugins/common/constants.py
+++ b/neutron/plugins/common/constants.py
@@ -48,6 +48,7 @@ DEFAULT_SERVICE_PLUGINS = {
     'network_ip_availability': 'network-ip-availability',
     'flavors': 'flavors',
     'revisions': 'revisions',
+    'segments': 'segments',
 }
 
 # Service operation status constants
diff --git a/neutron/plugins/ml2/driver_api.py b/neutron/plugins/ml2/driver_api.py
index 8572298..ff39dad 100644
--- a/neutron/plugins/ml2/driver_api.py
+++ b/neutron/plugins/ml2/driver_api.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 import abc
 
@@ -61,10 +68,11 @@ class _TypeDriverBase(object):
         """
 
     @abc.abstractmethod
-    def validate_provider_segment(self, segment):
+    def validate_provider_segment(self, segment, context=None):
         """Validate attributes of a provider network segment.
 
         :param segment: segment dictionary using keys defined above
+        :param context: optional request context
         :raises: neutron_lib.exceptions.InvalidInput if invalid
 
         Called outside transaction context to validate the provider
@@ -81,6 +89,19 @@ class _TypeDriverBase(object):
         pass
 
     @abc.abstractmethod
+    def update_provider_allocations(self, context):
+        """Update provider network segment allocations.
+
+        :param context: current request context
+
+        Called inside transaction context on session to release update
+        provider network's type-specific resource allocations. Runtime
+        errors are not expected, but raising an exception will result
+        in rollback of the transaction.
+        """
+        pass
+
+    @abc.abstractmethod
     def get_mtu(self, physical):
         """Get driver's network MTU.
 
@@ -117,11 +138,13 @@ class TypeDriver(_TypeDriverBase):
     """
 
     @abc.abstractmethod
-    def reserve_provider_segment(self, session, segment):
+    def reserve_provider_segment(self, session, segment, **filters):
         """Reserve resource associated with a provider network segment.
 
         :param session: database session
         :param segment: segment dictionary
+        :param filters: additional arguments used to select a suitable provider
+        network
         :returns: segment dictionary
 
         Called inside transaction context on session to reserve the
@@ -132,10 +155,12 @@ class TypeDriver(_TypeDriverBase):
         pass
 
     @abc.abstractmethod
-    def allocate_tenant_segment(self, session):
+    def allocate_tenant_segment(self, session, **filters):
         """Allocate resource for a new tenant network segment.
 
         :param session: database session
+        :param filters: additional arguments used to select a suitable provider
+        network
         :returns: segment dictionary using keys defined above
 
         Called inside transaction context on session to allocate a new
@@ -187,7 +212,7 @@ class ML2TypeDriver(_TypeDriverBase):
     """
 
     @abc.abstractmethod
-    def reserve_provider_segment(self, context, segment):
+    def reserve_provider_segment(self, context, segment, **filters):
         """Reserve resource associated with a provider network segment.
 
         :param context: instance of neutron context with DB session
diff --git a/neutron/plugins/ml2/drivers/helpers.py b/neutron/plugins/ml2/drivers/helpers.py
index c3f5f1e..6583086 100644
--- a/neutron/plugins/ml2/drivers/helpers.py
+++ b/neutron/plugins/ml2/drivers/helpers.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 import random
 
@@ -71,6 +78,27 @@ class SegmentTypeDriver(BaseTypeDriver):
             return arg.session, db_api.context_manager.writer.using(arg)
         return arg, arg.session.begin(subtransactions=True)
 
+    def enforce_segment_precedence(self, session, tenant_id, query):
+        """A subclass may override this method to control the order in which
+        unallocated segments are considered.
+        """
+        return query
+
+    def allow_dynamic_allocation(self):
+        """A subclass may override this method to prevent network creation
+        when the segment is not a member of the providernet. By default,
+        it permits dynamic allocation.
+        """
+        return True
+
+    def build_segment_query(self, session, **filters):
+        # Only uses filters that correspond to columns defined by this model.
+        # Subclasses may use/support additional filters
+        columns = set(dict(self.model.__table__.columns))
+        model_filters = dict((k, filters[k])
+                             for k in columns & set(filters.keys()))
+        return session.query(self.model).filter_by(**model_filters)
+
     def allocate_fully_specified_segment(self, context, **raw_segment):
         """Allocate segment fully specified by raw_segment.
 
@@ -78,10 +106,9 @@ class SegmentTypeDriver(BaseTypeDriver):
         If segment does not exists, then try to create it and return db object
         If allocation/creation failed, then return None
         """
-
         network_type = self.get_type()
         session, ctx_manager = self._get_session(context)
-
+        raw_segment = dict((k, raw_segment[k]) for k in self.primary_keys)
         try:
             with ctx_manager:
                 alloc = (
@@ -114,13 +141,17 @@ class SegmentTypeDriver(BaseTypeDriver):
                                   {"type": network_type,
                                    "segment": raw_segment})
 
-                # Segment to create or already allocated
-                LOG.debug("%(type)s segment %(segment)s create started",
-                          {"type": network_type, "segment": raw_segment})
-                alloc = self.model(allocated=True, **raw_segment)
-                alloc.save(session)
-                LOG.debug("%(type)s segment %(segment)s create done",
-                          {"type": network_type, "segment": raw_segment})
+                if self.allow_dynamic_allocation():
+                    # Segment to create or already allocated
+                    LOG.debug("%(type)s segment %(segment)s create started",
+                              {"type": network_type, "segment": raw_segment})
+                    alloc = self.model(allocated=True, **raw_segment)
+                    alloc.save(session)
+                    LOG.debug("%(type)s segment %(segment)s create done",
+                              {"type": network_type, "segment": raw_segment})
+                else:
+                    # Segment not member of this network
+                    raise exc.NetworkNotPredefined(**raw_segment)
 
         except db_exc.DBDuplicateEntry:
             # Segment already allocated (insert failure)
@@ -130,6 +161,9 @@ class SegmentTypeDriver(BaseTypeDriver):
 
         return alloc
 
+    def select_allocation(self, allocations):
+        return random.choice(allocations)
+
     def allocate_partially_specified_segment(self, context, **filters):
         """Allocate model segment from pool partially specified by filters.
 
@@ -139,8 +173,8 @@ class SegmentTypeDriver(BaseTypeDriver):
         network_type = self.get_type()
         session, ctx_manager = self._get_session(context)
         with ctx_manager:
-            select = (session.query(self.model).
-                      filter_by(allocated=False, **filters))
+            filters['allocated'] = False
+            select = self.build_segment_query(session, **filters)
 
             # Selected segment can be allocated before update by someone else,
             allocs = select.limit(IDPOOL_SELECT_SIZE).all()
@@ -149,7 +183,7 @@ class SegmentTypeDriver(BaseTypeDriver):
                 # No resource available
                 return
 
-            alloc = random.choice(allocs)
+            alloc = self.select_allocation(allocs)
             raw_segment = dict((k, alloc[k]) for k in self.primary_keys)
             LOG.debug("%(type)s segment allocate from pool "
                       "started with %(segment)s ",
diff --git a/neutron/plugins/ml2/drivers/type_flat.py b/neutron/plugins/ml2/drivers/type_flat.py
index 856d503..bbb97ae 100644
--- a/neutron/plugins/ml2/drivers/type_flat.py
+++ b/neutron/plugins/ml2/drivers/type_flat.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from neutron_lib import exceptions as exc
 from neutron_lib.plugins.ml2 import api
@@ -67,7 +74,7 @@ class FlatTypeDriver(helpers.BaseTypeDriver):
     def is_partial_segment(self, segment):
         return False
 
-    def validate_provider_segment(self, segment):
+    def validate_provider_segment(self, segment, context=None):
         physical_network = segment.get(api.PHYSICAL_NETWORK)
         if not physical_network:
             msg = _("physical_network required for flat provider network")
@@ -86,7 +93,7 @@ class FlatTypeDriver(helpers.BaseTypeDriver):
                 msg = _("%s prohibited for flat provider network") % key
                 raise exc.InvalidInput(error_message=msg)
 
-    def reserve_provider_segment(self, context, segment):
+    def reserve_provider_segment(self, context, segment, **filters):
         physical_network = segment[api.PHYSICAL_NETWORK]
         try:
             LOG.debug("Reserving flat network on physical "
@@ -101,7 +108,7 @@ class FlatTypeDriver(helpers.BaseTypeDriver):
         segment[api.MTU] = self.get_mtu(alloc.physical_network)
         return segment
 
-    def allocate_tenant_segment(self, context):
+    def allocate_tenant_segment(self, context, **filters):
         # Tenant flat networks are not supported.
         return
 
@@ -128,3 +135,8 @@ class FlatTypeDriver(helpers.BaseTypeDriver):
         if physical_network in self.physnet_mtus:
             mtu.append(int(self.physnet_mtus[physical_network]))
         return min(mtu) if mtu else 0
+
+    def update_provider_allocations(self, context):
+        # Nothing to do here since this driver has static vlan ranges setup in
+        # its configuration file... read once on startup.
+        return
diff --git a/neutron/plugins/ml2/drivers/type_gre.py b/neutron/plugins/ml2/drivers/type_gre.py
index 21ae52b..1b2fd3a 100644
--- a/neutron/plugins/ml2/drivers/type_gre.py
+++ b/neutron/plugins/ml2/drivers/type_gre.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from neutron_lib import exceptions as n_exc
 from oslo_config import cfg
diff --git a/neutron/plugins/ml2/drivers/type_local.py b/neutron/plugins/ml2/drivers/type_local.py
index b5d9acb..04f57fb 100644
--- a/neutron/plugins/ml2/drivers/type_local.py
+++ b/neutron/plugins/ml2/drivers/type_local.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2016 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from neutron_lib import exceptions as exc
 from neutron_lib.plugins.ml2 import api
@@ -46,17 +53,17 @@ class LocalTypeDriver(driver_api.ML2TypeDriver):
     def is_partial_segment(self, segment):
         return False
 
-    def validate_provider_segment(self, segment):
+    def validate_provider_segment(self, segment, context=None):
         for key, value in segment.items():
             if value and key != api.NETWORK_TYPE:
                 msg = _("%s prohibited for local provider network") % key
                 raise exc.InvalidInput(error_message=msg)
 
-    def reserve_provider_segment(self, context, segment):
+    def reserve_provider_segment(self, session, segment, **filters):
         # No resources to reserve
         return segment
 
-    def allocate_tenant_segment(self, context):
+    def allocate_tenant_segment(self, session, **filters):
         # No resources to allocate
         return {api.NETWORK_TYPE: p_const.TYPE_LOCAL}
 
@@ -66,3 +73,7 @@ class LocalTypeDriver(driver_api.ML2TypeDriver):
 
     def get_mtu(self, physical_network=None):
         pass
+
+    def update_provider_allocations(self, context):
+        # Nothing to do here.  Networks are adhoc.
+        pass
diff --git a/neutron/plugins/ml2/drivers/type_tunnel.py b/neutron/plugins/ml2/drivers/type_tunnel.py
index 4bf3e85..22ea667 100644
--- a/neutron/plugins/ml2/drivers/type_tunnel.py
+++ b/neutron/plugins/ml2/drivers/type_tunnel.py
@@ -12,6 +12,14 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
 import abc
 import itertools
 import operator
@@ -173,7 +181,7 @@ class _TunnelTypeDriverBase(helpers.SegmentTypeDriver):
     def is_partial_segment(self, segment):
         return segment.get(api.SEGMENTATION_ID) is None
 
-    def validate_provider_segment(self, segment):
+    def validate_provider_segment(self, segment, context=None):
         physical_network = segment.get(api.PHYSICAL_NETWORK)
         if physical_network:
             msg = _("provider:physical_network specified for %s "
@@ -213,15 +221,16 @@ class TunnelTypeDriver(_TunnelTypeDriverBase):
     - get_allocation
     """
 
-    def reserve_provider_segment(self, session, segment):
+    def reserve_provider_segment(self, session, segment, **filters):
         if self.is_partial_segment(segment):
-            alloc = self.allocate_partially_specified_segment(session)
+            alloc = self.allocate_partially_specified_segment(session,
+                                                              **filters)
             if not alloc:
                 raise exc.NoNetworkAvailable()
         else:
             segmentation_id = segment.get(api.SEGMENTATION_ID)
-            alloc = self.allocate_fully_specified_segment(
-                session, **{self.segmentation_key: segmentation_id})
+            filters[self.segmentation_key] = segmentation_id
+            alloc = self.allocate_fully_specified_segment(session, **filters)
             if not alloc:
                 raise exc.TunnelIdInUse(tunnel_id=segmentation_id)
         return {api.NETWORK_TYPE: self.get_type(),
@@ -229,8 +238,8 @@ class TunnelTypeDriver(_TunnelTypeDriverBase):
                 api.SEGMENTATION_ID: getattr(alloc, self.segmentation_key),
                 api.MTU: self.get_mtu()}
 
-    def allocate_tenant_segment(self, session):
-        alloc = self.allocate_partially_specified_segment(session)
+    def allocate_tenant_segment(self, session, **filters):
+        alloc = self.allocate_partially_specified_segment(session, **filters)
         if not alloc:
             return
         return {api.NETWORK_TYPE: self.get_type(),
@@ -281,7 +290,7 @@ class ML2TunnelTypeDriver(_TunnelTypeDriverBase):
     - get_allocation
     """
 
-    def reserve_provider_segment(self, context, segment):
+    def reserve_provider_segment(self, context, segment, **filters):
         if self.is_partial_segment(segment):
             alloc = self.allocate_partially_specified_segment(context)
             if not alloc:
@@ -306,6 +315,11 @@ class ML2TunnelTypeDriver(_TunnelTypeDriverBase):
                 api.SEGMENTATION_ID: getattr(alloc, self.segmentation_key),
                 api.MTU: self.get_mtu()}
 
+    def update_provider_allocations(self, context):
+        # Nothing to do here since this driver has static vlan ranges setup in
+        # its configuration file... read once on startup.
+        return
+
     def release_segment(self, context, segment):
         tunnel_id = segment[api.SEGMENTATION_ID]
 
diff --git a/neutron/plugins/ml2/drivers/type_vlan.py b/neutron/plugins/ml2/drivers/type_vlan.py
index 622854c..22ed24f 100644
--- a/neutron/plugins/ml2/drivers/type_vlan.py
+++ b/neutron/plugins/ml2/drivers/type_vlan.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 import sys
 
@@ -142,7 +149,7 @@ class VlanTypeDriver(helpers.SegmentTypeDriver):
     def is_partial_segment(self, segment):
         return segment.get(api.SEGMENTATION_ID) is None
 
-    def validate_provider_segment(self, segment):
+    def validate_provider_segment(self, segment, context=None):
         physical_network = segment.get(api.PHYSICAL_NETWORK)
         segmentation_id = segment.get(api.SEGMENTATION_ID)
         if physical_network:
@@ -175,8 +182,7 @@ class VlanTypeDriver(helpers.SegmentTypeDriver):
                 msg = _("%s prohibited for VLAN provider network") % key
                 raise exc.InvalidInput(error_message=msg)
 
-    def reserve_provider_segment(self, context, segment):
-        filters = {}
+    def reserve_provider_segment(self, context, segment, **filters):
         physical_network = segment.get(api.PHYSICAL_NETWORK)
         if physical_network is not None:
             filters['physical_network'] = physical_network
@@ -200,10 +206,11 @@ class VlanTypeDriver(helpers.SegmentTypeDriver):
                 api.SEGMENTATION_ID: alloc.vlan_id,
                 api.MTU: self.get_mtu(alloc.physical_network)}
 
-    def allocate_tenant_segment(self, context):
+    def allocate_tenant_segment(self, context, **filters):
         for physnet in self.network_vlan_ranges:
+            filters['physical_network'] = physnet
             alloc = self.allocate_partially_specified_segment(
-                context, physical_network=physnet)
+                context, **filters)
             if alloc:
                 break
         else:
@@ -255,3 +262,8 @@ class VlanTypeDriver(helpers.SegmentTypeDriver):
         if physical_network in self.physnet_mtus:
             mtu.append(int(self.physnet_mtus[physical_network]))
         return min(mtu) if mtu else 0
+
+    def update_provider_allocations(self, context):
+        # Nothing to do here since this driver has static vlan ranges setup in
+        # its configuration file... read once on startup.
+        return
diff --git a/neutron/plugins/ml2/drivers/type_vxlan.py b/neutron/plugins/ml2/drivers/type_vxlan.py
index 5d46424..014aa2b 100644
--- a/neutron/plugins/ml2/drivers/type_vxlan.py
+++ b/neutron/plugins/ml2/drivers/type_vxlan.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from neutron_lib import exceptions as n_exc
 from oslo_config import cfg
diff --git a/neutron/plugins/ml2/managers.py b/neutron/plugins/ml2/managers.py
index 0a1ebc6..bcd2ab9 100644
--- a/neutron/plugins/ml2/managers.py
+++ b/neutron/plugins/ml2/managers.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from neutron_lib.api.definitions import portbindings
 from neutron_lib.api.definitions import provider_net as provider
@@ -70,6 +77,9 @@ class TypeManager(stevedore.named.NamedExtensionManager):
                 self.drivers[network_type] = ext
         LOG.info("Registered types: %s", self.drivers.keys())
 
+    def network_type_supported(self, network_type):
+        return bool(network_type in self.drivers)
+
     def _check_tenant_network_types(self, types):
         self.tenant_network_types = []
         for network_type in types:
@@ -87,7 +97,7 @@ class TypeManager(stevedore.named.NamedExtensionManager):
                       "Service terminated!", ext_network_type)
             raise SystemExit(1)
 
-    def _process_provider_segment(self, segment):
+    def _process_provider_segment(self, context, segment):
         (network_type, physical_network,
          segmentation_id) = (self._get_attribute(segment, attr)
                              for attr in provider.ATTRIBUTES)
@@ -96,13 +106,13 @@ class TypeManager(stevedore.named.NamedExtensionManager):
             segment = {ml2_api.NETWORK_TYPE: network_type,
                        ml2_api.PHYSICAL_NETWORK: physical_network,
                        ml2_api.SEGMENTATION_ID: segmentation_id}
-            self.validate_provider_segment(segment)
+            self.validate_provider_segment(segment, context)
             return segment
 
         msg = _("network_type required")
         raise exc.InvalidInput(error_message=msg)
 
-    def _process_provider_create(self, network):
+    def _process_provider_create(self, context, network):
         if any(validators.is_attr_set(network.get(attr))
                for attr in provider.ATTRIBUTES):
             # Verify that multiprovider and provider attributes are not set
@@ -110,9 +120,9 @@ class TypeManager(stevedore.named.NamedExtensionManager):
             if validators.is_attr_set(network.get(mpnet.SEGMENTS)):
                 raise mpnet.SegmentsSetInConjunctionWithProviders()
             segment = self._get_provider_segment(network)
-            return [self._process_provider_segment(segment)]
+            return [self._process_provider_segment(context, segment)]
         elif validators.is_attr_set(network.get(mpnet.SEGMENTS)):
-            segments = [self._process_provider_segment(s)
+            segments = [self._process_provider_segment(context, s)
                         for s in network[mpnet.SEGMENTS]]
             mpnet.check_duplicate_segments(segments, self.is_partial_segment)
             return segments
@@ -191,21 +201,22 @@ class TypeManager(stevedore.named.NamedExtensionManager):
 
     def create_network_segments(self, context, network, tenant_id):
         """Call type drivers to create network segments."""
-        segments = self._process_provider_create(network)
+        segments = self._process_provider_create(context, network)
+        filters = {'tenant_id': tenant_id}
         with db_api.context_manager.writer.using(context):
             network_id = network['id']
             if segments:
                 for segment_index, segment in enumerate(segments):
                     segment = self.reserve_provider_segment(
-                        context, segment)
+                        context, segment, **filters)
                     self._add_network_segment(context, network_id, segment,
                                               segment_index)
             elif (cfg.CONF.ml2.external_network_type and
                   self._get_attribute(network, external_net.EXTERNAL)):
-                segment = self._allocate_ext_net_segment(context)
+                segment = self._allocate_ext_net_segment(context, **filters)
                 self._add_network_segment(context, network_id, segment)
             else:
-                segment = self._allocate_tenant_net_segment(context)
+                segment = self._allocate_tenant_net_segment(context, **filters)
                 self._add_network_segment(context, network_id, segment)
 
     def reserve_network_segment(self, context, segment_data):
@@ -226,6 +237,7 @@ class TypeManager(stevedore.named.NamedExtensionManager):
 
         # Reserve segment in type driver
         with db_api.context_manager.writer.using(context):
+            # TODO(alegacy): tenant_id filters
             return self.reserve_provider_segment(context, segment)
 
     def is_partial_segment(self, segment):
@@ -237,42 +249,43 @@ class TypeManager(stevedore.named.NamedExtensionManager):
             msg = _("network_type value '%s' not supported") % network_type
             raise exc.InvalidInput(error_message=msg)
 
-    def validate_provider_segment(self, segment):
+    def validate_provider_segment(self, segment, context=None):
         network_type = segment[ml2_api.NETWORK_TYPE]
         driver = self.drivers.get(network_type)
         if driver:
-            driver.obj.validate_provider_segment(segment)
+            driver.obj.validate_provider_segment(segment, context)
         else:
             msg = _("network_type value '%s' not supported") % network_type
             raise exc.InvalidInput(error_message=msg)
 
-    def reserve_provider_segment(self, context, segment):
+    def reserve_provider_segment(self, context, segment, **filters):
         network_type = segment.get(ml2_api.NETWORK_TYPE)
         driver = self.drivers.get(network_type)
         if isinstance(driver.obj, api.TypeDriver):
             return driver.obj.reserve_provider_segment(context.session,
-                                                       segment)
+                                                       segment, **filters)
         else:
             return driver.obj.reserve_provider_segment(context,
-                                                       segment)
+                                                       segment, **filters)
 
-    def _allocate_segment(self, context, network_type):
+    def _allocate_segment(self, context, network_type, **filters):
         driver = self.drivers.get(network_type)
         if isinstance(driver.obj, api.TypeDriver):
-            return driver.obj.allocate_tenant_segment(context.session)
+            return driver.obj.allocate_tenant_segment(context.session,
+                                                      **filters)
         else:
-            return driver.obj.allocate_tenant_segment(context)
+            return driver.obj.allocate_tenant_segment(context, **filters)
 
-    def _allocate_tenant_net_segment(self, context):
+    def _allocate_tenant_net_segment(self, context, **filters):
         for network_type in self.tenant_network_types:
-            segment = self._allocate_segment(context, network_type)
+            segment = self._allocate_segment(context, network_type, **filters)
             if segment:
                 return segment
         raise exc.NoNetworkAvailable()
 
-    def _allocate_ext_net_segment(self, context):
+    def _allocate_ext_net_segment(self, context, **filters):
         network_type = cfg.CONF.ml2.external_network_type
-        segment = self._allocate_segment(context, network_type)
+        segment = self._allocate_segment(context, network_type, **filters)
         if segment:
             return segment
         raise exc.NoNetworkAvailable()
@@ -333,6 +346,15 @@ class TypeManager(stevedore.named.NamedExtensionManager):
         else:
             LOG.debug("No segment found with id %(segment_id)s", segment_id)
 
+    def update_provider_allocations(self, context, network_type):
+        driver = self.drivers.get(network_type)
+        driver.obj.update_provider_allocations(context)
+
+    def network_segments_exist(self, context, network_type, physical_network,
+                          segment_range=None):
+        return segments_db.network_segments_exist(
+            context.session, network_type, physical_network, segment_range)
+
 
 class MechanismManager(stevedore.named.NamedExtensionManager):
     """Manage networking mechanisms using drivers."""
diff --git a/neutron/plugins/ml2/plugin.py b/neutron/plugins/ml2/plugin.py
index ebaee5e..8cbec75 100644
--- a/neutron/plugins/ml2/plugin.py
+++ b/neutron/plugins/ml2/plugin.py
@@ -20,6 +20,11 @@
 # of an applicable Wind River license agreement.
 #
 
+import re
+
+from sqlalchemy import and_
+from sqlalchemy import func
+
 from eventlet import greenthread
 from neutron_lib.api.definitions import extra_dhcp_opt as edo_ext
 from neutron_lib.api.definitions import network as net_def
@@ -77,6 +82,7 @@ from neutron.db import extradhcpopt_db
 from neutron.db import hosts_db
 from neutron.db.models import securitygroup as sg_models
 from neutron.db import models_v2
+from neutron.db import providernet_db
 from neutron.db import provisioning_blocks
 from neutron.db.quota import driver  # noqa
 from neutron.db import securitygroups_rpc_base as sg_db_rpc
@@ -88,6 +94,7 @@ from neutron.extensions import availability_zone as az_ext
 from neutron.extensions import netmtu_writable as mtu_ext
 from neutron.extensions import providernet as provider
 from neutron.extensions import vlantransparent
+from neutron.extensions import wrs_provider
 from neutron.plugins.common import utils as p_utils
 from neutron.plugins.ml2.common import exceptions as ml2_exc
 from neutron.plugins.ml2 import config  # noqa
@@ -132,7 +139,8 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
                 extradhcpopt_db.ExtraDhcpOptMixin,
                 address_scope_db.AddressScopeDbMixin,
                 service_type_db.SubnetServiceTypeMixin,
-                hosts_db.HostSchedulerDbMixin):
+                hosts_db.HostSchedulerDbMixin,
+                providernet_db.ProviderNetDbMixin):
 
     """Implement the Neutron L2 abstractions using modules.
 
@@ -163,7 +171,7 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
                                     "network_availability_zone",
                                     "default-subnetpools",
                                     "subnet-service-types",
-                                    "host"]
+                                    "host", "wrs-provider"]
 
     @property
     def supported_extension_aliases(self):
@@ -1941,3 +1949,384 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
             self.mechanism_manager.update_network_precommit(mech_context)
         elif event == events.AFTER_CREATE or event == events.AFTER_DELETE:
             self.mechanism_manager.update_network_postcommit(mech_context)
+
+    def _update_providernet_allocations(self, context, providernet):
+        self.type_manager.update_provider_allocations(context,
+                                                      providernet['type'])
+
+    def _is_providernet_referenced(self, context, providernet,
+                                   segment_range=None):
+        return self.type_manager.network_segments_exist(
+            context, providernet['type'], providernet['name'], segment_range)
+
+    def _is_providernet_type_supported(self, network_type):
+        if not self.type_manager.network_type_supported(network_type):
+            raise wrs_provider.ProviderNetTypeNotSupported(type=network_type)
+
+    def _check_providernet_mtu(self, context, id, data):
+        """
+        Determines if the MTU can be changed to a new value.  If the
+        providernet has not yet been associated to any compute node data
+        interfaces then the change is automatically allowed; otherwise the MTU
+        of the compute node interfaces must be large enough to support the new
+        value.
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            try:
+                min_mtu = (session.query(func.min(hosts_db.HostInterface.mtu)).
+                           select_from(hosts_db.HostInterface).
+                           join(hosts_db.HostInterfaceProviderNetBinding,
+                                (hosts_db.HostInterfaceProviderNetBinding.
+                                 interface_id == hosts_db.HostInterface.id)).
+                           filter(hosts_db.HostInterface.network_type ==
+                                  hosts_db.DATA_NETWORK).
+                           filter(hosts_db.HostInterfaceProviderNetBinding.
+                                  providernet_id == id).
+                           one()[0])
+            except sa_exc.NoResultFound:
+                # Not yet associated to any compute nodes; allow
+                return
+        if not min_mtu:
+            # Not yet associated to any compute nodes; allow
+            return
+        new_mtu = data['mtu']
+        required_mtu = self.get_providernet_required_mtu(context, id, new_mtu)
+        if required_mtu > min_mtu:
+            raise wrs_provider.ProviderNetMtuExceedsInterfaceMtu(
+                type=data['type'], value=data['mtu'],
+                required=required_mtu, minimum=min_mtu)
+
+    def update_providernet(self, context, id, providernet):
+        """
+        Run semantic checks prior to updates
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            existing = self.get_providernet_by_id(context, id)
+            updated = providernet.get('providernet')
+            existing.update(updated)
+            self._check_providernet_mtu(context, id, existing)
+            return super(Ml2Plugin, self).update_providernet(
+                context, id, providernet)
+
+    def create_providernet(self, context, providernet):
+        """
+        Update segmentation id allocations on provider network creation.
+        """
+        session = context.session
+        self._is_providernet_type_supported(providernet['providernet']['type'])
+        with session.begin(subtransactions=True):
+            providernet = super(Ml2Plugin, self).create_providernet(
+                context, providernet)
+            # Setup an empty list of ranges so that type drivers such as
+            # 'flat' get an opportunity to update their internal mappings
+            providernet['ranges'] = []
+            self._update_providernet_allocations(context, providernet)
+            return providernet
+
+    def delete_providernet(self, context, id):
+        """
+        Run semantic checks and update segmentation id allocations on
+        provider network deletion
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            providernet = self.get_providernet_by_id(context, id)
+
+            if self._is_providernet_referenced(context, providernet):
+                name = providernet.get('name')
+                raise wrs_provider.ProviderNetReferencedByTenant(name=name)
+            if self.get_providernet_hosts(context, id):
+                name = providernet.get('name')
+                raise wrs_provider.ProviderNetReferencedByComputeNode(
+                    name=name)
+
+            super(Ml2Plugin, self).delete_providernet(context, id)
+
+            # clear all ranges for the provider network being deleted
+            # which are deleted through cascade so that the allocations
+            # can be updated to reflect the removal of the entry
+            providernet['ranges'] = None
+            self._update_providernet_allocations(context, providernet)
+
+    def _check_for_range_overlaps(self, context,
+                                  providernet, range_data,
+                                  providernets):
+        """
+        Given a providernet and one of its ranges, check all ranges from all
+        provider networks supplied in the providernets list.  If a conflict is
+        found the DB object for the conflicting range is returned; otherwise
+        None is returned.
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            query = (
+                session.query(providernet_db.ProviderNetRange).
+                join(providernet_db.ProviderNet,
+                     providernet_db.ProviderNet.id ==
+                     providernet_db.ProviderNetRange.providernet_id).
+                filter(providernet_db.ProviderNet.id != providernet['id']).
+                filter(providernet_db.ProviderNet.type ==
+                       providernet['type']).
+                filter(and_((range_data['minimum'] <=
+                             providernet_db.ProviderNetRange.maximum),
+                            (range_data['maximum'] >=
+                             providernet_db.ProviderNetRange.minimum))))
+            if len(providernets) > 0:
+                query = (query.filter(providernet_db.ProviderNet.id.
+                                      in_(providernets)))
+            try:
+                return query.first()
+            except sa_exc.NoResultFound:
+                return None
+
+    def _validate_range_update_for_overlaps(self, context, providernet,
+                                            range_data):
+        """
+        Checks for segmentation id range overlaps against all other provider
+        networks of the same type that share a data interface on any compute
+        node.
+        """
+        # Query the list of interfaces that have a binding with this
+        # providernet instance.
+        session = context.session
+        interfaces = (session.query(hosts_db.HostInterfaceProviderNetBinding.
+                                    interface_id).
+                      filter(hosts_db.HostInterfaceProviderNetBinding.
+                             providernet_id == providernet['id']))
+        if interfaces.count() == 0:
+            return
+        # Query the other providernet instances that are related to the same
+        # interfaces and have the same type
+        providernets = (
+            session.query(providernet_db.ProviderNet).
+            select_from(providernet_db.ProviderNetRange).
+            join(providernet_db.ProviderNet,
+                 providernet_db.ProviderNet.id ==
+                 providernet_db.ProviderNetRange.providernet_id).
+            join(hosts_db.HostInterfaceProviderNetBinding,
+                 and_(hosts_db.HostInterfaceProviderNetBinding.
+                      providernet_id == providernet_db.ProviderNet.id,
+                      providernet_db.ProviderNet.type ==
+                      providernet['type'])).
+            filter(hosts_db.HostInterfaceProviderNetBinding.
+                   interface_id.in_(interfaces)).
+            group_by(providernet_db.ProviderNet.id))
+        if providernets.count() == 0:
+            return
+        providernet_ids = [p.id for p in providernets.all()]
+        conflict = self._check_for_range_overlaps(
+            context, providernet, range_data, providernet_ids)
+        if conflict:
+            raise wrs_provider.ProviderNetRangeOverlaps(id=conflict['id'])
+        return
+
+    def create_providernet_range(self, context, providernet_range):
+        """
+        Update segmentation id allocations on range creation.
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            providernet_range = (
+                super(Ml2Plugin, self).create_providernet_range(
+                    context, providernet_range))
+            providernet = self.get_providernet_by_id(
+                context, providernet_range['providernet_id'])
+            self._update_providernet_allocations(context, providernet)
+            return providernet_range
+
+    def _validate_range_update_for_orphans(self, context, existing, updates):
+        """
+        Determine if the new range will exclude any values that were
+        previously in the range.  If so throw an exception.
+        """
+        providernet_id = existing['providernet_id']
+        providernet = self.get_providernet_by_id(context, providernet_id)
+        old_min = existing['minimum']
+        new_min = updates['minimum']
+        old_max = existing['maximum']
+        new_max = updates['maximum']
+        ranges = []
+        if (new_min > old_max) or (new_max < old_min):
+            # The entire old range needs to be checked since the new range
+            # has no overlap
+            ranges = [existing]
+        elif (new_min <= old_min and new_max >= old_max):
+            # Nothing needs to be checked since the new range completely
+            # overlaps the old range
+            return
+        else:
+            # Calculate the difference between the two sets to find out which
+            # id values will no longer be part of the range
+            if new_min > old_min:
+                ranges.append({'minimum': old_min,
+                               'maximum': new_min - 1})
+            if new_max < old_max:
+                ranges.append({'minimum': new_max + 1,
+                               'maximum': old_max})
+        for r in ranges:
+            if self._is_providernet_referenced(context, providernet, r):
+                name = existing.get('name')
+                raise wrs_provider.ProviderNetRangeReferencedByTenant(
+                    name=name)
+
+    def _validate_range_update_for_reserved(self, context, providernet,
+                                            range_data):
+        """
+        Checks for segmentation id range overlaps against all reserved vlan
+        values on any interface for which the providernet is associated.
+        """
+        if providernet['type'] != n_const.PROVIDERNET_VLAN:
+            return
+        # Query all interfaces that have an association to this provider
+        # network.
+        session = context.session
+        query = (session.query(hosts_db.HostInterface).
+                 join(hosts_db.HostInterfaceProviderNetBinding,
+                      (hosts_db.HostInterfaceProviderNetBinding.
+                       interface_id == hosts_db.HostInterface.id)).
+                 filter(hosts_db.HostInterfaceProviderNetBinding.
+                        providernet_id == providernet['id']))
+        # Check each returned interface to determine if it has any reserved
+        # vlan values that conflict with this provider network range.
+        for entry in [x for x in query.all() if x['vlans']]:
+            vlans = re.sub(',,+', ',', entry['vlans'])
+            vlans = vlans.split(',')
+            if any(range_data['minimum'] <= int(x) <= range_data['maximum']
+                   for x in vlans):
+                raise wrs_provider.ProviderNetRangeReferencedBySystemVlans(
+                    host=entry['host_id'], interface=entry['id'])
+
+    def _validate_providernet_range(self, context, providernet, updates):
+        super(Ml2Plugin, self)._validate_providernet_range(
+            context, providernet, updates)
+        if 'id' in updates:
+            # This is an update rather than a create
+            existing = self.get_providernet_range_by_id(context, updates['id'])
+            self._validate_range_update_for_orphans(context, existing, updates)
+        self._validate_range_update_for_overlaps(context, providernet, updates)
+        self._validate_range_update_for_reserved(context, providernet, updates)
+
+    def update_providernet_range(self, context, id, providernet_range):
+        """
+        Update segmentation id allocations on range updates.
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            providernet_range = (
+                super(Ml2Plugin, self).update_providernet_range(
+                    context, id, providernet_range))
+            providernet = self.get_providernet_by_id(
+                context, providernet_range['providernet_id'])
+            self._update_providernet_allocations(context, providernet)
+            return providernet_range
+
+    def delete_providernet_range(self, context, id):
+        """
+        Update segmentation id allocations on range deletion.
+        """
+        session = context.session
+        with session.begin(subtransactions=True):
+            providernet_range = self.get_providernet_range_by_id(context, id)
+            providernet = self.get_providernet_by_id(
+                context, providernet_range['providernet_id'])
+            segments = {'minimum': providernet_range['minimum'],
+                        'maximum': providernet_range['maximum']}
+            if self._is_providernet_referenced(context, providernet, segments):
+                name = providernet_range.get('name')
+                raise wrs_provider.ProviderNetRangeReferencedByTenant(
+                    name=name)
+            super(Ml2Plugin, self).delete_providernet_range(context, id)
+            self._update_providernet_allocations(context, providernet)
+
+    def _validate_providernets_compatibility(self, context, interface):
+        """
+        Validate that the list of provider networks associated to this
+        interface are all compatible with each other.   Also validate that the
+        provider networks are compatible with the interface type.
+        """
+        providernets = interface['providernet_ids']
+        if len(providernets) == 0:
+            # Nothing to check
+            return
+        types = set()
+        for providernet_id in providernets:
+            providernet = self._get_providernet_by_id(context, providernet_id)
+            types.add(providernet['type'])
+            for r in providernet['ranges']:
+                # Check each range against all other ranges for the other
+                # provider networks on this same interface.
+                conflict = self._check_for_range_overlaps(
+                    context, providernet, r, providernets)
+                if conflict:
+                    raise wrs_provider.ProviderNetExistingRangeOverlaps(
+                        first=r['id'], second=conflict['id'])
+        incompatible_types = [n_const.PROVIDERNET_FLAT,
+                              n_const.PROVIDERNET_VXLAN]
+        if all(x in types for x in incompatible_types):
+            raise wrs_provider.ProviderNetTypesIncompatible(
+                types=','.join(list(incompatible_types)))
+        if interface['network_type'] in [hosts_db.PCI_PASSTHROUGH]:
+            incompatible_types = [n_const.PROVIDERNET_VXLAN]
+            if all(x in types for x in incompatible_types):
+                raise wrs_provider.ProviderNetTypesIncompatibleWithPthru(
+                    types=','.join(list(incompatible_types)))
+
+    def _validate_providernets_system_vlans(self, context, interface):
+        """
+        Validate that the list of system vlans assigned to the interface does
+        not conflict with any provider network range associated with the
+        interface.
+        """
+        vlan_ids = interface['vlan_ids']
+        if not vlan_ids:
+            # Nothing to check
+            return
+        providernets = interface['providernet_ids']
+        session = context.session
+        with session.begin(subtransactions=True):
+            query = (session.query(providernet_db.ProviderNetRange).
+                     join(providernet_db.ProviderNet,
+                          (providernet_db.ProviderNet.id ==
+                           providernet_db.ProviderNetRange.providernet_id)).
+                     filter(providernet_db.ProviderNet.type ==
+                            n_const.PROVIDERNET_VLAN).
+                     filter(providernet_db.ProviderNetRange.providernet_id.
+                            in_(providernets)))
+            for r in query.all():
+                matches = any(r['minimum'] <= x <= r['maximum']
+                              for x in vlan_ids)
+                if matches:
+                    raise wrs_provider. \
+                        ProviderNetRangeConflictsWithSystemVlans(
+                            providernet=r['providernet_id'],
+                            providernet_range=r['id'],
+                            vlan_ids=','.join([str(x) for x in vlan_ids]))
+
+    def _validate_mtu_compatibility(self, context, interface):
+        """
+        Validate that the interface MTU is large enough to support the
+        provider networks that are assigned to it.
+        """
+        mtu = int(interface['mtu'])
+        providernets = interface['providernet_ids']
+        for providernet_id in providernets:
+            required_mtu = self.get_providernet_required_mtu(
+                context, providernet_id)
+            if required_mtu > mtu:
+                raise wrs_provider.ProviderNetRequiresInterfaceMtu(
+                    providernet=providernet_id, mtu=required_mtu)
+
+    def _validate_interface(self, context, body):
+        """
+        Override the bind interface validation to perform semantic checks that
+        are beyond the scope of only the host module.
+        """
+        # The super class will do syntax and basic semantic checking on the
+        # input parameters.
+        super(Ml2Plugin, self)._validate_interface(context, body)
+        interface = body['interface']
+        self._validate_providernets_compatibility(context, interface)
+        self._validate_providernets_system_vlans(context, interface)
+        self._validate_mtu_compatibility(context, interface)
diff --git a/neutron/plugins/ml2/rpc.py b/neutron/plugins/ml2/rpc.py
index ed9a868..a539da4 100644
--- a/neutron/plugins/ml2/rpc.py
+++ b/neutron/plugins/ml2/rpc.py
@@ -73,6 +73,27 @@ class RpcCallbacks(type_tunnel.TunnelRpcCallbackMixin):
             if port['status'] != new_status:
                 return new_status
 
+    def get_provider_details(self, rpc_context, **kwargs):
+        """Agent requests provider network details configuration details."""
+        agent_id = kwargs.get('agent_id')
+        hostname = kwargs.get('host')
+        network_type = kwargs.get('network_type')
+        physical_network = kwargs.get('physical_network')
+        segmentation_id = kwargs.get('segmentation_id')
+
+        LOG.debug("Provider network configuration details requested from "
+                  "%(agent_id)s on %(host)s for providernet "
+                  "%(type)s:%(name)s:%(id)s",
+                  {'agent_id': agent_id, 'host': hostname,
+                   'type': network_type,
+                   'name': physical_network,
+                   'id': segmentation_id})
+        plugin = directory.get_plugin()
+        data = plugin.get_providernet_segment_details(
+            rpc_context, network_type, physical_network, segmentation_id)
+        LOG.debug("Returning: %s", data)
+        return data
+
     def get_host_details(self, rpc_context, **kwargs):
         """Agent requests host configuration details."""
         agent_id = kwargs.get('agent_id')
diff --git a/neutron/plugins/wrs/drivers/type_generic.py b/neutron/plugins/wrs/drivers/type_generic.py
new file mode 100644
index 0000000..727d132
--- /dev/null
+++ b/neutron/plugins/wrs/drivers/type_generic.py
@@ -0,0 +1,358 @@
+# Copyright (c) 2013 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import abc
+
+import six
+
+from neutron_lib import exceptions as exc
+from oslo_log import log as logging
+from sqlalchemy import and_, or_
+from sqlalchemy import asc
+from sqlalchemy.orm import exc as sa_exc
+from sqlalchemy import sql
+
+from neutron._i18n import _
+from neutron.common import exceptions as n_exc
+from neutron.db import api as db_api
+from neutron.db import providernet_db as pnet_db
+from neutron.plugins.ml2 import driver_api as api
+from neutron.plugins.ml2.drivers import helpers
+
+LOG = logging.getLogger(__name__)
+
+
+class GenericProvidernetTypeDriverMixin(object):
+
+    def _get_providernet(self, session, physical_network):
+        """A private function to query a provider network by name.  This
+        function is used rather than the get_providernet_by_name because we do
+        not have access to the 'context' object from within this class.
+        """
+        with session.begin(subtransactions=True):
+            query = (session.query(pnet_db.ProviderNet).
+                     filter_by(name=physical_network))
+            return query.one()
+
+    def _get_providernet_ranges(self, session):
+        """A private function to query all provider network ranges.  This
+        function is used rather than the get_providernet_ranges because we do
+        not always have access to the 'context' object from within this class.
+        """
+        ranges = {}
+        with session.begin(subtransactions=True):
+            query = (session.query(pnet_db.ProviderNetRange).
+                     join(pnet_db.ProviderNet).
+                     filter(pnet_db.ProviderNet.type == self.get_type()))
+            for entry in query.all():
+                physical_network = entry['providernet']['name']
+                if physical_network not in ranges:
+                    ranges[physical_network] = []
+                ranges[physical_network].append((entry['minimum'],
+                                                 entry['maximum']))
+        return ranges
+
+    def _providernet_exists(self, context, physical_network):
+        """Determines if a provider network exist as an entry in the
+        providernet db table.
+        """
+        try:
+            session = context.session
+            providernet = self._get_providernet(session, physical_network)
+            if providernet and providernet.type == self.get_type():
+                return True
+            return False
+        except sa_exc.NoResultFound:
+            return False
+
+
+class GenericRangeTypeDriver(helpers.SegmentTypeDriver,
+                             GenericProvidernetTypeDriverMixin):
+    """
+    Manages allocation state for any segmentation id range based type
+    drivers.  Only appropriate for WRS based classes that support the concept
+    of managed provider networks because of the dependency on
+    enforce_segment_precedence().  Subclasses must provide implementations of
+    abstract methods in order to operate correctly.  Subclasses must also
+    define these instance variables:
+
+        self.model_key
+        self.segmentation_key
+
+    """
+
+    def __init__(self, model):
+        super(GenericRangeTypeDriver, self).__init__(model)
+
+    @abc.abstractmethod
+    def get_segmentation_key(self):
+        """
+        Return the column name which represents the segmentation id field.
+        """
+        pass
+
+    @abc.abstractmethod
+    def is_valid_segmentation_id(self, value):
+        """
+        Determines whether a segmentation id is valid for a specific type
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_min_id(self):
+        """
+        Return the minimum valid segmentation id
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_max_id(self):
+        """
+        Return the maximum valid segmentation id
+        """
+        pass
+
+    def get_mtu(self, physical_network):
+        session = db_api.get_current_session()
+        providernet = self._get_providernet(session, physical_network)
+        return providernet.mtu
+
+    def _sync_allocations(self, session=None):
+        session = session or db_api.get_current_session()
+        with session.begin(subtransactions=True):
+            # get existing allocations for all physical networks
+            allocations = dict()
+            allocs = (session.query(self.model).
+                      with_lockmode('update'))
+            for alloc in allocs:
+                if alloc.physical_network not in allocations:
+                    allocations[alloc.physical_network] = set()
+                allocations[alloc.physical_network].add(alloc)
+
+            # process segmentation ranges for each configured physical network
+            ranges = self._get_providernet_ranges(session)
+            for (physical_network, ranges) in ranges.items():
+                # determine current configured allocatable segmentation ids for
+                # this physical network
+                ids = set()
+                for id_min, id_max in ranges:
+                    ids |= set(six.moves.range(id_min, id_max + 1))
+
+                # remove from table unallocated segmentation ids not currently
+                # allocatable
+                if physical_network in allocations:
+                    for alloc in allocations[physical_network]:
+                        try:
+                            # see if segmentation id is allocatable
+                            segmentation_id = self.get_segmentation_id(alloc)
+                            ids.remove(segmentation_id)
+                        except KeyError:
+                            # it's not allocatable, so check if its allocated
+                            if not alloc.allocated:
+                                # it's not, so remove it from table
+                                LOG.debug("Removing %(type)s %(id)s on "
+                                          "physical network "
+                                          "%(physical_network)s from pool",
+                                          {'type': self.get_type(),
+                                           'id': segmentation_id,
+                                           'physical_network':
+                                           physical_network})
+                                session.delete(alloc)
+                    del allocations[physical_network]
+
+                # add missing allocatable segments to table
+                for segmentation_id in sorted(ids):
+                    res = {'physical_network': physical_network,
+                           self.segmentation_key: segmentation_id,
+                           'allocated': False}
+                    alloc = self.model(**res)
+                    session.add(alloc)
+
+            # remove from table unallocated segmentation ids for any
+            # unconfigured physical networks
+            for allocs in allocations.itervalues():
+                for alloc in allocs:
+                    if not alloc.allocated:
+                        segmentation_id = self.get_segmentation_id(alloc)
+                        LOG.debug("Removing %(type)s %(id)s on physical "
+                                  "network %(physical_network)s from pool",
+                                  {'type': self.get_type(),
+                                   'id': segmentation_id,
+                                   'physical_network':
+                                   alloc.physical_network})
+                        session.delete(alloc)
+
+    def initialize(self):
+        self._sync_allocations()
+
+    def is_partial_segment(self, segment):
+        return segment.get(api.SEGMENTATION_ID) is None
+
+    def get_segmentation_id(self, data):
+        """
+        Return the column data from the database object using the field name
+        supplied by get_segmentation_key()
+        """
+        return getattr(data, self.get_segmentation_key())
+
+    def select_allocation(self, allocations):
+        """Select a segment allocation from a set of available free segments.
+        This is currently overridden from the default behaviour because some of
+        our lab deployments depend on sequential allocations.
+        """
+        return allocations[0]
+
+    def build_segment_query(self, session, **filters):
+        """Enforces that segments are allocated from provider network
+        segmentation ranges that are owned by the tenant, and then from shared
+        ranges, but never from ranges owned by other tenants.  This method also
+        enforces that other provider attributes are used when constraining the
+        set of possible segments to be used.
+        """
+        tenant_id = filters.pop('tenant_id', None)
+        columns = set(dict(self.model.__table__.columns))
+        model_filters = dict((k, filters[k])
+                             for k in columns & set(filters.keys()))
+        query = (session.query(self.model)
+                 .filter_by(**model_filters)
+                 .join(pnet_db.ProviderNet,
+                       and_(self.model.physical_network ==
+                            pnet_db.ProviderNet.name,
+                            pnet_db.ProviderNet.type == self.get_type()))
+                 .join(pnet_db.ProviderNetRange,
+                       and_(pnet_db.ProviderNet.id ==
+                            pnet_db.ProviderNetRange.providernet_id,
+                            self.model_key >=
+                            pnet_db.ProviderNetRange.minimum,
+                            self.model_key <=
+                            pnet_db.ProviderNetRange.maximum))
+                 .filter(or_(pnet_db.ProviderNetRange.tenant_id == tenant_id,
+                             (pnet_db.ProviderNetRange.shared ==
+                              sql.expression.true())))
+                 .order_by(asc(pnet_db.ProviderNetRange.shared),
+                           asc(self.model_key)))
+        return query
+
+    def validate_provider_segment(self, segment, context=None):
+        physical_network = segment.get(api.PHYSICAL_NETWORK)
+        segmentation_id = segment.get(api.SEGMENTATION_ID)
+        if physical_network:
+            if not self._providernet_exists(context, physical_network):
+                msg = (_("physical_network '%(physical_network)s' unknown "
+                         " for %(type)s provider network"),
+                       {'physical_network': physical_network,
+                        'type': self.get_type()})
+                raise exc.InvalidInput(error_message=msg)
+            if segmentation_id:
+                if not self.is_valid_segmentation_id(segmentation_id):
+                    msg = (_("segmentation_id out of range (%(min)s through "
+                             "%(max)s)") %
+                           {'min': self.get_min_id(),
+                            'max': self.get_max_id()})
+                    raise exc.InvalidInput(error_message=msg)
+        elif segmentation_id:
+            msg = (_("segmentation_id requires physical_network for %(type)s "
+                     "provider network"), {'type': self.get_type()})
+            raise exc.InvalidInput(error_message=msg)
+
+        for key, value in segment.items():
+            if value and key not in [api.NETWORK_TYPE,
+                                     api.PHYSICAL_NETWORK,
+                                     api.SEGMENTATION_ID]:
+                msg = (_("%(key)s prohibited for %(type)s provider network"),
+                       {'key': key, 'type': self.get_type()})
+                raise exc.InvalidInput(error_message=msg)
+
+    def reserve_provider_segment(self, context, segment, **filters):
+        physical_network = segment.get(api.PHYSICAL_NETWORK)
+        if physical_network is not None:
+            filters['physical_network'] = physical_network
+            segmentation_id = segment.get(api.SEGMENTATION_ID)
+            if segmentation_id is not None:
+                filters[self.segmentation_key] = segmentation_id
+
+        if self.is_partial_segment(segment):
+            alloc = self.allocate_partially_specified_segment(
+                context, **filters)
+            if not alloc:
+                raise exc.NoNetworkAvailable()
+        else:
+            alloc = self.allocate_fully_specified_segment(
+                context, **filters)
+            if not alloc:
+                filters['id'] = filters[self.segmentation_key]
+                del filters[self.segmentation_key]
+                raise n_exc.SegmentationIdInUse(**filters)
+
+        segmentation_id = getattr(alloc, self.segmentation_key)
+        return {api.NETWORK_TYPE: self.get_type(),
+                api.PHYSICAL_NETWORK: alloc.physical_network,
+                api.SEGMENTATION_ID: segmentation_id}
+
+    def allocate_tenant_segment(self, context, **filters):
+        alloc = self.allocate_partially_specified_segment(context, **filters)
+        if not alloc:
+            self._sync_allocations(context.session)
+            return
+        segmentation_id = getattr(alloc, self.segmentation_key)
+        return {api.NETWORK_TYPE: self.get_type(),
+                api.PHYSICAL_NETWORK: alloc.physical_network,
+                api.SEGMENTATION_ID: segmentation_id}
+
+    def release_segment(self, context, segment):
+        session = context.session
+        physical_network = segment[api.PHYSICAL_NETWORK]
+        segmentation_id = segment[api.SEGMENTATION_ID]
+
+        ranges = self._get_providernet_ranges(session)
+        ranges = ranges.get(physical_network, [])
+        inside = any(lo <= segmentation_id <= hi for lo, hi in ranges)
+        with session.begin(subtransactions=True):
+            query = (session.query(self.model).
+                     filter_by(**{'physical_network': physical_network,
+                                  self.segmentation_key: segmentation_id}))
+            if inside:
+                count = query.update({"allocated": False})
+                if count:
+                    LOG.debug("Releasing %(type) %(id)s on physical "
+                              "network %(physical_network)s to pool",
+                              {'type': self.get_type(),
+                               'id': segmentation_id,
+                               'physical_network': physical_network})
+            else:
+                count = query.delete()
+                if count:
+                    LOG.debug("Releasing %(type) %(id)s on physical "
+                              "network %(physical_network)s outside pool",
+                              {'type': self.get_type(),
+                               'id': segmentation_id,
+                               'physical_network': physical_network})
+
+        if not count:
+            LOG.warning("No %(type)s %(id)s found on physical "
+                        "network %(physical_network)s",
+                        {'type': self.get_type(),
+                         'id': segmentation_id,
+                         'physical_network': physical_network})
+
+    def update_provider_allocations(self, context):
+        self._sync_allocations(context.session)
diff --git a/neutron/plugins/wrs/drivers/type_managed_flat.py b/neutron/plugins/wrs/drivers/type_managed_flat.py
new file mode 100644
index 0000000..b7ff1d2
--- /dev/null
+++ b/neutron/plugins/wrs/drivers/type_managed_flat.py
@@ -0,0 +1,70 @@
+# Copyright (c) 2013-2014 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import six
+
+from neutron_lib import exceptions as exc
+from oslo_log import log as logging
+
+from neutron.db import api as db_api
+from neutron.plugins.ml2 import driver_api as api
+from neutron.plugins.ml2.drivers import type_flat
+from neutron.plugins.wrs.drivers import type_generic
+
+LOG = logging.getLogger(__name__)
+
+
+class ManagedFlatTypeDriver(type_flat.FlatTypeDriver,
+                            type_generic.GenericProvidernetTypeDriverMixin):
+
+    def __init__(self):
+        super(ManagedFlatTypeDriver, self).__init__()
+
+    def get_mtu(self, physical_network):
+        session = db_api.get_current_session()
+        providernet = self._get_providernet(session, physical_network)
+        return providernet.mtu
+
+    def _parse_networks(self, entries):
+        # Noop this method so that the parent class initializer does not
+        # update the physical networks
+        self.flat_networks = []
+
+    def validate_provider_segment(self, segment, context=None):
+        physical_network = segment.get(api.PHYSICAL_NETWORK)
+        if not physical_network:
+            msg = ("physical_network required for flat provider network")
+            raise exc.InvalidInput(error_message=msg)
+        if not self._providernet_exists(context, physical_network):
+            msg = (("physical_network '%s' unknown for flat provider network")
+                   % physical_network)
+            raise exc.InvalidInput(error_message=msg)
+
+        for key, value in six.iteritems(segment):
+            if value and key not in [api.NETWORK_TYPE,
+                                     api.PHYSICAL_NETWORK]:
+                msg = ("%s prohibited for flat provider network") % key
+                raise exc.InvalidInput(error_message=msg)
+
+    def initialize(self):
+        LOG.info(("ML2 ManagedFlatTypeDriver initialization complete"))
diff --git a/neutron/plugins/wrs/drivers/type_managed_vlan.py b/neutron/plugins/wrs/drivers/type_managed_vlan.py
new file mode 100644
index 0000000..b42138a
--- /dev/null
+++ b/neutron/plugins/wrs/drivers/type_managed_vlan.py
@@ -0,0 +1,71 @@
+# Copyright (c) 2013-2014 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+from oslo_log import log as logging
+
+from neutron.common import constants as q_const
+from neutron.plugins.common import constants as p_const
+from neutron.plugins.common import utils
+
+from neutron.db.models.plugins.ml2 import vlanallocation as vlan_alloc_model
+from neutron.objects.plugins.ml2 import vlanallocation as vlanalloc
+from neutron.plugins.wrs.drivers import type_generic
+
+LOG = logging.getLogger(__name__)
+
+
+class ManagedVlanTypeDriver(type_generic.GenericRangeTypeDriver):
+    """The class is a refinement of the default VLAN type driver.
+
+    Its purpose is to allocate VLAN segments based on the enhanced provider
+    network extension.  The main difference being that VLAN segments are
+    allocated according to the tenant ownership rules defined by the
+    administrator rather than treating all possible segments as equal.
+    """
+
+    def __init__(self):
+        super(ManagedVlanTypeDriver, self).__init__(vlanalloc.VlanAllocation)
+        self.model_key = vlan_alloc_model.VlanAllocation.vlan_id
+        self.segmentation_key = "vlan_id"
+
+    def allow_dynamic_allocation(self):
+        return False
+
+    def get_type(self):
+        return p_const.TYPE_VLAN
+
+    def get_segmentation_key(self):
+        return "vlan_id"
+
+    def is_valid_segmentation_id(self, value):
+        return utils.is_valid_vlan_tag(value)
+
+    def get_min_id(self):
+        return q_const.MIN_VLAN_TAG
+
+    def get_max_id(self):
+        return q_const.MAX_VLAN_TAG
+
+    def initialize(self):
+        self._sync_allocations()
+        LOG.info(("ML2 ManagedVlanTypeDriver initialization complete"))
diff --git a/neutron/plugins/wrs/drivers/type_managed_vxlan.py b/neutron/plugins/wrs/drivers/type_managed_vxlan.py
new file mode 100644
index 0000000..3432ffb
--- /dev/null
+++ b/neutron/plugins/wrs/drivers/type_managed_vxlan.py
@@ -0,0 +1,92 @@
+# Copyright (c) 2013-2014 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+#
+# Copyright (c) 2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+from neutron_lib.db import model_base
+from oslo_log import log as logging
+import sqlalchemy as sa
+from sqlalchemy import sql
+
+from neutron.common import constants as q_const
+from neutron.plugins.common import constants as p_const
+from neutron.plugins.common import utils
+from neutron.plugins.wrs.drivers import type_generic
+
+LOG = logging.getLogger(__name__)
+
+
+class ManagedVxlanAllocation(model_base.BASEV2):
+    """
+    This class is a refinement of the ML2 VXLAN allocation table.  The ML2
+    version of the table does not include the physical network name which we
+    require for management of provider networks.  We cannot extend the
+    existing table directly because it would break existing unit tests.
+    """
+    __tablename__ = 'wrs_vxlan_allocations'
+
+    physical_network = sa.Column(sa.String(64), nullable=False,
+                                 primary_key=True)
+    vxlan_vni = sa.Column(sa.Integer, nullable=False, primary_key=True,
+                          autoincrement=False)
+    allocated = sa.Column(sa.Boolean, nullable=False, default=False,
+                          server_default=sql.false())
+
+
+class ManagedVxlanTypeDriver(type_generic.GenericRangeTypeDriver):
+    """
+    This class is a refinement of the default VXLAN type driver.
+
+    Its purpose is to allocate VXLAN segments based on the enhanced provider
+    network extension.  The main difference being that VXLAN segments are
+    allocated according to the tenant ownership rules defined by the
+    administrator rather than treating all possible segments as equal. It also
+    adds the physical_network attribute to all VXLAN segmentation id
+    allocations to support tracking unique VXLAN instances on a per provider
+    network basis.
+    """
+
+    def __init__(self):
+        super(ManagedVxlanTypeDriver, self).__init__(ManagedVxlanAllocation)
+        self.model_key = ManagedVxlanAllocation.vxlan_vni
+        self.segmentation_key = "vxlan_vni"
+
+    def allow_dynamic_allocation(self):
+        return False
+
+    def get_type(self):
+        return p_const.TYPE_VXLAN
+
+    def get_segmentation_key(self):
+        return "vxlan_vni"
+
+    def is_valid_segmentation_id(self, value):
+        return utils.is_valid_vxlan_vni(value)
+
+    def get_min_id(self):
+        return q_const.MIN_VXLAN_VNI
+
+    def get_max_id(self):
+        return q_const.MAX_VXLAN_VNI
+
+    def initialize(self):
+        self._sync_allocations()
+        LOG.info(("ML2 ManagedVxlanTypeDriver initialization complete"))
diff --git a/neutron/scheduler/dhcp_agent_scheduler.py b/neutron/scheduler/dhcp_agent_scheduler.py
index d4463ce..3cad9fc 100644
--- a/neutron/scheduler/dhcp_agent_scheduler.py
+++ b/neutron/scheduler/dhcp_agent_scheduler.py
@@ -116,8 +116,9 @@ class ChanceScheduler(base_scheduler.BaseChanceScheduler, AutoScheduler):
 
 class WeightScheduler(base_scheduler.BaseWeightScheduler, AutoScheduler):
 
-    def __init__(self):
-        super(WeightScheduler, self).__init__(DhcpFilter())
+    def __init__(self, dhcp_filter=None):
+        dhcp_filter = dhcp_filter or DhcpFilter()
+        super(WeightScheduler, self).__init__(dhcp_filter)
 
 
 class AZAwareWeightScheduler(WeightScheduler):
diff --git a/neutron/scheduler/dhcp_host_agent_scheduler.py b/neutron/scheduler/dhcp_host_agent_scheduler.py
index db75942..b6ab409 100644
--- a/neutron/scheduler/dhcp_host_agent_scheduler.py
+++ b/neutron/scheduler/dhcp_host_agent_scheduler.py
@@ -14,18 +14,24 @@
 #    under the License.
 
 #
-# Copyright (c) 2013-2014 Wind River Systems, Inc.
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
 #
 # The right to copy, distribute, modify, or otherwise make use
 # of this software may be licensed only pursuant to the terms
 # of an applicable Wind River license agreement.
 #
 
+from neutron_lib import constants
 from oslo_log import log as logging
+from sqlalchemy import sql
 
+from neutron.common import constants as n_const
+from neutron.db import hosts_db
+from neutron.db.models import agent as agent_model
+from neutron.db.models import segment as segments_model
+from neutron.db import models_v2
 from neutron.scheduler import dhcp_agent_scheduler
 
-
 LOG = logging.getLogger(__name__)
 
 
@@ -33,10 +39,26 @@ class HostBasedScheduler(dhcp_agent_scheduler.AZAwareWeightScheduler):
     """
     Allocate a DHCP agent for a network based on the least loaded DHCP agent.
 
-    This is a refinement of the default WeightScheduler.  Its purpose is to
-    only schedule networks to hosts that are attached to the provider networks
-    that are needed in order to correctly implement the router instance.
+    This is a refinement of the default AZAwareWeightScheduler.  Its purpose
+    is to block scheduling of networks onto hosts that are not yet available.
     """
+    def __init__(self):
+        super(HostBasedScheduler, self).__init__(HostDhcpFilter())
+
+    def get_dhcp_subnets_for_host(self, plugin, context, host, fields):
+        query = (context.session.query(models_v2.Subnet)
+                 .join(models_v2.Network,
+                       models_v2.Network.id == models_v2.Subnet.network_id)
+                 .join(segments_model.NetworkSegment,
+                       segments_model.NetworkSegment.network_id ==
+                       models_v2.Network.id)
+                 .join(segments_model.SegmentHostMapping,
+                       segments_model.SegmentHostMapping.segment_id ==
+                       segments_model.NetworkSegment.id)
+                 .filter(models_v2.Subnet.enable_dhcp == sql.expression.true())
+                 .filter(segments_model.SegmentHostMapping.host == host))
+        return [plugin._make_subnet_dict(s, fields=fields, context=context)
+                for s in query.all()]
 
     def auto_schedule_networks(self, plugin, context, host):
         if not plugin.is_host_available(context, host):
@@ -45,10 +67,25 @@ class HostBasedScheduler(dhcp_agent_scheduler.AZAwareWeightScheduler):
         return super(HostBasedScheduler, self).auto_schedule_networks(
             plugin, context, host)
 
-    def get_dhcp_subnets_for_host(self, plugin, context, host):
-        # TODO(alegacy): return an empty list until providernet code is merged
-        return []
 
-    def filter_agents(self, plugin, context, network):
-        # TODO(alegacy): return an empty list until providernet code is merged
-        return []
+class HostDhcpFilter(dhcp_agent_scheduler.DhcpFilter):
+
+    def _get_active_agents(self, plugin, context, az_hints):
+        """Return a list of active dhcp agents."""
+        with context.session.begin(subtransactions=True):
+            query = (context.session.query(agent_model.Agent)
+                     .join(hosts_db.Host,
+                           hosts_db.Host.name == agent_model.Agent.host)
+                     .filter(agent_model.Agent.agent_type ==
+                             constants.AGENT_TYPE_DHCP)
+                     .filter(agent_model.Agent.admin_state_up ==
+                             sql.expression.true())
+                     .filter(hosts_db.Host.availability == n_const.HOST_UP))
+            if az_hints:
+                query = query.filter(agent_model.Agent.availability_zone ==
+                                     az_hints)
+            active_dhcp_agents = query.all()
+            if not active_dhcp_agents:
+                LOG.warning(('No more active DHCP agents on active hosts'))
+                return []
+        return active_dhcp_agents
diff --git a/neutron/scheduler/l3_host_agent_scheduler.py b/neutron/scheduler/l3_host_agent_scheduler.py
index bd0216f..b9a8bf5 100644
--- a/neutron/scheduler/l3_host_agent_scheduler.py
+++ b/neutron/scheduler/l3_host_agent_scheduler.py
@@ -21,8 +21,23 @@
 # of an applicable Wind River license agreement.
 #
 
-from oslo_log import log as logging
+import datetime
 
+from neutron_lib import constants
+from oslo_config import cfg
+from oslo_log import log as logging
+from oslo_utils import timeutils
+from sqlalchemy import sql
+from sqlalchemy import and_, or_, func
+
+from neutron.common import constants as n_const
+from neutron.db import hosts_db
+from neutron.db.models import agent as agent_model
+from neutron.db.models import l3 as l3_model
+from neutron.db.models import l3agent as l3agent_model
+from neutron.db.models import segment as segment_model
+from neutron.db import models_v2
+from neutron.db import providernet_db as pnet_db
 from neutron.scheduler import l3_agent_scheduler
 
 
@@ -38,26 +53,147 @@ class HostBasedScheduler(l3_agent_scheduler.AZLeastRoutersScheduler):
     that are needed in order to correctly implement the router instance.
     """
 
-    def _get_routers_for_host(self, context, plugin, host):
-        # TODO(alegacy): return an empty list until providernet code is merged
-        return []
-
-    def _get_routers_can_schedule(self, context, plugin, routers, l3_agent):
+    def _get_oldest_acceptable_hearbeat_timestamp(self):
+        cutoff = timeutils.utcnow() - datetime.timedelta(
+            seconds=cfg.CONF.agent_down_time)
+        return cutoff
+
+    def _get_routers_for_host(self, plugin, context, host):
+        """Get the list of routers that can be scheduled to the specified
+        host.  This function takes in to consideration the networks that the
+        router is attached to and whether the host implements those provider
+        networks.
+
+        :param context: the context
+        :param plugin: the core plugin
+        :param host: the hostname of the target server
+        :returns: the list of router ids that can be scheduled on to the
+        specified host
+        """
+        # retrieve the host id
+        data = plugin.get_host_by_name(context, host)
+        host_id = data['id']
+        # query the list of routers that are attached to networks, that are
+        # implemented by provider networks that are associated to the
+        # specific host.
+        routers = (context.session.query(l3_model.Router.id)
+                   .join(models_v2.Port,
+                         or_(models_v2.Port.device_id ==
+                             l3_model.Router.id,
+                             models_v2.Port.id ==
+                             l3_model.Router.gw_port_id))
+                   .join(segment_model.NetworkSegment,
+                         (segment_model.NetworkSegment.network_id ==
+                          models_v2.Port.network_id))
+                   .join(pnet_db.ProviderNet,
+                         (pnet_db.ProviderNet.name ==
+                          segment_model.NetworkSegment.physical_network))
+                   .outerjoin(hosts_db.HostInterfaceProviderNetBinding,
+                              (hosts_db.HostInterfaceProviderNetBinding
+                               .providernet_id ==
+                               pnet_db.ProviderNet.id))
+                   .join(hosts_db.HostInterface,
+                         (hosts_db.HostInterface.id ==
+                          hosts_db.HostInterfaceProviderNetBinding.
+                          interface_id))
+                   .filter(hosts_db.HostInterface.host_id == host_id)
+                   .filter(hosts_db.HostInterface.network_type ==
+                           hosts_db.DATA_NETWORK)
+                   .outerjoin(l3agent_model.RouterL3AgentBinding,
+                              (l3agent_model.RouterL3AgentBinding
+                               .router_id == l3_model.Router.id)))
+        # Group by unique router-id values but exclude any that have a port
+        # without a provider network binding on the specified host
+        routers = (routers
+                   .group_by(l3_model.Router.id)
+                   .having(func.count(models_v2.Port.id) ==
+                           func.count(hosts_db.HostInterfaceProviderNetBinding.
+                                      providernet_id)))
+        return [entry[0] for entry in routers.all()]
+
+    def _get_routers_can_schedule(self, plugin, context, routers, l3_agent):
         """Filter the list of routers to remove those that are not supported
         by the specified host.
         """
         host = l3_agent['host']
-        router_ids = self._get_routers_for_host(context, plugin, host)
+        router_ids = self._get_routers_for_host(plugin, context, host)
         target_routers = [r for r in routers if r['id'] in router_ids]
         # Continue to the parent class and let it further refine the list of
         # target routers.
         return super(HostBasedScheduler, self)._get_routers_can_schedule(
-            context, plugin, target_routers, l3_agent)
+            plugin, context, target_routers, l3_agent)
 
     def _get_l3_agents_for_router(self, plugin, context, router_id,
                                   agent_id=None):
-        # TODO(alegacy): return an empty list until providernet code is merged
-        return []
+        """Query the list of agents that can support scheduling the specified
+        router.  To be considered as capable of scheduling the specified
+        router the list of networks that the router is attached to is used to
+        filter out agents on hosts that do not implement the the provider
+        networks that implement those networks.
+
+        :param context: the context
+        :param plugin: the core plugin
+        :param router_id: the router to be scheduled
+        :param agent_id: only consider one agent
+        :returns: the list of agents that can service the router
+        """
+        # Count the number of networks that this router is attached to.
+        networks = (context.session.query(models_v2.Port.network_id)
+                    .select_from(l3_model.Router)
+                    .join(models_v2.Port,
+                          or_(models_v2.Port.device_id ==
+                              l3_model.Router.id,
+                              models_v2.Port.id ==
+                              l3_model.Router.gw_port_id))
+                    .filter(l3_model.Router.id == router_id)
+                    .distinct(models_v2.Port.network_id))
+        count = networks.count()
+        if not count:
+            LOG.debug("router {} has no network attachments".format(router_id))
+            return []
+        # Find agents that are connected to the same set of networks as the
+        # specified router.
+        alive_cutoff = self._get_oldest_acceptable_hearbeat_timestamp()
+        agents = (context.session.query(agent_model.Agent)
+                  .join(hosts_db.Host,
+                        and_((hosts_db.Host.name == agent_model.Agent.host),
+                             (hosts_db.Host.availability ==
+                              n_const.HOST_UP)))
+                  .join(hosts_db.HostInterface,
+                        and_((hosts_db.HostInterface.network_type ==
+                              hosts_db.DATA_NETWORK),
+                             (hosts_db.HostInterface.host_id ==
+                              hosts_db.Host.id)))
+                  .join(hosts_db.HostInterfaceProviderNetBinding,
+                        (hosts_db.HostInterfaceProviderNetBinding.
+                         interface_id == hosts_db.HostInterface.id))
+                  .join(pnet_db.ProviderNet,
+                        (pnet_db.ProviderNet.id ==
+                         hosts_db.HostInterfaceProviderNetBinding.
+                         providernet_id))
+                  .join(segment_model.NetworkSegment,
+                        (segment_model.NetworkSegment.physical_network ==
+                         pnet_db.ProviderNet.name))
+                  .filter(segment_model.NetworkSegment
+                          .network_id.in_(networks.subquery()))
+                  .filter(agent_model.Agent
+                          .agent_type == constants.AGENT_TYPE_L3)
+                  .filter(agent_model.Agent
+                          .admin_state_up == sql.expression.true())
+                  .filter(agent_model.Agent.heartbeat_timestamp > alive_cutoff)
+                  )
+
+        if agent_id:
+            # Consider only the specified agent
+            agents = agents.filter(agent_model.Agent.id == agent_id)
+
+        # Group by agents and check that the agent has the same network
+        # attachments as is required by the router
+        agents = (agents
+                  .group_by(agent_model.Agent)
+                  .having(func.count(segment_model.NetworkSegment.network_id)
+                          == count))
+        return agents.all()
 
     def get_l3_agents_for_router(self, plugin, context, router_id):
         """See _get_l3_agents_for_router for a function description.
@@ -74,7 +210,7 @@ class HostBasedScheduler(l3_agent_scheduler.AZLeastRoutersScheduler):
         :returns: True if given L3 agent can host the given router id
         """
         return bool(self._get_l3_agents_for_router(
-                plugin, context, router_id, agent_id=agent_id))
+                    plugin, context, router_id, agent_id=agent_id))
 
     def auto_schedule_routers(self, plugin, context, host, router_ids,
                               exclude_distributed=False):
diff --git a/neutron/tests/etc/policy.json b/neutron/tests/etc/policy.json
index 792c43a..744ca82 100644
--- a/neutron/tests/etc/policy.json
+++ b/neutron/tests/etc/policy.json
@@ -69,6 +69,19 @@
     "update_segment": "rule:admin_only",
     "delete_segment": "rule:admin_only",
 
+    "get_providernet": "rule:admin_only",
+    "get_providernets": "rule:admin_only",
+    "create_providernet": "rule:admin_only",
+    "update_providernet": "rule:admin_only",
+    "delete_providernet": "rule:admin_only",
+    "get_providernet_range": "rule:admin_only",
+    "get_providernet_ranges": "rule:admin_only",
+    "create_providernet_range": "rule:admin_only",
+    "update_providernet_range": "rule:admin_only",
+    "delete_providernet_range": "rule:admin_only",
+    "get_providernet_types": "rule:admin_only",
+    "get_providernet-bindings": "rule:admin_only",
+
     "network_device": "field:port:device_owner=~^network:",
     "create_port": "",
     "create_port:device_owner": "not rule:network_device or rule:context_is_advsvc or rule:admin_or_network_owner",
diff --git a/neutron/tests/unit/db/test_db_base_plugin_v2.py b/neutron/tests/unit/db/test_db_base_plugin_v2.py
index 294eecb..9d9f943 100644
--- a/neutron/tests/unit/db/test_db_base_plugin_v2.py
+++ b/neutron/tests/unit/db/test_db_base_plugin_v2.py
@@ -309,7 +309,8 @@ class NeutronDbPluginV2TestCase(testlib_api.WebTestCase):
                      'availability_zone_hints') + (arg_list or ())):
             # Arg must be present
             if arg in kwargs:
-                data['network'][arg] = kwargs[arg]
+                new_arg = arg.replace('__', ':')
+                data['network'][new_arg] = kwargs[arg]
         network_req = self.new_create_request('networks', data, fmt)
         if set_context and tenant_id:
             # create a specific auth context for this request
diff --git a/neutron/tests/unit/plugins/ml2/_test_mech_agent.py b/neutron/tests/unit/plugins/ml2/_test_mech_agent.py
index 0118923..c5e76d4 100644
--- a/neutron/tests/unit/plugins/ml2/_test_mech_agent.py
+++ b/neutron/tests/unit/plugins/ml2/_test_mech_agent.py
@@ -307,3 +307,28 @@ class AgentMechanismGreTestCase(AgentMechanismBaseTestCase):
                                   self.GRE_SEGMENTS)
         self.driver.bind_port(context)
         self._check_unbound(context)
+
+
+class AgentMechanismVxlanTestCase(AgentMechanismBaseTestCase):
+    VXLAN_SEGMENTS = [{mech_api.ID: 'unknown_segment_id',
+                       mech_api.NETWORK_TYPE: 'no_such_type',
+                       mech_api.NETWORK_ID: 'fake_network_id'},
+                      {mech_api.ID: 'vlan_segment_id',
+                       mech_api.NETWORK_TYPE: 'vxlan',
+                       mech_api.PHYSICAL_NETWORK: 'fake_physical_network',
+                       mech_api.SEGMENTATION_ID: 1234,
+                       mech_api.NETWORK_ID: 'fake_network_id'}]
+
+    def test_type_vlan(self):
+        context = FakePortContext(self.AGENT_TYPE,
+                                  self.AGENTS,
+                                  self.VXLAN_SEGMENTS)
+        self.driver.bind_port(context)
+        self._check_bound(context, self.VXLAN_SEGMENTS[1])
+
+    def test_type_vlan_bad(self):
+        context = FakePortContext(self.AGENT_TYPE,
+                                  self.AGENTS_BAD,
+                                  self.VXLAN_SEGMENTS)
+        self.driver.bind_port(context)
+        self._check_unbound(context)
diff --git a/neutron/tests/unit/plugins/ml2/drivers/base_type_tunnel.py b/neutron/tests/unit/plugins/ml2/drivers/base_type_tunnel.py
index 1eed54e..d54ff1d 100644
--- a/neutron/tests/unit/plugins/ml2/drivers/base_type_tunnel.py
+++ b/neutron/tests/unit/plugins/ml2/drivers/base_type_tunnel.py
@@ -42,6 +42,7 @@ class TunnelTypeTestMixin(object):
 
     def setUp(self):
         super(TunnelTypeTestMixin, self).setUp()
+        self.plugin = mock.Mock()
         self.driver = self.DRIVER_CLASS()
         self.driver.tunnel_ranges = TUNNEL_RANGES
         self.driver.sync_allocations()
diff --git a/neutron/tests/unit/plugins/ml2/test_plugin.py b/neutron/tests/unit/plugins/ml2/test_plugin.py
index 5c85515..096d1f8 100644
--- a/neutron/tests/unit/plugins/ml2/test_plugin.py
+++ b/neutron/tests/unit/plugins/ml2/test_plugin.py
@@ -17,6 +17,7 @@ import functools
 
 import fixtures
 import mock
+import six
 import testtools
 import webob
 
@@ -314,7 +315,8 @@ class TestMl2NetworksV2(test_plugin.TestNetworksV2,
                 expected_segments = net[mpnet.SEGMENTS]
                 self.assertEqual(len(expected_segments), len(segments))
                 for expected, actual in zip(expected_segments, segments):
-                    self.assertEqual(expected, actual)
+                    for k, v in six.iteritems(expected):
+                        self.assertEqual(expected[k], actual[k])
 
     def _lookup_network_by_segmentation_id(self, seg_id, num_expected_nets):
         params_str = "%s=%s" % (pnet.SEGMENTATION_ID, seg_id)
@@ -355,7 +357,8 @@ class TestMl2NetworksV2(test_plugin.TestNetworksV2,
         expected_segments = self.mp_nets[0][mpnet.SEGMENTS]
         self.assertEqual(len(expected_segments), len(segments))
         for expected, actual in zip(expected_segments, segments):
-            self.assertEqual(expected, actual)
+            for k, v in six.iteritems(expected):
+                self.assertEqual(expected[k], actual[k])
 
     def test_create_network_segment_allocation_fails(self):
         plugin = directory.get_plugin()
@@ -2165,8 +2168,10 @@ class TestMultiSegmentNetworks(Ml2PluginV2TestCase):
         segment = {pnet.NETWORK_TYPE: None,
                    pnet.PHYSICAL_NETWORK: 'phys_net',
                    pnet.SEGMENTATION_ID: None}
+        mock_context = mock.Mock()
         with testtools.ExpectedException(exc.InvalidInput):
-            self.driver.type_manager._process_provider_create(segment)
+            self.driver.type_manager._process_provider_create(
+                mock_context, segment)
 
     def test_create_network_plugin(self):
         data = {'network': {'name': 'net1',
diff --git a/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_flat.py b/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_flat.py
new file mode 100644
index 0000000..20511d6
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_flat.py
@@ -0,0 +1,125 @@
+# Copyright 2013 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+from neutron_lib import exceptions as exc
+from oslo_log import log as logging
+
+from neutron.common import constants as n_const
+from neutron.common import exceptions as n_exc
+from neutron import context
+from neutron.db import api as db
+from neutron.db.models.plugins.ml2 import flatallocation
+from neutron.plugins.common import constants as p_const
+from neutron.plugins.ml2 import driver_api as api
+from neutron.plugins.wrs.drivers import type_managed_flat
+from neutron.tests.unit.plugins.wrs import test_extension_pnet as test_pnet
+from neutron.tests.unit.plugins.wrs import test_wrs_plugin
+
+LOG = logging.getLogger(__name__)
+
+
+FLAT_PNET1 = {'name': 'flat-pnet0',
+              'type': n_const.PROVIDERNET_FLAT,
+              'description': 'flat test provider network'}
+
+
+class ManagedFlatTypeDriverTestCase(test_pnet.ProvidernetTestCaseMixin,
+                                    test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setUp(self):
+        super(ManagedFlatTypeDriverTestCase, self).setUp()
+        self.context = context.get_admin_context()
+        self.driver = type_managed_flat.ManagedFlatTypeDriver()
+        self.session = db.get_session()
+        self._pnet1 = FLAT_PNET1
+
+    def tearDown(self):
+        super(ManagedFlatTypeDriverTestCase, self).tearDown()
+
+    def _get_allocation(self, context, segment):
+        session = context.session
+        return session.query(flatallocation.FlatAllocation).filter_by(
+            physical_network=segment[api.PHYSICAL_NETWORK]).first()
+
+    def test_validate_provider_segment(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT,
+                       api.PHYSICAL_NETWORK: pnet_data['name']}
+            self.driver.validate_provider_segment(segment, self.context)
+
+    def test_validate_provider_segment_with_missing_physical_network(self):
+        segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT}
+        self.assertRaises(exc.InvalidInput,
+                          self.driver.validate_provider_segment,
+                          segment, self.context)
+
+    def test_validate_provider_segment_with_unknown_physical_network(self):
+        segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT,
+                   api.PHYSICAL_NETWORK: 'unknown'}
+        self.assertRaises(exc.InvalidInput,
+                          self.driver.validate_provider_segment,
+                          segment, self.context)
+
+    def test_validate_provider_segment_with_unallowed_segmentation_id(self):
+        segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT,
+                   api.PHYSICAL_NETWORK: self._pnet1['name'],
+                   api.SEGMENTATION_ID: 1234}
+        self.assertRaises(exc.InvalidInput,
+                          self.driver.validate_provider_segment,
+                          segment, self.context)
+
+    def test_reserve_provider_segment(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT,
+                       api.PHYSICAL_NETWORK: pnet_data['name']}
+            observed = self.driver.reserve_provider_segment(
+                self.context, segment, tenant_id=self._tenant_id)
+            alloc = self._get_allocation(self.context, observed)
+            self.assertEqual(segment[api.PHYSICAL_NETWORK],
+                             alloc.physical_network)
+            self.driver.release_segment(self.context, observed)
+
+    def test_release_segment(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT,
+                       api.PHYSICAL_NETWORK: pnet_data['name']}
+            observed = self.driver.reserve_provider_segment(
+                self.context, segment, tenant_id=self._tenant_id)
+            alloc = self._get_allocation(self.context, segment)
+            self.assertIsNotNone(alloc)
+            self.driver.release_segment(self.context, observed)
+            alloc = self._get_allocation(self.context, segment)
+            self.assertIsNone(alloc)
+
+    def test_reserve_provider_segment_already_reserved(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: p_const.TYPE_FLAT,
+                       api.PHYSICAL_NETWORK: pnet_data['name']}
+            observed = self.driver.reserve_provider_segment(
+                self.context, segment, tenant_id=self._tenant_id)
+            self.assertRaises(n_exc.FlatNetworkInUse,
+                              self.driver.reserve_provider_segment,
+                              self.context, segment, tenant_id=self._tenant_id)
+            self.driver.release_segment(self.context, observed)
diff --git a/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vlan.py b/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vlan.py
new file mode 100644
index 0000000..8824bf0
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vlan.py
@@ -0,0 +1,215 @@
+# Copyright 2013 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+from testtools import matchers
+
+from neutron_lib import exceptions as exc
+from oslo_log import log as logging
+
+from neutron.common import constants
+from neutron.common import exceptions as n_exc
+from neutron import context
+from neutron.db import api as db
+from neutron.db.models.plugins.ml2 import vlanallocation as vlan_alloc_model
+from neutron.plugins.ml2 import driver_api as api
+from neutron.plugins.wrs.drivers import type_managed_vlan
+from neutron.tests.unit.db import test_db_base_plugin_v2
+from neutron.tests.unit.plugins.wrs import test_extension_pnet as test_pnet
+from neutron.tests.unit.plugins.wrs import test_wrs_plugin
+
+LOG = logging.getLogger(__name__)
+
+
+VLAN_PNET1 = {'name': 'vlan-pnet0',
+              'type': constants.PROVIDERNET_VLAN,
+              'description': 'vlan test provider network'}
+
+VLAN_PNET1_RANGE1 = {'name': 'vlan-pnet0-0',
+                     'description': 'vlan range1',
+                     'shared': False,
+                     'minimum': 10,
+                     'maximum': 100,
+                     'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+
+class ManagedVlanTypeDriverTestCase(test_pnet.ProvidernetTestCaseMixin,
+                                    test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setUp(self):
+        super(ManagedVlanTypeDriverTestCase, self).setUp()
+        self.context = context.get_admin_context()
+        self.driver = type_managed_vlan.ManagedVlanTypeDriver()
+        self.session = db.get_session()
+        self._pnet1 = VLAN_PNET1
+        self._pnet_range1 = VLAN_PNET1_RANGE1
+
+    def tearDown(self):
+        super(ManagedVlanTypeDriverTestCase, self).tearDown()
+
+    def _get_allocation(self, context, segment):
+        session = context.session
+        return session.query(vlan_alloc_model.VlanAllocation).filter_by(
+            physical_network=segment[api.PHYSICAL_NETWORK],
+            vlan_id=segment[api.SEGMENTATION_ID]).first()
+
+    def test_validate_provider_segment(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: pnet_data['type'],
+                       api.PHYSICAL_NETWORK: pnet_data['name'],
+                       api.SEGMENTATION_ID: 1}
+            self.assertIsNone(self.driver.validate_provider_segment(
+                segment, self.context))
+
+    def test_validate_provider_segment_without_segmentation_id(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: pnet_data['type'],
+                       api.PHYSICAL_NETWORK: pnet_data['name']}
+            self.driver.validate_provider_segment(segment, self.context)
+
+    def test_validate_provider_segment_without_physical_network(self):
+        segment = {api.NETWORK_TYPE: self._pnet1['type']}
+        self.driver.validate_provider_segment(segment, self.context)
+
+    def test_validate_provider_segment_with_missing_physical_network(self):
+        segment = {api.NETWORK_TYPE: self._pnet1['type'],
+                   api.SEGMENTATION_ID: 1}
+        self.assertRaises(exc.InvalidInput,
+                          self.driver.validate_provider_segment,
+                          segment, self.context)
+
+    def test_validate_provider_segment_with_invalid_physical_network(self):
+        with self.pnet(self._pnet1):
+            segment = {api.NETWORK_TYPE: self._pnet1['type'],
+                       api.PHYSICAL_NETWORK: 'invalid',
+                       api.SEGMENTATION_ID: 1}
+            self.assertRaises(exc.InvalidInput,
+                              self.driver.validate_provider_segment,
+                              segment, self.context)
+
+    def test_validate_provider_segment_with_invalid_segmentation_id(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: pnet_data['type'],
+                       api.PHYSICAL_NETWORK: pnet_data['name'],
+                       api.SEGMENTATION_ID: 2 ** 24}
+            self.assertRaises(exc.InvalidInput,
+                              self.driver.validate_provider_segment,
+                              segment, self.context)
+
+    def test_validate_provider_segment_with_invalid_input(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: pnet_data['type'],
+                       api.PHYSICAL_NETWORK: pnet_data['name'],
+                       api.SEGMENTATION_ID: 1,
+                       'invalid': 1}
+            self.assertRaises(exc.InvalidInput,
+                              self.driver.validate_provider_segment,
+                              segment, self.context)
+
+    def test_reserve_provider_segment(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, self._pnet_range1) as pnet_range:
+                data = pnet_range['providernet_range']
+                segment = {api.NETWORK_TYPE: pnet_data['type'],
+                           api.PHYSICAL_NETWORK: pnet_data['name'],
+                           api.SEGMENTATION_ID: data['minimum']}
+                alloc = self._get_allocation(self.context, segment)
+                self.assertFalse(alloc.allocated)
+                observed = self.driver.reserve_provider_segment(
+                    self.context, segment, tenant_id=self._tenant_id)
+                alloc = self._get_allocation(self.context, observed)
+                self.assertTrue(alloc.allocated)
+                self.driver.release_segment(self.context, observed)
+
+    def test_reserve_provider_segment_already_allocated(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, self._pnet_range1) as pnet_range:
+                data = pnet_range['providernet_range']
+                segment = {api.NETWORK_TYPE: pnet_data['type'],
+                           api.PHYSICAL_NETWORK: pnet_data['name'],
+                           api.SEGMENTATION_ID: data['minimum']}
+                observed = self.driver.reserve_provider_segment(
+                    self.context, segment, tenant_id=self._tenant_id)
+                self.assertRaises(n_exc.SegmentationIdInUse,
+                                  self.driver.reserve_provider_segment,
+                                  self.context,
+                                  observed,
+                                  tenant_id=self._tenant_id)
+                self.driver.release_segment(self.context, observed)
+
+    def test_reserve_provider_segment_without_segmentation_id(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, self._pnet_range1) as pnet_range:
+                data = pnet_range['providernet_range']
+                segment = {api.NETWORK_TYPE: pnet_data['type'],
+                           api.PHYSICAL_NETWORK: pnet_data['name']}
+                observed = self.driver.reserve_provider_segment(
+                    self.context, segment, tenant_id=self._tenant_id)
+                alloc = self._get_allocation(self.context, observed)
+                self.assertTrue(alloc.allocated)
+                self.driver.release_segment(self.context, observed)
+                vlan_id = observed[api.SEGMENTATION_ID]
+                self.assertThat(vlan_id,
+                                matchers.GreaterThan(data['minimum'] - 1))
+                self.assertThat(vlan_id,
+                                matchers.LessThan(data['maximum'] + 1))
+
+    def test_reserve_provider_segment_without_physical_network(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, self._pnet_range1) as pnet_range:
+                data = pnet_range['providernet_range']
+                segment = {api.NETWORK_TYPE: pnet_data['type']}
+                observed = self.driver.reserve_provider_segment(
+                    self.context, segment, tenant_id=self._tenant_id)
+                alloc = self._get_allocation(self.context, observed)
+                self.assertTrue(alloc.allocated)
+                vlan_id = observed[api.SEGMENTATION_ID]
+                self.assertEqual(alloc.physical_network, pnet_data['name'])
+                self.assertThat(vlan_id,
+                                matchers.GreaterThan(data['minimum'] - 1))
+                self.assertThat(vlan_id,
+                                matchers.LessThan(data['maximum'] + 1))
+                self.driver.release_segment(self.context, observed)
+
+    def test_reserve_provider_segment_none_available(self):
+        with self.pnet(self._pnet1) as pnet:
+            pnet_data = pnet['providernet']
+            segment = {api.NETWORK_TYPE: pnet_data['type']}
+            self.assertRaises(exc.NoNetworkAvailable,
+                              self.driver.reserve_provider_segment,
+                              self.context,
+                              segment,
+                              tenant_id=self._tenant_id)
+
+    def test_reserve_provider_segment_none_created(self):
+        segment = {api.NETWORK_TYPE: self._pnet1['type']}
+        self.assertRaises(exc.NoNetworkAvailable,
+                          self.driver.reserve_provider_segment,
+                          self.context,
+                          segment,
+                          tenant_id=self._tenant_id)
diff --git a/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vxlan.py b/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vxlan.py
new file mode 100644
index 0000000..3fd6632
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/drivers/test_type_managed_vxlan.py
@@ -0,0 +1,68 @@
+# Copyright 2013 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+from oslo_log import log as logging
+
+from neutron.common import constants
+from neutron.plugins.ml2 import driver_api as api
+from neutron.plugins.wrs.drivers import type_managed_vxlan
+from neutron.tests.unit.db import test_db_base_plugin_v2
+from neutron.tests.unit.plugins.wrs.drivers import test_type_managed_vlan
+
+LOG = logging.getLogger(__name__)
+
+
+VXLAN_PNET1 = {'name': 'vxlan-pnet0',
+               'type': constants.PROVIDERNET_VXLAN,
+               'mtu': constants.DEFAULT_MTU - constants.VXLAN_MTU_OVERHEAD,
+               'description': 'vxlan test provider network'}
+
+VXLAN_PNET1_RANGE1 = {'name': 'vxlan-pnet0-0',
+                      'description': 'vxlan range1',
+                      'shared': False,
+                      'minimum': 10,
+                      'maximum': 100,
+                      'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID,
+                      'group': '239.0.0.1',
+                      'port': 8472,
+                      'ttl': 1}
+
+
+class ManagedVxlanTypeDriverTestCase(
+        test_type_managed_vlan.ManagedVlanTypeDriverTestCase):
+
+    def setUp(self):
+        super(ManagedVxlanTypeDriverTestCase, self).setUp()
+        self.driver = type_managed_vxlan.ManagedVxlanTypeDriver()
+        self._pnet1 = VXLAN_PNET1
+        self._pnet_range1 = VXLAN_PNET1_RANGE1
+
+    def tearDown(self):
+        super(ManagedVxlanTypeDriverTestCase, self).tearDown()
+
+    def _get_allocation(self, context, segment):
+        session = context.session
+        return (session.query(type_managed_vxlan.ManagedVxlanAllocation).
+                filter_by(physical_network=segment[api.PHYSICAL_NETWORK],
+                          vxlan_vni=segment[api.SEGMENTATION_ID]).first())
+
+    # There are no tests defined here because they are all reused from the
+    # parent class
diff --git a/neutron/tests/unit/plugins/wrs/test_agent_scheduler.py b/neutron/tests/unit/plugins/wrs/test_agent_scheduler.py
new file mode 100644
index 0000000..e907b43
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/test_agent_scheduler.py
@@ -0,0 +1,563 @@
+# Copyright (c) 2013 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import copy
+import uuid
+
+import six
+
+
+from neutron_lib import constants
+from oslo_log import log as logging
+from oslo_utils import timeutils
+
+from neutron.common import constants as n_const
+from neutron.common import topics
+from neutron.db import agents_db
+from neutron.extensions import wrs_net
+from neutron.plugins.ml2 import config
+from neutron.tests.unit.db import test_db_base_plugin_v2
+from neutron.tests.unit.extensions import test_agent
+from neutron.tests.unit.extensions import test_l3
+from neutron.tests.unit.plugins.wrs import test_extension_host
+from neutron.tests.unit.plugins.wrs import test_extension_pnet
+from neutron.tests.unit.plugins.wrs import test_wrs_plugin
+from neutron_lib.plugins import constants as plugin_constants
+from neutron_lib.plugins import directory
+
+LOG = logging.getLogger(__name__)
+
+HOST1 = {'name': 'compute-0',
+         'id': '065aa1d1-84ed-4d59-a777-16b0ea8a5640',
+         'availability': n_const.HOST_DOWN}
+
+HOST2 = {'name': 'compute-1',
+         'id': '28c25767-e6e7-49c3-9735-2ef5ff04c4a2',
+         'availability': n_const.HOST_DOWN}
+
+HOST3 = {'name': 'compute-2',
+         'id': 'c947cbd0-f59a-4ab1-b0c6-1e12bd4846ab',
+         'availability': n_const.HOST_DOWN}
+
+HOST4 = {'name': 'compute-3',
+         'id': '89bfbe7a-c416-4c32-ae65-bc390fa0a908',
+         'availability': n_const.HOST_DOWN}
+
+HOSTS = (HOST1, HOST2, HOST3, HOST4)
+
+PNET1 = {'name': 'vlan-pnet0',
+         'type': n_const.PROVIDERNET_VLAN,
+         'description': 'vlan test provider network'}
+
+PNET2 = {'name': 'vlan-pnet1',
+         'type': n_const.PROVIDERNET_VLAN,
+         'description': 'vlan test provider network'}
+
+PNET3 = {'name': 'flat-pnet0',
+         'type': n_const.PROVIDERNET_FLAT,
+         'description': 'flat test provider network'}
+
+# PNET4 should not be bound to a compute node
+PNET4 = {'name': 'flat-pnet1',
+         'type': n_const.PROVIDERNET_FLAT,
+         'description': 'flat test provider network'}
+
+PNET5 = {'name': 'flat-sriov-pnet1',
+         'type': n_const.PROVIDERNET_FLAT,
+         'description': 'flat test provider network for sriov networks'}
+
+PNETS = (PNET1, PNET2, PNET3, PNET4, PNET5)
+
+PNET1_RANGE1 = {'name': 'vlan-pnet0-0',
+                'description': 'vlan range1',
+                'shared': False,
+                'minimum': 1,
+                'maximum': 100,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET2_RANGE1 = {'name': 'vlan-pnet1-0',
+                'description': 'vlan range1',
+                'shared': False,
+                'minimum': 101,
+                'maximum': 200,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET_RANGES = {'vlan-pnet0': [PNET1_RANGE1],
+               'vlan-pnet1': [PNET2_RANGE1]}
+
+PNET_BINDINGS = {'compute-0': ['vlan-pnet0', 'vlan-pnet1', 'flat-pnet0'],
+                 'compute-1': ['vlan-pnet0', 'vlan-pnet1', 'flat-pnet0'],
+                 'compute-2': ['vlan-pnet0', 'vlan-pnet1'],
+                 'compute-3': ['flat-sriov-pnet1']}
+
+INTERFACE1 = {'uuid': str(uuid.uuid4()),
+              'mtu': n_const.DEFAULT_MTU,
+              'vlans': '',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-0'])}
+
+INTERFACE2 = {'uuid': str(uuid.uuid4()),
+              'mtu': n_const.DEFAULT_MTU,
+              'vlans': '4001,,4002, 4003',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-1'])}
+
+INTERFACE3 = {'uuid': str(uuid.uuid4()),
+              'mtu': n_const.DEFAULT_MTU,
+              'vlans': '4001',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-2'])}
+
+INTERFACE4 = {'uuid': str(uuid.uuid4()),
+              'mtu': n_const.DEFAULT_MTU,
+              'vlans': '4001',
+              'network_type': 'pci-sriov',
+              'providernets': ','.join(PNET_BINDINGS['compute-3'])}
+
+INTERFACES = {'compute-0': INTERFACE1,
+              'compute-1': INTERFACE2,
+              'compute-2': INTERFACE3,
+              'compute-3': INTERFACE4}
+
+NET1 = {'name': 'tenant-net0',
+        'provider__physical_network': 'vlan-pnet0',
+        'provider__network_type': n_const.PROVIDERNET_VLAN}
+
+NET2 = {'name': 'tenant-net1',
+        'provider__physical_network': 'vlan-pnet1',
+        'provider__network_type': n_const.PROVIDERNET_VLAN}
+
+NET3 = {'name': 'external-net0',
+        'router__external': True,
+        'provider__physical_network': 'flat-pnet0',
+        'provider__network_type': n_const.PROVIDERNET_FLAT}
+
+NET4 = {'name': 'tenant-net2',
+        'provider__physical_network': 'flat-pnet1',
+        'provider__network_type': n_const.PROVIDERNET_FLAT}
+
+NET5 = {'name': 'tenant-net3',
+        'provider__physical_network': 'flat-sriov-pnet1',
+        'provider__network_type': n_const.PROVIDERNET_FLAT}
+
+NETS = (NET1, NET2, NET3, NET4, NET5)
+
+SUBNET1 = {'name': 'tenant-subnet0',
+           'cidr': '192.168.1.0/24',
+           'shared': False,
+           'enable_dhcp': True,
+           'gateway': '192.168.1.1'}
+
+SUBNET2 = {'name': 'tenant-subnet1',
+           'cidr': '192.168.2.0/24',
+           'shared': False,
+           'enable_dhcp': True,
+           'gateway': '192.168.2.1'}
+
+SUBNET3 = {'name': 'external-subnet0',
+           'cidr': '192.168.3.0/24',
+           'shared': True,
+           'enable_dhcp': False,
+           'gateway': '192.168.3.1'}
+
+SUBNET4 = {'name': 'tenant-subnet3',
+           'cidr': '192.168.4.0/24',
+           'shared': False,
+           'enable_dhcp': True,
+           'gateway': '192.168.4.1'}
+
+SUBNET5 = {'name': 'tenant-subnet4',
+           'cidr': '192.168.5.0/24',
+           'shared': False,
+           'enable_dhcp': True,
+           'gateway': '192.168.5.1'}
+
+SUBNETS = {'tenant-net0': [SUBNET1],
+           'tenant-net1': [SUBNET2],
+           'external-net0': [SUBNET3],
+           'tenant-net2': [SUBNET4],
+           'tenant-net3': [SUBNET5]}
+
+L3_AGENT_TEMPLATE = {
+    'binary': 'neutron-l3-agent',
+    'host': 'TBD',
+    'topic': topics.L3_AGENT,
+    'admin_state_up': True,
+    'configurations': {'use_namespaces': True,
+                       'router_id': None,
+                       'handle_internal_only_routers': True,
+                       'gateway_external_network_id': None,
+                       'interface_driver': 'interface_driver',
+                       },
+    'agent_type': constants.AGENT_TYPE_L3}
+
+DHCP_AGENT_TEMPLATE = {
+    'binary': 'neutron-dhcp-agent',
+    'host': 'TBD',
+    'topic': topics.DHCP_AGENT,
+    'admin_state_up': True,
+    'configurations': {'dhcp_driver': 'dhcp_driver',
+                       'use_namespaces': True,
+                       },
+    'agent_type': constants.AGENT_TYPE_DHCP}
+
+
+class FakeAgent(object):
+    def __init__(self, **kwargs):
+        self.__dict__.update(kwargs)
+
+    def __getitem__(self, k):
+        return self.__dict__.get(k)
+
+
+class WrsAgentSchedulerTestCase(test_extension_pnet.ProvidernetTestCaseMixin,
+                                test_extension_host.HostTestCaseMixin,
+                                test_l3.L3NatTestCaseMixin,
+                                test_agent.AgentDBTestMixIn,
+                                test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setup_config(self):
+        super(WrsAgentSchedulerTestCase, self).setup_config()
+        # Instantiate a fake host driver to allow us to control the host to
+        # provider network mappings
+        config.cfg.CONF.set_override('host_driver',
+                                     'neutron.tests.unit.plugins.wrs.'
+                                     'test_host_driver.TestHostDriver')
+
+    def setUp(self, plugin=None, ext_mgr=None):
+        self._hosts = {}
+        self._pnets = {}
+        self._pnet_ranges = {}
+        self._nets = {}
+        self._subnets = {}
+        self._dhcp_agents = {}
+        self._l3_agents = {}
+        super(WrsAgentSchedulerTestCase, self).setUp()
+        self._plugin = directory.get_plugin()
+        self._l3_plugin = directory.get_plugin(plugin_constants.L3)
+        self._l3_scheduler = self._l3_plugin.router_scheduler
+        self._dhcp_scheduler = self._plugin.network_scheduler
+        self._host_driver = self._plugin.host_driver
+        self._l3_plugin.agent_notifiers = {}
+        self._plugin.agent_notifiers = {}
+        self._prepare_test_dependencies(hosts=HOSTS,
+                                        providernets=PNETS,
+                                        providernet_ranges=PNET_RANGES,
+                                        interfaces=INTERFACES,
+                                        networks=NETS,
+                                        subnets=SUBNETS)
+
+    def tearDown(self):
+        self._cleanup_test_dependencies()
+        super(WrsAgentSchedulerTestCase, self).tearDown()
+
+    def _get_subnet_id(self, name):
+        return self._subnets[name]['id']
+
+    def _get_net_id(self, name):
+        return self._nets[name]['id']
+
+    def _get_network(self, name):
+        return self._nets[name]
+
+    def _get_host_id(self, name):
+        return self._hosts[name]['id']
+
+    def _lock_test_host(self, id):
+        body = {'availability': n_const.HOST_DOWN}
+        data = self._update_host(id, body)
+        self.assertEqual(data['host']['availability'],
+                         n_const.HOST_DOWN)
+
+    def _lock_test_hosts(self, hosts=HOSTS):
+        for host in hosts:
+            self._lock_test_host(host['id'])
+
+    def _query_router_host(self, id):
+        router = self._l3_plugin.get_router(self.adminContext, id)
+        self.assertIsNotNone(router)
+        return router[wrs_net.HOST]
+
+    def _create_subnets_for_network(self, data, subnets):
+        network = data['network']
+        for subnet in subnets[network['name']]:
+            arg_list = ('enable_dhcp')
+            args = dict((k, v) for k, v in six.iteritems(subnet)
+                        if k in arg_list)
+            data = self._make_subnet(self.fmt, data,
+                                     subnet['gateway'],
+                                     subnet['cidr'], **args)
+            self._subnets[subnet['name']] = data['subnet']
+
+    def _create_test_networks(self, networks, subnets):
+        for net in networks:
+            arg_list = ('provider__physical_network',
+                        'provider__network_type',
+                        'provider__segmentation_id',
+                        'router__external')
+            args = dict((k, v) for k, v in six.iteritems(net)
+                        if k in arg_list)
+            data = self._make_network(self.fmt,
+                                      name=net['name'],
+                                      admin_state_up=True,
+                                      arg_list=arg_list,
+                                      **args)
+            self._nets[net['name']] = data['network']
+            self._create_subnets_for_network(data, subnets)
+
+    def _delete_test_networks(self):
+        for name, data in six.iteritems(self._nets):
+            self._delete('networks', data['id'])
+        self._nets = []
+
+    def _register_dhcp_agent(self, hostname):
+        agent = copy.deepcopy(DHCP_AGENT_TEMPLATE)
+        agent['host'] = hostname
+        callback = agents_db.AgentExtRpcCallback()
+        callback.report_state(self.adminContext,
+                              agent_state={'agent_state': agent},
+                              time=timeutils.utcnow().isoformat())
+        self._dhcp_agents[hostname] = FakeAgent(**agent)
+
+    def _register_dhcp_agents(self, hosts):
+        for host in HOSTS:
+            self._register_dhcp_agent(host['name'])
+
+    def _register_l3_agent(self, hostname):
+        agent = copy.deepcopy(L3_AGENT_TEMPLATE)
+        agent['host'] = hostname
+        callback = agents_db.AgentExtRpcCallback()
+        callback.report_state(self.adminContext,
+                              agent_state={'agent_state': agent},
+                              time=timeutils.utcnow().isoformat())
+        self._l3_agents[hostname] = FakeAgent(**agent)
+
+    def _register_l3_agents(self, hosts):
+        for host in HOSTS:
+            self._register_l3_agent(host['name'])
+
+    def _list_dhcp_agents(self):
+        return self._list_agents(query_string='binary=neutron-dhcp-agent')
+
+    def _list_l3_agents(self):
+        return self._list_agents(query_string='binary=neutron-l3-agent')
+
+    def _prepare_test_dependencies(self, hosts, providernets,
+                                   providernet_ranges, interfaces,
+                                   networks, subnets):
+        super(WrsAgentSchedulerTestCase, self)._prepare_test_dependencies(
+            hosts=hosts, providernets=providernets,
+            providernet_ranges=providernet_ranges,
+            interfaces=interfaces)
+        self._create_test_networks(networks=networks, subnets=subnets)
+
+    def _cleanup_test_dependencies(self):
+        self._delete_test_networks()
+        super(WrsAgentSchedulerTestCase, self)._cleanup_test_dependencies()
+
+
+class WrsL3AgentSchedulerTestCase(WrsAgentSchedulerTestCase):
+
+    def test_router_without_interfaces(self):
+        self._register_l3_agents(HOSTS)
+        data = self._list_agents()
+        self.assertEqual(len(data['agents']), len(HOSTS))
+        with self.router(name='router1',
+                         tenant_id=self._tenant_id) as r1:
+            # Check that it has no candidate hosts
+            agents = self._l3_scheduler.get_l3_agents_for_router(
+                self._plugin, self.adminContext, r1['router']['id'])
+            self.assertEqual(len(agents), 0)
+
+    def test_router_with_isolated_host(self):
+        self._register_l3_agents(HOSTS)
+        data = self._list_agents()
+        self.assertEqual(len(data['agents']), len(HOSTS))
+        with self.router(name='router1',
+                         tenant_id=self._tenant_id) as r1:
+            # Attach it to an external network
+            self._add_external_gateway_to_router(
+                r1['router']['id'], self._get_net_id(NET3['name']))
+            # Check that it has only 2 of 3 candidate hosts
+            agents = self._l3_scheduler.get_l3_agents_for_router(
+                self._plugin, self.adminContext, r1['router']['id'])
+            self.assertEqual(len(agents), 2)
+            # Confirm that the 1st host can support this router
+            routers = self._l3_scheduler._get_routers_can_schedule(
+                self._l3_plugin, self.adminContext, [r1['router']],
+                self._l3_agents[HOST1['name']])
+            self.assertEqual(len(routers), 1)
+            # Confirm that the 2st host can support this router
+            routers = self._l3_scheduler._get_routers_can_schedule(
+                self._l3_plugin, self.adminContext, [r1['router']],
+                self._l3_agents[HOST2['name']])
+            self.assertEqual(len(routers), 1)
+            # Confirm that the 3rd host cannot support this router
+            routers = self._l3_scheduler._get_routers_can_schedule(
+                self._l3_plugin, self.adminContext, [r1['router']],
+                self._l3_agents[HOST3['name']])
+            self.assertEqual(len(routers or []), 0)
+            # Remove the attachment
+            self._remove_external_gateway_from_router(
+                r1['router']['id'], self._get_net_id(NET3['name']))
+            # Check that it can no longer be scheduled
+            agents = self._l3_scheduler.get_l3_agents_for_router(
+                self._plugin, self.adminContext, r1['router']['id'])
+            self.assertEqual(len(agents), 0)
+
+    def test_router_with_multiple_interfaces(self):
+        self._register_l3_agents(HOSTS)
+        data = self._list_agents()
+        self.assertEqual(len(data['agents']), len(HOSTS))
+        with self.router(name='router1',
+                         tenant_id=self._tenant_id) as r1:
+            # Attach to 2 tenant networks
+            self._router_interface_action(
+                'add', r1['router']['id'],
+                self._get_subnet_id(SUBNET1['name']), None)
+            self._router_interface_action(
+                'add', r1['router']['id'],
+                self._get_subnet_id(SUBNET2['name']), None)
+            # Check that it has the first 3 hosts as candidates
+            agents = self._l3_scheduler.get_l3_agents_for_router(
+                self._plugin, self.adminContext, r1['router']['id'])
+            self.assertEqual(len(agents), 3)
+            # Attach it to an external network
+            self._add_external_gateway_to_router(
+                r1['router']['id'], self._get_net_id(NET3['name']))
+            # Check that it can now only be scheduled to 2 of 3 hosts
+            agents = self._l3_scheduler.get_l3_agents_for_router(
+                self._plugin, self.adminContext, r1['router']['id'])
+            self.assertEqual(len(agents), 2)
+            # Remove the attachments
+            self._remove_external_gateway_from_router(
+                r1['router']['id'], self._get_net_id(NET3['name']))
+            self._router_interface_action(
+                'remove', r1['router']['id'],
+                self._get_subnet_id(SUBNET1['name']), None)
+            self._router_interface_action(
+                'remove', r1['router']['id'],
+                self._get_subnet_id(SUBNET2['name']), None)
+
+    def test_router_rescheduled_on_locked_host(self):
+        self._register_l3_agents(HOSTS)
+        data = self._list_agents()
+        self.assertEqual(len(data['agents']), len(HOSTS))
+        with self.router(name='router1',
+                         tenant_id=self._tenant_id) as r1:
+            # Attach to 2 tenant networks
+            self._router_interface_action(
+                'add', r1['router']['id'],
+                self._get_subnet_id(SUBNET1['name']), None)
+            self._router_interface_action(
+                'add', r1['router']['id'],
+                self._get_subnet_id(SUBNET2['name']), None)
+            # Check that it has the first 3 hosts as candidates
+            agents = self._l3_scheduler.get_l3_agents_for_router(
+                self._plugin, self.adminContext, r1['router']['id'])
+            self.assertEqual(len(agents), 3)
+            # Check that it was assigned to one of them
+            original_host = self._query_router_host(r1['router']['id'])
+            self.assertIsNotNone(original_host)
+            self.assertIn(original_host, [h['name'] for h in HOSTS])
+            # Lock that host
+            self._lock_test_host(self._get_host_id(original_host))
+            # Check that it was assigned to a different host
+            current_host = self._query_router_host(r1['router']['id'])
+            self.assertIsNotNone(current_host)
+            self.assertIn(current_host, [h['name'] for h in HOSTS])
+            self.assertNotEqual(current_host, original_host)
+            # Remove the attachments
+            self._router_interface_action(
+                'remove', r1['router']['id'],
+                self._get_subnet_id(SUBNET1['name']), None)
+            self._router_interface_action(
+                'remove', r1['router']['id'],
+                self._get_subnet_id(SUBNET2['name']), None)
+
+
+# TODO(alegacy): these tests are disabled because the code was replaced
+# with similar upstream behaviour.  Need to determine if upstream test
+# coverage is sufficient or if these needs to be rewritten.
+#
+#class WrsDhcpAgentSchedulerTestCase(WrsAgentSchedulerTestCase):
+#
+#    def test_get_dhcp_networks_for_host_with_no_networks(self):
+#        # Check which dhcp networks can be scheduled on this host
+#        data = self._dhcp_scheduler.get_dhcp_networks_for_host(
+#            self._plugin, self.adminContext, HOST4['name'])
+#        ids = data.keys()
+#        # Should not be any networks available for this agent as HOST4 is
+#        # only associated with pci-sriov data interfaces and those interface
+#        # types are excluded by the scheduler
+#        self.assertEqual(len(ids), 0)
+#
+#    def test_get_dhcp_networks_for_host(self):
+#        # Check which dhcp networks can be scheduled on this host
+#        data = self._dhcp_scheduler.get_dhcp_networks_for_host(
+#            self._plugin, self.adminContext, HOST1['name'])
+#        ids = data.keys()
+#        # Should only be the first 2 networks that can be scheduled on this
+#        # host node.
+#        self.assertEqual(len(ids), 2)
+#
+#    def test_get_agents_for_network_without_agents(self):
+#        data = self._dhcp_scheduler.resource_filter._get_candidate_agents(
+#            self._plugin, self.adminContext,
+#            self._get_network(NET1['name']))
+#        # Should not have any candidate agents since there are no agents
+#        self.assertEqual(len(data), 0)
+#
+#    def test_get_agents_for_network(self):
+#        self._register_dhcp_agents(HOSTS)
+#        data = self._list_agents()
+#        self.assertEqual(len(data['agents']), len(HOSTS))
+#        # Get the list of agents that can support this network
+#        data = self._dhcp_scheduler.resource_filter._get_candidate_agents(
+#            self._plugin, self.adminContext,
+#            self._get_network(NET1['name']))
+#        # It should be schedulable on the first 3 nodes
+#        self.assertEqual(len(data), 3)
+#
+#    def test_get_agents_for_network_isolated(self):
+#        self._register_dhcp_agents(HOSTS)
+#        data = self._list_agents()
+#        self.assertEqual(len(data['agents']), len(HOSTS))
+#        # Get the list of agents that can support this network
+#        data = self._dhcp_scheduler.resource_filter._get_candidate_agents(
+#            self._plugin, self.adminContext,
+#            self._get_network(NET4['name']))
+#        # It should not be schedulable on any nodes
+#        self.assertEqual(len(data), 0)
+#
+#    def test_get_agents_for_network_sriov(self):
+#        self._register_dhcp_agents(HOSTS)
+#        data = self._list_agents()
+#        self.assertEqual(len(data['agents']), len(HOSTS))
+#        # Get the list of agents that can support this network
+#        data = self._dhcp_scheduler.resource_filter._get_candidate_agents(
+#            self._plugin, self.adminContext,
+#            self._get_network(NET5['name']))
+#        # It should not be schedulable on any nodes because NET5 is
+#        # associated only with pci-sriov data interfaces and the scheduler
+#        # should be excluding these from the choices.
+#        self.assertEqual(len(data), 0)
diff --git a/neutron/tests/unit/plugins/wrs/test_extension_host.py b/neutron/tests/unit/plugins/wrs/test_extension_host.py
new file mode 100644
index 0000000..966d547
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/test_extension_host.py
@@ -0,0 +1,193 @@
+# Copyright 2013 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import contextlib
+import copy
+
+import six
+import webob.exc
+
+from oslo_log import log as logging
+
+from neutron.common import constants
+from neutron.tests.unit.plugins.wrs import test_wrs_plugin
+
+LOG = logging.getLogger(__name__)
+
+
+HOST1 = {'name': 'compute-0',
+         'id': '065aa1d1-84ed-4d59-a777-16b0ea8a5640',
+         'availability': constants.HOST_UP}
+
+HOST2 = {'name': 'compute-1',
+         'id': '31df579d-d9ea-4623-a5a6-1bb0ccad22ef',
+         'availability': constants.HOST_DOWN}
+
+
+class HostTestCaseMixin(object):
+
+    def _update_host(self, id, body):
+        data = {'host': body}
+        request = self.new_update_request('hosts', data, id)
+        response = request.get_response(self.ext_api)
+        return self.deserialize(self.fmt, response)
+
+    def _bind_interface(self, id, body):
+        data = {'interface': body}
+        request = self.new_action_request('hosts', data, id,
+                                          'bind_interface')
+        return request.get_response(self.ext_api)
+
+    def _unbind_interface(self, id, body):
+        data = {'interface': body}
+        request = self.new_action_request('hosts', data, id,
+                                          'unbind_interface')
+        return request.get_response(self.ext_api)
+
+    def _create_host(self, host):
+        data = {'host': {'name': host['name'],
+                         'tenant_id': self._tenant_id}}
+        for arg in ('id', 'availability'):
+            data['host'][arg] = host[arg]
+        request = self.new_create_request('hosts', data)
+        return request.get_response(self.ext_api)
+
+    def _make_host(self, host):
+        response = self._create_host(host)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    def _make_interface(self, id, interface):
+        response = self._bind_interface(id, interface)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    def _delete_interface(self, id, interface):
+        response = self._unbind_interface(id, interface)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    @contextlib.contextmanager
+    def host(self, host, no_delete=False):
+        host = self._make_host(host)
+        try:
+            yield host
+        finally:
+            if not no_delete:
+                self._delete('hosts', host['host']['id'])
+
+    def _create_test_interfaces(self, interfaces):
+        self._interfaces = copy.deepcopy(interfaces)
+        for name, host in six.iteritems(self._hosts):
+            interface = self._interfaces.get(host['name'])
+            if not interface:
+                continue
+            # Add to "sysinv" first
+            self._host_driver.add_interface(host['name'], interface)
+            # Then, add to the plugin
+            self._make_interface(host['id'], interface)
+
+    def _delete_test_interfaces(self):
+        for name, host in six.iteritems(self._hosts):
+            interface = self._interfaces.get(host['name'])
+            if not interface:
+                continue
+            self._delete_interface(host['id'], interface)
+
+    def _create_test_hosts(self, hosts):
+        for host in hosts:
+            data = self._make_host(host)
+            self._hosts[host['name']] = data['host']
+            self._host_driver.add_host(data['host'])
+
+    def _delete_test_hosts(self):
+        for name, host in six.iteritems(self._hosts):
+            self._delete('hosts', host['id'])
+        self._hosts = []
+
+    def _get_pnet(self, name):
+        return self._pnets.get(name, None)
+
+    def _create_test_providernets(self, pnets, pnet_ranges):
+        for pnet in pnets:
+            data = self._make_pnet(pnet)
+            self._pnets[pnet['name']] = data['providernet']
+            # create segmentation ranges for each provider network
+            pnet_ranges.setdefault(pnet['name'], [])
+            for pnet_range in pnet_ranges[pnet['name']]:
+                data = self._make_pnet_range(data['providernet'], pnet_range)
+                self._pnet_ranges[pnet_range['name']] = data
+
+    def _delete_test_providernets(self):
+        for name, pnet in six.iteritems(self._pnets):
+            self._delete('wrs-provider/providernets', pnet['id'])
+        self._pnets = []
+
+    def _update_host_states(self):
+        for name, host in six.iteritems(self._hosts):
+            updates = {'availability': constants.HOST_UP}
+            data = self._update_host(host['id'], updates)
+            self._hosts[name] = data['host']
+
+    def _prepare_test_dependencies(self, hosts, providernets,
+                                   providernet_ranges, interfaces):
+        self._create_test_hosts(hosts)
+        self._create_test_providernets(providernets, providernet_ranges)
+        self._create_test_interfaces(interfaces)
+        self._update_host_states()
+
+    def _cleanup_test_dependencies(self):
+        self._delete_test_interfaces()
+        self._delete_test_hosts()
+        self._delete_test_providernets()
+
+
+class HostTestCase(HostTestCaseMixin,
+                   test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setUp(self, plugin=None, ext_mgr=None):
+        self.host1 = HOST1
+        self.host2 = HOST2
+        super(HostTestCase, self).setUp()
+
+    def tearDown(self):
+        super(HostTestCase, self).tearDown()
+
+    def test_create_host(self):
+        with self.host(self.host1) as host:
+            self.assertEqual(host['host']['name'], self.host1['name'])
+            self.assertIsNotNone(host['host']['id'])
+
+    def test_update_host(self):
+        with self.host(self.host1) as host:
+            self.assertEqual(host['host']['availability'],
+                             constants.HOST_UP)
+            data = {'host': {'availability': constants.HOST_DOWN}}
+            request = self.new_update_request('hosts', data,
+                                              host['host']['id'])
+            response = request.get_response(self.ext_api)
+            self.assertEqual(response.status_int, 200)
+            body = self.deserialize(self.fmt, response)
+            self.assertEqual(body['host']['availability'],
+                             constants.HOST_DOWN)
diff --git a/neutron/tests/unit/plugins/wrs/test_extension_interface.py b/neutron/tests/unit/plugins/wrs/test_extension_interface.py
new file mode 100644
index 0000000..ba526f4
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/test_extension_interface.py
@@ -0,0 +1,498 @@
+# Copyright (c) 2013 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import copy
+import uuid
+
+import webob.exc
+
+from oslo_log import log as logging
+
+from neutron.common import constants
+from neutron.plugins.ml2 import config
+from neutron.tests.unit.db import test_db_base_plugin_v2
+from neutron.tests.unit.plugins.wrs import test_extension_host
+from neutron.tests.unit.plugins.wrs import test_extension_pnet
+from neutron.tests.unit.plugins.wrs import test_wrs_plugin
+from neutron_lib import context
+from neutron_lib.plugins import constants as plugin_constants
+from neutron_lib.plugins import directory
+
+LOG = logging.getLogger(__name__)
+
+HOST1 = {'name': 'compute-0',
+         'id': '065aa1d1-84ed-4d59-a777-16b0ea8a5640',
+         'availability': constants.HOST_DOWN}
+
+HOST2 = {'name': 'compute-1',
+         'id': '28c25767-e6e7-49c3-9735-2ef5ff04c4a2',
+         'availability': constants.HOST_DOWN}
+
+HOST3 = {'name': 'compute-2',
+         'id': 'c947cbd0-f59a-4ab1-b0c6-1e12bd4846ab',
+         'availability': constants.HOST_DOWN}
+
+HOST4 = {'name': 'compute-3',
+         'id': '5e00774c-d132-4403-aa66-91d97ab6c6e6',
+         'availability': constants.HOST_DOWN}
+
+HOST5 = {'name': 'compute-4',
+         'id': 'cf82551c-b39f-49e1-8e25-fd946b57c697',
+         'availability': constants.HOST_DOWN}
+
+HOSTS = (HOST1, HOST2, HOST3, HOST4, HOST5)
+
+PNET1 = {'name': 'vlan-pnet1',
+         'type': constants.PROVIDERNET_VLAN,
+         'mtu': constants.DEFAULT_MTU,
+         'description': 'vlan test provider network'}
+
+PNET2 = {'name': 'vlan-pnet2',
+         'type': constants.PROVIDERNET_VLAN,
+         'mtu': constants.DEFAULT_MTU,
+         'description': 'vlan test provider network'}
+
+PNET3 = {'name': 'vlan-pnet3',
+         'type': constants.PROVIDERNET_VLAN,
+         'mtu': constants.DEFAULT_MTU,
+         'description': 'vlan test provider network'}
+
+PNET4 = {'name': 'vxlan-pnet1',
+         'type': constants.PROVIDERNET_VXLAN,
+         'mtu': constants.DEFAULT_MTU,
+         'description': 'vxlan test provider network'}
+
+PNET5 = {'name': 'vxlan-pnet2',
+         'type': constants.PROVIDERNET_VXLAN,
+         'mtu': constants.DEFAULT_MTU,
+         'description': 'vxlan test provider network'}
+
+PNET6 = {'name': 'flat-pnet1',
+         'type': constants.PROVIDERNET_FLAT,
+         'mtu': constants.DEFAULT_MTU,
+         'description': 'flat test provider network'}
+
+PNETS = (PNET1, PNET2, PNET3, PNET4, PNET5, PNET6)
+
+PNET1_RANGE1 = {'name': 'vlan-pnet1-0',
+                'description': 'vlan range1',
+                'shared': False,
+                'minimum': 1,
+                'maximum': 100,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET2_RANGE1 = {'name': 'vlan-pnet2-0',
+                'description': 'vlan range1',
+                'shared': False,
+                'minimum': 101,
+                'maximum': 200,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET3_RANGE1 = {'name': 'vlan-pnet3-0',
+                'description': 'vlan range1',
+                'shared': False,
+                'minimum': 101,
+                'maximum': 200,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET4_RANGE1 = {'name': 'vxlan-pnet1-0',
+                'description': 'vxlan range1',
+                'shared': False,
+                'minimum': 1,
+                'maximum': 1000,
+                'group': '239.0.0.1',
+                'port': 4789,
+                'ttl': 10,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET5_RANGE1 = {'name': 'vxlan-pnet2-0',
+                'description': 'vxlan range1',
+                'shared': False,
+                'minimum': 10000,
+                'maximum': 100010,
+                'group': '239.0.0.2',
+                'port': 8472,
+                'ttl': 1,
+                'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+PNET_RANGES = {'vlan-pnet1': [PNET1_RANGE1],
+               'vlan-pnet2': [PNET2_RANGE1],
+               'vlan-pnet3': [PNET3_RANGE1],
+               'vxlan-pnet1': [PNET4_RANGE1],
+               'vxlan-petn2': [PNET5_RANGE1]}
+
+PNET_BINDINGS = {'compute-0': ['vlan-pnet1', 'vlan-pnet2', 'vxlan-pnet1'],
+                 'compute-1': ['vlan-pnet1', 'vxlan-pnet1'],
+                 'compute-2': ['vlan-pnet1', 'vlan-pnet3', 'vxlan-pnet1'],
+                 'compute-3': ['flat-pnet1'],
+                 'compute-4': ['flat-pnet1']}
+
+INTERFACE1 = {'uuid': str(uuid.uuid4()),
+              'mtu': constants.DEFAULT_MTU + constants.VXLAN_MTU_OVERHEAD,
+              'vlans': '',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-0'])}
+
+INTERFACE2 = {'uuid': str(uuid.uuid4()),
+              'mtu': constants.DEFAULT_MTU + constants.VXLAN_MTU_OVERHEAD,
+              'vlans': '4001,,4002, 4003',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-1'])}
+
+INTERFACE3 = {'uuid': str(uuid.uuid4()),
+              'mtu': constants.DEFAULT_MTU + constants.VXLAN_MTU_OVERHEAD,
+              'vlans': '4001',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-2'])}
+
+INTERFACE4 = {'uuid': str(uuid.uuid4()),
+              'mtu': constants.DEFAULT_MTU,
+              'vlans': '',
+              'network_type': 'data',
+              'providernets': ','.join(PNET_BINDINGS['compute-3'])}
+
+INTERFACE5 = {'uuid': str(uuid.uuid4()),
+              'mtu': constants.DEFAULT_MTU,
+              'vlans': '',
+              'network_type': 'pci-passthrough',
+              'providernets': ','.join(PNET_BINDINGS['compute-4'])}
+
+INTERFACES = {'compute-0': INTERFACE1,
+              'compute-1': INTERFACE2,
+              'compute-2': INTERFACE3,
+              'compute-3': INTERFACE4,
+              'compute-4': INTERFACE5}
+
+
+class WrsHostInterfaceTestCase(test_extension_pnet.ProvidernetTestCaseMixin,
+                               test_extension_host.HostTestCaseMixin,
+                               test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setup_config(self):
+        super(WrsHostInterfaceTestCase, self).setup_config()
+        # Instantiate a fake host driver to allow us to control the host to
+        # provider network mappings
+        config.cfg.CONF.set_override('host_driver',
+                                     'neutron.tests.unit.plugins.wrs.'
+                                     'test_host_driver.TestHostDriver')
+
+    def setUp(self, plugin=None, ext_mgr=None):
+        self._hosts = {}
+        self._interfaces = {}
+        self._pnets = {}
+        self._pnet_ranges = {}
+        super(WrsHostInterfaceTestCase, self).setUp()
+        self._plugin = directory.get_plugin()
+        self._l3_plugin = directory.get_plugin(plugin_constants.L3)
+        self._host_driver = self._plugin.host_driver
+        self._prepare_test_dependencies(hosts=HOSTS,
+                                        providernets=PNETS,
+                                        providernet_ranges=PNET_RANGES,
+                                        interfaces=INTERFACES)
+
+    def tearDown(self):
+        self._cleanup_test_dependencies()
+        super(WrsHostInterfaceTestCase, self).tearDown()
+
+    def test_create_interface(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        data = self._make_interface(HOST1['id'], interface_data)
+        self.assertEqual(data['interface']['uuid'], interface_data['uuid'])
+        self.assertEqual(data['interface']['mtu'], interface_data['mtu'])
+
+    def test_create_interface_duplicate_providernet(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': PNET5['name'] + ',' + PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        data = self._make_interface(HOST1['id'], interface_data)
+        self.assertEqual(data['interface']['uuid'], interface_data['uuid'])
+        self.assertEqual(data['interface']['mtu'], interface_data['mtu'])
+
+    def test_update_vxlan_providernet_matches_link_mtu(self):
+        link_mtu = INTERFACE1['mtu']
+        link_mtu -= constants.VXLAN_MTU_OVERHEAD
+        data = {'providernet': {'mtu': link_mtu}}
+        pnet = self._get_pnet(PNET4['name'])
+        request = self.new_update_request('wrs-provider/providernets',
+                                          data, pnet['id'])
+        response = request.get_response(self.ext_api)
+        self.assertEqual(response.status_int, 200)
+        body = self.deserialize(self.fmt, response)
+        self.assertEqual(body['providernet']['mtu'], link_mtu)
+
+    def test_update_vxlan_providernet_exceeds_link_mtu(self):
+        data = {'mtu': INTERFACE1['mtu'] + 1}
+        pnet = self._get_pnet(PNET4['name'])
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._update_pnet,
+                          pnet['id'], data)
+
+    def test_update_vlan_providernet_matches_link_mtu(self):
+        link_mtu = INTERFACE1['mtu']
+        data = {'providernet': {'mtu': link_mtu}}
+        pnet = self._get_pnet(PNET1['name'])
+        request = self.new_update_request('wrs-provider/providernets',
+                                          data, pnet['id'])
+        response = request.get_response(self.ext_api)
+        self.assertEqual(response.status_int, 200)
+        body = self.deserialize(self.fmt, response)
+        self.assertEqual(body['providernet']['mtu'], link_mtu)
+
+    def test_update_vlan_providernet_exceeds_link_mtu(self):
+        data = {'mtu': INTERFACE1['mtu'] + 1}
+        pnet = self._get_pnet(PNET1['name'])
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._update_pnet,
+                          pnet['id'], data)
+
+    def test_create_interface_invalid_mtu(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': 'invalid',
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_small_mtu(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': 1,
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_large_mtu(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': 99999,
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_unknown_host(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        host_id = str(uuid.uuid4())
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          host_id, interface_data)
+
+    def test_create_interface_invalid_host(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': ''}
+        host_id = 'invalid'
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          host_id, interface_data)
+
+    def test_create_interface_unknown_providernet(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': 'unknown',
+                          'network_type': 'data',
+                          'vlans': ''}
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_invalid_vlans(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': 'invalid,invalid'}
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_out_of_range_vlans(self):
+        interface_data = {'uuid': str(uuid.uuid4()),
+                          'mtu': (constants.DEFAULT_MTU +
+                                  constants.VXLAN_MTU_OVERHEAD),
+                          'providernets': PNET5['name'],
+                          'network_type': 'data',
+                          'vlans': '0,5000'}
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+
+class WrsHostInterfaceProviderNetSemanticTestCase(WrsHostInterfaceTestCase):
+
+    def test_create_range_no_overlap(self):
+        range_data = {'name': 'no-overlap',
+                      'shared': True,
+                      'minimum': 2000,
+                      'maximum': 2010,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET1['name'])
+        with self.pnet_range(pnet, range_data) as pnet_range:
+            data = pnet_range['providernet_range']
+            self.assertEqual(data['name'], range_data['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['shared'], range_data['shared'])
+
+    def test_create_range_overlap_self(self):
+        range_data = {'name': 'self-overlap',
+                      'shared': True,
+                      'minimum': 20,
+                      'maximum': 30,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET1['name'])
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet_range,
+                          pnet, range_data)
+
+    def test_create_range_overlap_same_interface(self):
+        range_data = {'name': 'peer-overlap',
+                      'shared': True,
+                      'minimum': 120,
+                      'maximum': 130,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET1['name'])
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet_range,
+                          pnet, range_data)
+
+    def test_create_range_no_overlap_different_interface(self):
+        range_data = {'name': 'no-overlap',
+                      'shared': True,
+                      'minimum': 220,
+                      'maximum': 230,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET2['name'])
+        with self.pnet_range(pnet, range_data) as pnet_range:
+            data = pnet_range['providernet_range']
+            self.assertEqual(data['name'], range_data['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['shared'], range_data['shared'])
+
+    def test_create_range_overlap_different_type(self):
+        range_data = {'name': 'overlap',
+                      'shared': True,
+                      'minimum': 900,
+                      'maximum': 1000,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET1['name'])
+        with self.pnet_range(pnet, range_data) as pnet_range:
+            data = pnet_range['providernet_range']
+            self.assertEqual(data['name'], range_data['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['shared'], range_data['shared'])
+
+    def test_create_range_overlap_system_vlans(self):
+        range_data = {'name': 'overlap',
+                      'shared': True,
+                      'minimum': 4000,
+                      'maximum': 4010,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET1['name'])
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet_range,
+                          pnet, range_data)
+
+    def test_create_range_no_overlap_system_vlans(self):
+        range_data = {'name': 'overlap',
+                      'shared': True,
+                      'minimum': 4000,
+                      'maximum': 4010,
+                      'tenant_id': self._tenant_id}
+        ctxt = context.get_admin_context()
+        pnet = self._plugin.get_providernet_by_name(ctxt, PNET2['name'])
+        with self.pnet_range(pnet, range_data) as pnet_range:
+            data = pnet_range['providernet_range']
+            self.assertEqual(data['name'], range_data['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['shared'], range_data['shared'])
+
+    def test_create_interface_peer_overlap(self):
+        interface_data = copy.deepcopy(INTERFACE1)
+        interface_data['providernets'] += ',' + PNET3['name']
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_incompatible_providernets(self):
+        interface_data = copy.deepcopy(INTERFACE1)
+        interface_data['providernets'] += ',' + PNET6['name']
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_incompatible_pci_providernets(self):
+        interface_data = copy.deepcopy(INTERFACE5)
+        interface_data['providernets'] = PNET4['name']
+        interface_data['mtu'] = PNET4['mtu'] + constants.VXLAN_MTU_OVERHEAD
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST5['id'], interface_data)
+
+    def test_create_interface_system_vlan_overlap(self):
+        interface_data = copy.deepcopy(INTERFACE1)
+        interface_data['vlans'] = '1'
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST1['id'], interface_data)
+
+    def test_create_interface_incompatible_flat_mtu(self):
+        interface_data = copy.deepcopy(INTERFACE4)
+        interface_data['mtu'] = PNET6['mtu'] - 1
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST4['id'], interface_data)
+
+    def test_create_interface_incompatible_vxlan_mtu(self):
+        interface_data = copy.deepcopy(INTERFACE2)
+        interface_data['mtu'] = PNET4['mtu']
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_interface,
+                          HOST2['id'], interface_data)
diff --git a/neutron/tests/unit/plugins/wrs/test_extension_manager.py b/neutron/tests/unit/plugins/wrs/test_extension_manager.py
new file mode 100644
index 0000000..0374272
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/test_extension_manager.py
@@ -0,0 +1,43 @@
+# Copyright 2013 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+from neutron.extensions import agent as ext_agent
+from neutron.extensions import host as ext_host
+from neutron.extensions import wrs_provider as ext_pnet
+
+
+class WrsExtensionManager(object):
+
+    def get_resources(self):
+        return (ext_host.Host.get_resources() +
+                ext_pnet.Wrs_provider.get_resources() +
+                ext_agent.Agent.get_resources())
+
+    def get_actions(self):
+        return []
+
+    def get_request_extensions(self):
+        return []
+
+    def get_extended_resources(self, version):
+        return (ext_pnet.get_extended_resources(version) +
+                ext_host.get_extended_resources(version) +
+                ext_agent.get_extended_resources(version))
diff --git a/neutron/tests/unit/plugins/wrs/test_extension_pnet.py b/neutron/tests/unit/plugins/wrs/test_extension_pnet.py
new file mode 100644
index 0000000..8ef1e53
--- /dev/null
+++ b/neutron/tests/unit/plugins/wrs/test_extension_pnet.py
@@ -0,0 +1,1111 @@
+# Copyright 2013 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+# Copyright (c) 2013-2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+import contextlib
+import copy
+import uuid
+
+import webob.exc
+
+from oslo_log import log as logging
+
+from neutron.common import constants as n_const
+from neutron.plugins.ml2 import config
+from neutron.tests.unit.db import test_db_base_plugin_v2
+from neutron.tests.unit.plugins.wrs import test_wrs_plugin
+from neutron_lib import context
+from neutron_lib.plugins import directory
+
+LOG = logging.getLogger(__name__)
+
+
+FLAT_PNET1 = {'name': 'flat-pnet0',
+              'type': n_const.PROVIDERNET_FLAT,
+              'description': 'flat test provider network'}
+
+VLAN_PNET1 = {'name': 'vlan-pnet0',
+              'type': n_const.PROVIDERNET_VLAN,
+              'description': 'vlan test provider network'}
+
+VLAN_PNET1_RANGE1 = {'name': 'vlan-pnet0-0',
+                     'description': 'vlan range1',
+                     'shared': False,
+                     'minimum': 10,
+                     'maximum': 100,
+                     'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+VLAN_PNET2 = {'name': 'vlan-pnet1',
+              'type': n_const.PROVIDERNET_VLAN,
+              'description': 'vlan test provider network'}
+
+VLAN_PNET2_RANGE1 = {'name': 'vlan-pnet1-0',
+                     'description': 'vlan range1',
+                     'shared': True,
+                     'minimum': 1,
+                     'maximum': 100,
+                     'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+VXLAN_PNET1 = {'name': 'vxlan-pnet0',
+               'type': n_const.PROVIDERNET_VXLAN,
+               'mtu': n_const.DEFAULT_MTU - n_const.VXLAN_MTU_OVERHEAD,
+               'description': 'vxlan test provider network'}
+
+VXLAN_PNET1_RANGE1 = {'name': 'vxlan-pnet0-0',
+                      'description': 'vxlan range1',
+                      'shared': False,
+                      'minimum': 10,
+                      'maximum': 100,
+                      'group': '239.0.0.1',
+                      'port': 8472,
+                      'ttl': 1,
+                      'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+VXLAN_PNET1_RANGE2 = {'name': 'vxlan-pnet0-1',
+                      'description': 'vxlan range2',
+                      'shared': False,
+                      'minimum': 1000,
+                      'maximum': 9999,
+                      'group': 'ff0e::239.0.0.1',
+                      'port': 4789,
+                      'ttl': 10,
+                      'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+VXLAN_PNET2 = {'name': 'vxlan-pnet1',
+               'type': n_const.PROVIDERNET_VXLAN,
+               'mtu': n_const.DEFAULT_MTU - n_const.VXLAN_MTU_OVERHEAD,
+               'description': 'vxlan test provider network'}
+
+VXLAN_PNET2_RANGE1 = {'name': 'vxlan-pnet1-0',
+                      'description': 'vxlan range1',
+                      'shared': False,
+                      'minimum': 101,
+                      'maximum': 200,
+                      'group': '239.0.0.2',
+                      'port': 4789,
+                      'ttl': 10,
+                      'tenant_id': test_db_base_plugin_v2.TEST_TENANT_ID}
+
+
+class ProvidernetTestCaseMixin(object):
+
+    @contextlib.contextmanager
+    def network(self, name='net1',
+                admin_state_up=True,
+                fmt=None,
+                **kwargs):
+        # This is being overriden from the test_db_plugin version because that
+        # version does not automatically cleanup the network upon exiting the
+        # context manager context.
+        network = self._make_network(fmt or self.fmt, name,
+                                     admin_state_up, **kwargs)
+        try:
+            yield network
+        finally:
+            self._delete('networks', network['network']['id'])
+
+    def _update_pnet(self, id, body):
+        data = {'providernet': body}
+        request = self.new_update_request('wrs-provider/providernets',
+                                          data, id)
+        response = request.get_response(self.ext_api)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    def _create_pnet(self, pnet):
+        data = {'providernet': {'name': pnet['name'],
+                                'tenant_id': self._tenant_id}}
+        for arg in ('name', 'type', 'mtu', 'description'):
+            if arg in pnet:
+                data['providernet'][arg] = pnet[arg]
+        request = self.new_create_request('wrs-provider/providernets', data)
+        return request.get_response(self.ext_api)
+
+    def _make_pnet(self, data):
+        response = self._create_pnet(data)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    @contextlib.contextmanager
+    def pnet(self, data, no_delete=False):
+        obj = self._make_pnet(data)
+        try:
+            yield obj
+        finally:
+            if not no_delete:
+                self._delete('wrs-provider/providernets',
+                             obj['providernet']['id'])
+
+    def _update_pnet_range(self, id, body):
+        data = {'providernet_range': body}
+        request = self.new_update_request('wrs-provider/providernet_range',
+                                          data, id)
+        response = request.get_response(self.ext_api)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    def _create_pnet_range(self, pnet, pnet_range):
+        data = {'providernet_range': {'providernet_id': pnet['id']}}
+        for arg in ('name', 'description', 'shared',
+                    'minimum', 'maximum', 'tenant_id',
+                    'group', 'port', 'ttl'):
+            if arg in pnet_range:
+                data['providernet_range'][arg] = pnet_range[arg]
+        request = self.new_create_request('wrs-provider/providernet-ranges',
+                                          data)
+        return request.get_response(self.ext_api)
+
+    def _make_pnet_range(self, pnet, data):
+        response = self._create_pnet_range(pnet, data)
+        if response.status_int >= 400:
+            raise webob.exc.HTTPClientError(code=response.status_int)
+        return self.deserialize(self.fmt, response)
+
+    @contextlib.contextmanager
+    def pnet_range(self, pnet, data, no_delete=False):
+        obj = self._make_pnet_range(pnet, data)
+        try:
+            yield obj
+        finally:
+            if not no_delete:
+                self._delete('wrs-provider/providernet-ranges',
+                             obj['providernet_range']['id'])
+
+    def assertReturnsApiError(self, expected_type, function, *args, **kwargs):
+        response = function(*args, **kwargs)
+        body = response.json_body
+        actual_type = body['NeutronError']['type']
+        self.assertEqual(expected_type, actual_type)
+
+
+class ProvidernetTestCase(ProvidernetTestCaseMixin,
+                          test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setup_config(self):
+        super(ProvidernetTestCase, self).setup_config()
+        # Instantiate a fake host driver to allow us to control the host to
+        # provider network mappings
+        config.cfg.CONF.set_override('host_driver',
+                                     'neutron.tests.unit.plugins.wrs.'
+                                     'test_host_driver.TestHostDriver')
+
+    def setUp(self):
+        super(ProvidernetTestCase, self).setUp()
+        self._plugin = directory.get_plugin()
+        self._host_driver = self._plugin.host_driver
+
+    def tearDown(self):
+        super(ProvidernetTestCase, self).tearDown()
+
+    def test_create_vlan_providernet(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            data = pnet['providernet']
+            self.assertEqual(data['name'], VLAN_PNET1['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['type'], n_const.PROVIDERNET_VLAN)
+
+    def test_create_providernet_invalid_name(self):
+        pnet = copy.deepcopy(VLAN_PNET1)
+        pnet['name'] = "   "
+        self.assertReturnsApiError("HTTPBadRequest", self._create_pnet, pnet)
+
+    def test_create_providernet_invalid_mtu(self):
+        pnet = copy.deepcopy(VLAN_PNET1)
+        pnet['mtu'] = 12345678
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet, pnet)
+
+    def test_create_providernet_invalid_type(self):
+        pnet = copy.deepcopy(VLAN_PNET1)
+        pnet['type'] = 'invalid'
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet, pnet)
+
+    def test_create_providernet_unsupported(self):
+        pnet = copy.deepcopy(VLAN_PNET1)
+        pnet['type'] = 'gre'
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet, pnet)
+
+    def test_create_providernet_minimum_mtu(self):
+        pnet = copy.deepcopy(VLAN_PNET1)
+        pnet['mtu'] = n_const.MINIMUM_TTL - 1
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet, pnet)
+
+    def test_create_providernet_maximum_mtu(self):
+        pnet = copy.deepcopy(VLAN_PNET1)
+        pnet['mtu'] = n_const.MAXIMUM_TTL + 1
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet, pnet)
+
+    def test_create_vxlan_providernet(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            data = pnet['providernet']
+            self.assertEqual(data['name'], VXLAN_PNET1['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['type'], n_const.PROVIDERNET_VXLAN)
+
+    def test_update_vlan_providernet(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            data = {'providernet': {'mtu': 1234}}
+            request = self.new_update_request('wrs-provider/providernets',
+                                              data, pnet['providernet']['id'])
+            response = request.get_response(self.ext_api)
+            self.assertEqual(response.status_int, 200)
+            body = self.deserialize(self.fmt, response)
+            self.assertEqual(body['providernet']['mtu'], 1234)
+
+    def test_update_vlan_providernet_minimum_mtu(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            data = {'providernet': {'mtu': n_const.MINIMUM_MTU}}
+            request = self.new_update_request('wrs-provider/providernets',
+                                              data, pnet['providernet']['id'])
+            response = request.get_response(self.ext_api)
+            self.assertEqual(response.status_int, 200)
+            body = self.deserialize(self.fmt, response)
+            self.assertEqual(body['providernet']['mtu'], n_const.MINIMUM_MTU)
+
+    def test_update_vlan_providernet_below_minimum_mtu(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            body = {'mtu': n_const.MINIMUM_MTU - 1}
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._update_pnet,
+                              pnet['providernet']['id'], body)
+
+    def test_update_vxlan_providernet(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            data = {'providernet': {'mtu': 1234}}
+            request = self.new_update_request('wrs-provider/providernets',
+                                              data, pnet['providernet']['id'])
+            response = request.get_response(self.ext_api)
+            self.assertEqual(response.status_int, 200)
+            body = self.deserialize(self.fmt, response)
+            self.assertEqual(body['providernet']['mtu'], 1234)
+
+    def test_update_vxlan_providernet_minimum_mtu(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            data = {'providernet': {'mtu': n_const.MINIMUM_MTU}}
+            request = self.new_update_request('wrs-provider/providernets',
+                                              data, pnet['providernet']['id'])
+            response = request.get_response(self.ext_api)
+            self.assertEqual(response.status_int, 200)
+            body = self.deserialize(self.fmt, response)
+            self.assertEqual(body['providernet']['mtu'], n_const.MINIMUM_MTU)
+
+    def test_update_vxlan_providernet_below_minimum_mtu(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            body = {'mtu': n_const.MINIMUM_MTU - 1}
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._update_pnet,
+                              pnet['providernet']['id'], body)
+
+    def test_create_vlan_tenant_net(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1):
+                with self.network() as net:
+                    data = net['network']
+                    self.assertEqual(data['provider:physical_network'],
+                                     VLAN_PNET1['name'])
+                    self.assertEqual(data['provider:network_type'],
+                                     VLAN_PNET1['type'])
+                    self.assertIsNotNone(data['provider:segmentation_id'])
+                    self.assertEqual(data['mtu'], pnet_data['mtu'])
+
+    def test_create_vxlan_tenant_net(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1):
+                with self.network() as net:
+                    data = net['network']
+                    self.assertEqual(data['provider:physical_network'],
+                                     VXLAN_PNET1['name'])
+                    self.assertEqual(data['provider:network_type'],
+                                     VXLAN_PNET1['type'])
+                    self.assertIsNotNone(data['provider:segmentation_id'])
+                    self.assertEqual(data['mtu'], pnet_data['mtu'])
+
+    def test_create_flat_tenant_net(self):
+        with self.pnet(FLAT_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.network(arg_list=('provider__physical_network',
+                                        'provider__network_type',),
+                              provider__physical_network=pnet_data['name'],
+                              provider__network_type='flat') as net:
+                data = net['network']
+                self.assertEqual(data['provider:physical_network'],
+                                 FLAT_PNET1['name'])
+                self.assertEqual(data['provider:network_type'],
+                                 FLAT_PNET1['type'])
+                self.assertIsNone(data['provider:segmentation_id'])
+                self.assertEqual(data['mtu'], pnet_data['mtu'])
+
+    def test_create_vxlan_tenant_net_with_out_of_range_vxlan(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1):
+                self.assertRaises(webob.exc.HTTPClientError,
+                                  self._make_network,
+                                  self.fmt, 'net1', True,
+                                  arg_list=('provider__physical_network',
+                                            'provider__network_type',
+                                            'provider__segmentation_id'),
+                                  provider__physical_network=pnet_data['name'],
+                                  provider__network_type='vxlan',
+                                  provider__segmentation_id='999')
+
+    def test_create_flat_tenant_net_on_vlan_pnet(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_network,
+                              self.fmt, 'net1', True,
+                              arg_list=('provider__physical_network',
+                                        'provider__network_type',),
+                              provider__physical_network=pnet_data['name'],
+                              provider__network_type='flat')
+
+    def test_create_flat_tenant_net_on_vxlan_pnet(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_network,
+                              self.fmt, 'net1', True,
+                              arg_list=('provider__physical_network',
+                                        'provider__network_type',),
+                              provider__physical_network=pnet_data['name'],
+                              provider__network_type='flat')
+
+    def test_create_vlan_tenant_net_on_flat_pnet(self):
+        with self.pnet(FLAT_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_network,
+                              self.fmt, 'net1', True,
+                              arg_list=('provider__physical_network',
+                                        'provider__network_type',),
+                              provider__physical_network=pnet_data['name'],
+                              provider__network_type='vlan')
+
+    def test_create_vlan_tenant_net_with_no_providernets(self):
+        with self.pnet(VLAN_PNET1):
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_network,
+                              self.fmt, 'net1', True)
+
+    def test_create_flat_providernet(self):
+        with self.pnet(FLAT_PNET1) as pnet:
+            data = pnet['providernet']
+            self.assertEqual(data['name'], FLAT_PNET1['name'])
+            self.assertIsNotNone(data['id'])
+            self.assertEqual(data['type'], n_const.PROVIDERNET_FLAT)
+            ctxt = context.get_admin_context()
+            result = self._plugin.get_providernet_segment_details(
+                ctxt, n_const.PROVIDERNET_FLAT,
+                data['name'], None)
+            self.assertEqual(data['name'], result['name'])
+            self.assertEqual(n_const.PROVIDERNET_FLAT, result['type'])
+
+
+class ProvidernetRangeTestCase(ProvidernetTestCaseMixin,
+                               test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setup_config(self):
+        super(ProvidernetRangeTestCase, self).setup_config()
+        # Instantiate a fake host driver to allow us to control the host to
+        # provider network mappings
+        config.cfg.CONF.set_override('host_driver',
+                                     'neutron.tests.unit.plugins.wrs.'
+                                     'test_host_driver.TestHostDriver')
+
+    def setUp(self, plugin=None, ext_mgr=None):
+        super(ProvidernetRangeTestCase, self).setUp()
+        self._plugin = directory.get_plugin()
+        self._host_driver = self._plugin.host_driver
+
+    def tearDown(self):
+        super(ProvidernetRangeTestCase, self).tearDown()
+
+    def test_create_vlan_providernet_range(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                self.assertEqual(data['name'], VLAN_PNET1_RANGE1['name'])
+                self.assertEqual(data['providernet_id'], pnet_data['id'])
+                self.assertIsNotNone(data['id'])
+                self.assertEqual(data['shared'], VLAN_PNET1_RANGE1['shared'])
+                self.assertEqual(data['tenant_id'],
+                                 VLAN_PNET1_RANGE1['tenant_id'])
+                ctxt = context.get_admin_context()
+                result = self._plugin.get_providernet_segment_details(
+                    ctxt, n_const.PROVIDERNET_VLAN,
+                    VLAN_PNET1['name'], VLAN_PNET1_RANGE1['minimum'])
+                self.assertEqual(VLAN_PNET1['name'], result['name'])
+                self.assertEqual(n_const.PROVIDERNET_VLAN, result['type'])
+
+    def test_create_vlan_providernet_range_overlap_all(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                range_data = copy.deepcopy(data)
+                range_data['name'] = 'overlap'
+                range_data['minimum'] -= 1
+                range_data['maximum'] += 1
+                self.assertRaises(webob.exc.HTTPClientError,
+                                  self._make_pnet_range,
+                                  pnet_data, range_data)
+
+    def test_create_vlan_providernet_range_overlap_bottom(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                range_data = copy.deepcopy(data)
+                range_data['name'] = 'overlap'
+                range_data['minimum'] -= 1
+                range_data['maximum'] -= 1
+                self.assertRaises(webob.exc.HTTPClientError,
+                                  self._make_pnet_range,
+                                  pnet_data, range_data)
+
+    def test_create_vlan_providernet_range_overlap_top(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                range_data = copy.deepcopy(data)
+                range_data['name'] = 'overlap'
+                range_data['minimum'] += 1
+                range_data['maximum'] += 1
+                self.assertRaises(webob.exc.HTTPClientError,
+                                  self._make_pnet_range,
+                                  pnet_data, range_data)
+
+    def test_create_vlan_providernet_range_overlap_inside(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                range_data = copy.deepcopy(data)
+                range_data['name'] = 'overlap'
+                range_data['minimum'] += 1
+                range_data['maximum'] -= 1
+                self.assertRaises(webob.exc.HTTPClientError,
+                                  self._make_pnet_range,
+                                  pnet_data, range_data)
+
+    def test_create_vlan_providernet_range_no_overlap_below(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                range_data = copy.deepcopy(data)
+                range_data['name'] = 'overlap'
+                range_data['minimum'] -= 5
+                range_data['maximum'] = range_data['minimum'] + 2
+                with self.pnet_range(pnet_data, range_data) as pnet_range2:
+                    data = pnet_range2['providernet_range']
+                    self.assertEqual(data['name'], range_data['name'])
+
+    def test_create_vlan_providernet_range_no_overlap_above(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                range_data = copy.deepcopy(data)
+                range_data['name'] = 'overlap'
+                range_data['maximum'] += 50
+                range_data['minimum'] = range_data['maximum'] - 2
+                with self.pnet_range(pnet_data, range_data) as pnet_range2:
+                    data = pnet_range2['providernet_range']
+                    self.assertEqual(data['name'], range_data['name'])
+
+    def test_create_vlan_providernet_range_minimum_id(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VLAN_PNET1_RANGE1)
+            range_data['minimum'] = 0
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vlan_providernet_range_maximum_id(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VLAN_PNET1_RANGE1)
+            range_data['maximum'] = 16384
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vlan_providernet_range_shared(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = VLAN_PNET1_RANGE1
+            range_data['shared'] = True
+            with self.pnet_range(pnet_data, range_data) as pnet_range:
+                data = pnet_range['providernet_range']
+                self.assertEqual(data['name'], range_data['name'])
+                self.assertEqual(data['providernet_id'], pnet_data['id'])
+                self.assertIsNotNone(data['id'])
+                self.assertTrue(data['shared'])
+
+    def test_create_vlan_providernet_range_invalid_pnet(self):
+        pnet = VLAN_PNET1
+        pnet['id'] = str(uuid.uuid4())
+        self.assertRaises(webob.exc.HTTPClientError,
+                          self._make_pnet_range,
+                          pnet, VLAN_PNET1_RANGE1)
+
+    def test_create_vxlan_providernet_range(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                data = pnet_range['providernet_range']
+                self.assertEqual(data['name'], VXLAN_PNET1_RANGE1['name'])
+                self.assertEqual(data['providernet_id'], pnet_data['id'])
+                self.assertIsNotNone(data['id'])
+                vxlan = data['vxlan']
+                self.assertEqual(vxlan['group'], VXLAN_PNET1_RANGE1['group'])
+                self.assertEqual(vxlan['port'], VXLAN_PNET1_RANGE1['port'])
+                self.assertEqual(vxlan['ttl'], VXLAN_PNET1_RANGE1['ttl'])
+                ctxt = context.get_admin_context()
+                result = self._plugin.get_providernet_segment_details(
+                    ctxt, n_const.PROVIDERNET_VXLAN,
+                    VXLAN_PNET1['name'], VXLAN_PNET1_RANGE1['minimum'])
+                self.assertEqual(VXLAN_PNET1['name'], result['name'])
+                self.assertEqual(n_const.PROVIDERNET_VXLAN, result['type'])
+                self.assertEqual(VXLAN_PNET1_RANGE1['group'],
+                                 result['vxlan']['group'])
+
+    def test_create_vxlan_multiple_providernet_range(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1):
+                with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE2):
+                    ctxt = context.get_admin_context()
+                    # Check that get_providernet_segment_details returns the
+                    # range data for the first range.
+                    result1 = self._plugin.get_providernet_segment_details(
+                        ctxt, n_const.PROVIDERNET_VXLAN,
+                        VXLAN_PNET1['name'], VXLAN_PNET1_RANGE1['minimum'])
+                    self.assertEqual(VXLAN_PNET1['name'], result1['name'])
+                    self.assertEqual(n_const.PROVIDERNET_VXLAN,
+                                     result1['type'])
+                    self.assertEqual(VXLAN_PNET1_RANGE1['group'],
+                                     result1['vxlan']['group'])
+                    # Check that get_providernet_segment_details returns the
+                    # range data for the second range.
+                    result2 = self._plugin.get_providernet_segment_details(
+                        ctxt, n_const.PROVIDERNET_VXLAN,
+                        VXLAN_PNET1['name'], VXLAN_PNET1_RANGE2['minimum'])
+                    self.assertEqual(VXLAN_PNET1['name'], result2['name'])
+                    self.assertEqual(n_const.PROVIDERNET_VXLAN,
+                                     result2['type'])
+                    self.assertEqual(VXLAN_PNET1_RANGE2['group'],
+                                     result2['vxlan']['group'])
+
+    def test_create_vxlan_providernet_range_minimum_ttl(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE2)
+            range_data['ttl'] = n_const.MINIMUM_TTL
+            with self.pnet_range(pnet_data, range_data) as pnet_range:
+                data = pnet_range['providernet_range']
+                self.assertEqual(data['name'], VXLAN_PNET1_RANGE2['name'])
+                self.assertEqual(data['providernet_id'], pnet_data['id'])
+                self.assertIsNotNone(data['id'])
+                vxlan = data['vxlan']
+                self.assertEqual(vxlan['group'], VXLAN_PNET1_RANGE2['group'])
+                self.assertEqual(vxlan['port'], VXLAN_PNET1_RANGE2['port'])
+                self.assertEqual(vxlan['ttl'], range_data['ttl'])
+
+    def test_create_vxlan_providernet_range_maximum_ttl(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE2)
+            range_data['ttl'] = n_const.MAXIMUM_TTL
+            with self.pnet_range(pnet_data, range_data) as pnet_range:
+                data = pnet_range['providernet_range']
+                self.assertEqual(data['name'], VXLAN_PNET1_RANGE2['name'])
+                self.assertEqual(data['providernet_id'], pnet_data['id'])
+                self.assertIsNotNone(data['id'])
+                vxlan = data['vxlan']
+                self.assertEqual(vxlan['group'], VXLAN_PNET1_RANGE2['group'])
+                self.assertEqual(vxlan['port'], VXLAN_PNET1_RANGE2['port'])
+                self.assertEqual(vxlan['ttl'], range_data['ttl'])
+
+    def test_create_vxlan_providernet_range_invalid_port(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['port'] = 1234
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_below_minimum_ttl(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['ttl'] = n_const.MINIMUM_TTL - 1
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_above_maximum_ttl(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['ttl'] = n_const.MAXIMUM_TTL + 1
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_missing_ttl(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            del range_data['ttl']
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_invalid_group(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['group'] = 'invalid'
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_missing_group(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            del range_data['group']
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_unicast_group(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['group'] = '1.2.3.4'
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_unicast_ipv6_group(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['group'] = 'fd10::1'
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_minimum_id(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['minimum'] = 0
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_maximum_id(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['maximum'] = (2 ** 24)
+            self.assertRaises(webob.exc.HTTPClientError,
+                              self._make_pnet_range,
+                              pnet_data, range_data)
+
+    def test_create_vxlan_providernet_range_shared(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            range_data = copy.deepcopy(VXLAN_PNET1_RANGE1)
+            range_data['shared'] = True
+            with self.pnet_range(pnet_data, range_data) as pnet_range:
+                data = pnet_range['providernet_range']
+                self.assertEqual(data['name'], range_data['name'])
+                self.assertEqual(data['providernet_id'], pnet_data['id'])
+                self.assertIsNotNone(data['id'])
+                self.assertTrue(data['shared'])
+
+
+class ProvidernetRangeUpdateTestCase(ProvidernetTestCaseMixin,
+                                     test_wrs_plugin.WrsMl2PluginV2TestCase):
+
+    def setup_config(self):
+        super(ProvidernetRangeUpdateTestCase, self).setup_config()
+        # Instantiate a fake host driver to allow us to control the host to
+        # provider network mappings
+        config.cfg.CONF.set_override('host_driver',
+                                     'neutron.tests.unit.plugins.wrs.'
+                                     'test_host_driver.TestHostDriver')
+
+    def setUp(self, plugin=None, ext_mgr=None):
+        super(ProvidernetRangeUpdateTestCase, self).setUp()
+        self._plugin = directory.get_plugin()
+        self._host_driver = self._plugin.host_driver
+
+    def tearDown(self):
+        super(ProvidernetRangeUpdateTestCase, self).tearDown()
+
+    def test_update_vlan_range_with_larger_range_no_orphans(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VLAN_PNET1_RANGE1['minimum'] - 1,
+                             'maximum': VLAN_PNET1_RANGE1['maximum'] + 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+                    body = self.deserialize(self.fmt, response)
+                    self.assertEqual(body['providernet_range']['minimum'],
+                                     data['providernet_range']['minimum'])
+                    self.assertEqual(body['providernet_range']['maximum'],
+                                     data['providernet_range']['maximum'])
+
+    def test_update_vlan_range_with_smaller_range_no_orphans(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VLAN_PNET1_RANGE1['minimum'],
+                             'maximum': VLAN_PNET1_RANGE1['maximum'] - 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+                    body = self.deserialize(self.fmt, response)
+                    self.assertEqual(body['providernet_range']['minimum'],
+                                     data['providernet_range']['minimum'])
+                    self.assertEqual(body['providernet_range']['maximum'],
+                                     data['providernet_range']['maximum'])
+
+    def test_update_vlan_range_with_orphans_above(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VLAN_PNET1_RANGE1['maximum'] + 1,
+                             'maximum': VLAN_PNET1_RANGE1['maximum'] + 2}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vlan_range_with_orphans_below(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VLAN_PNET1_RANGE1['minimum'] - 2,
+                             'maximum': VLAN_PNET1_RANGE1['minimum'] - 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vlan_range_with_no_orphans_bottom(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VLAN_PNET1_RANGE1['minimum'] - 2,
+                             'maximum': VLAN_PNET1_RANGE1['minimum'] + 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+                    body = self.deserialize(self.fmt, response)
+                    self.assertEqual(body['providernet_range']['minimum'],
+                                     data['providernet_range']['minimum'])
+                    self.assertEqual(body['providernet_range']['maximum'],
+                                     data['providernet_range']['maximum'])
+
+    def test_update_vlan_range_with_orphans_top(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VLAN_PNET1_RANGE1['maximum'] - 2,
+                             'maximum': VLAN_PNET1_RANGE1['maximum'] + 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vlan_range_with_orphans_inside(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network() as net:
+                    net_data = net['network']
+                    segmentation_id = net_data['provider:segmentation_id']
+                    data = {'providernet_range':
+                            {'minimum': segmentation_id + 1,
+                             'maximum': VLAN_PNET1_RANGE1['maximum']}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vlan_range_with_no_orphans_used(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network() as net:
+                    net_data = net['network']
+                    segmentation_id = net_data['provider:segmentation_id']
+                    data = {'providernet_range':
+                            {'minimum': segmentation_id,
+                             'maximum': segmentation_id}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+
+    def test_update_vlan_range_with_invalid_minimum(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                data = {'providernet_range': {'minimum': 0,
+                                              'maximum': 10}}
+                request = self.new_update_request(
+                    'wrs-provider/providernet-ranges',
+                    data, range_data['id'])
+                response = request.get_response(self.ext_api)
+                self.assertEqual(response.status_int, 500)
+
+    def test_update_vlan_range_with_invalid_maximum(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                data = {'providernet_range': {'minimum': 1,
+                                              'maximum': (2 ** 24)}}
+                request = self.new_update_request(
+                    'wrs-provider/providernet-ranges',
+                    data, range_data['id'])
+                response = request.get_response(self.ext_api)
+                self.assertEqual(response.status_int, 500)
+
+    def test_update_vxlan_range_with_larger_range_no_orphans(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VXLAN_PNET1_RANGE1['minimum'] - 1,
+                             'maximum': VXLAN_PNET1_RANGE1['maximum'] + 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+                    body = self.deserialize(self.fmt, response)
+                    self.assertEqual(body['providernet_range']['minimum'],
+                                     data['providernet_range']['minimum'])
+                    self.assertEqual(body['providernet_range']['maximum'],
+                                     data['providernet_range']['maximum'])
+
+    def test_update_vxlan_range_with_smaller_range_no_orphans(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VXLAN_PNET1_RANGE1['minimum'],
+                             'maximum': VXLAN_PNET1_RANGE1['maximum'] - 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+                    body = self.deserialize(self.fmt, response)
+                    self.assertEqual(body['providernet_range']['minimum'],
+                                     data['providernet_range']['minimum'])
+                    self.assertEqual(body['providernet_range']['maximum'],
+                                     data['providernet_range']['maximum'])
+
+    def test_update_vxlan_range_with_orphans_above(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VXLAN_PNET1_RANGE1['maximum'] + 1,
+                             'maximum': VXLAN_PNET1_RANGE1['maximum'] + 2}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vxlan_range_with_orphans_below(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VXLAN_PNET1_RANGE1['minimum'] - 2,
+                             'maximum': VXLAN_PNET1_RANGE1['minimum'] - 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vxlan_range_with_no_orphans_bottom(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VXLAN_PNET1_RANGE1['minimum'] - 2,
+                             'maximum': VXLAN_PNET1_RANGE1['minimum'] + 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+                    body = self.deserialize(self.fmt, response)
+                    self.assertEqual(body['providernet_range']['minimum'],
+                                     data['providernet_range']['minimum'])
+                    self.assertEqual(body['providernet_range']['maximum'],
+                                     data['providernet_range']['maximum'])
+
+    def test_update_vxlan_range_with_orphans_top(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network():
+                    data = {'providernet_range':
+                            {'minimum': VXLAN_PNET1_RANGE1['maximum'] - 2,
+                             'maximum': VXLAN_PNET1_RANGE1['maximum'] + 1}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vxlan_range_with_orphans_inside(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network() as net:
+                    net_data = net['network']
+                    segmentation_id = net_data['provider:segmentation_id']
+                    data = {'providernet_range':
+                            {'minimum': segmentation_id + 1,
+                             'maximum': VXLAN_PNET1_RANGE1['maximum']}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 500)
+
+    def test_update_vxlan_range_with_no_orphans_used(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                with self.network() as net:
+                    net_data = net['network']
+                    segmentation_id = net_data['provider:segmentation_id']
+                    data = {'providernet_range':
+                            {'minimum': segmentation_id,
+                             'maximum': segmentation_id}}
+                    request = self.new_update_request(
+                        'wrs-provider/providernet-ranges',
+                        data, range_data['id'])
+                    response = request.get_response(self.ext_api)
+                    self.assertEqual(response.status_int, 200)
+
+    def test_update_vxlan_range_with_invalid_minimum(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                data = {'providernet_range': {'minimum': 0,
+                                              'maximum': 10}}
+                request = self.new_update_request(
+                    'wrs-provider/providernet-ranges',
+                    data, range_data['id'])
+                response = request.get_response(self.ext_api)
+                self.assertEqual(response.status_int, 500)
+
+    def test_update_vxlan_range_with_invalid_maximum(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1) as pnet_range:
+                range_data = pnet_range['providernet_range']
+                data = {'providernet_range': {'minimum': 1,
+                                              'maximum': (2 ** 24)}}
+                request = self.new_update_request(
+                              'wrs-provider/providernet-ranges',
+                              data, range_data['id'])
+                response = request.get_response(self.ext_api)
+                self.assertEqual(response.status_int, 500)
diff --git a/setup.cfg b/setup.cfg
index bc888de..7cdcebb 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -90,6 +90,9 @@ neutron.ml2.type_drivers =
     geneve = neutron.plugins.ml2.drivers.type_geneve:GeneveTypeDriver
     gre = neutron.plugins.ml2.drivers.type_gre:GreTypeDriver
     vxlan = neutron.plugins.ml2.drivers.type_vxlan:VxlanTypeDriver
+    managed_vlan = neutron.plugins.wrs.drivers.type_managed_vlan:ManagedVlanTypeDriver
+    managed_vxlan = neutron.plugins.wrs.drivers.type_managed_vxlan:ManagedVxlanTypeDriver
+    managed_flat = neutron.plugins.wrs.drivers.type_managed_flat:ManagedFlatTypeDriver
 neutron.ml2.mechanism_drivers =
     logger = neutron.tests.unit.plugins.ml2.drivers.mechanism_logger:LoggerMechanismDriver
     test = neutron.tests.unit.plugins.ml2.drivers.mechanism_test:TestMechanismDriver
-- 
2.7.4

