From c85f0445a80600a18110ddd5236e2e824a503c44 Mon Sep 17 00:00:00 2001
From: Matt Peters <matt.peters@windriver.com>
Date: Fri, 20 May 2016 09:48:18 -0400
Subject: [PATCH 010/155] subnet: custom guest vlan tagged subnets

This commit introduces a custom extension to implement VLAN tagged guest
subnets.  A custom can associate a VLAN tag to a tenant subnet.  That
subnet will be segragated from other traffic using a unique provider
segmentation id.  The subnet will be able to access router and DHCP
services as is already possible on untagged tenant networks.

Conflicts:
	neutron/agent/dhcp/agent.py
	neutron/agent/linux/dhcp.py
	neutron/common/constants.py
	neutron/db/db_base_plugin_v2.py
	neutron/db/migration/alembic_migrations/versions/HEAD
	neutron/db/migration/models/frozen.py
	neutron/db/models_v2.py
	neutron/plugins/ml2/db.py
	neutron/plugins/ml2/driver_context.py
	neutron/plugins/ml2/managers.py
	neutron/plugins/ml2/plugin.py
	neutron/plugins/ml2/rpc.py
	neutron/tests/unit/agent/dhcp/test_agent.py
	neutron/tests/unit/agent/linux/test_dhcp.py
	neutron/tests/unit/db/test_db_base_plugin_v2.py

CGTS-7049: vlan: validate requested fixed_ip list upfront

Because of how we allocate ips seperately for each layer of vlan subnets we
need to do some upfront validation.  This is because one of 2 things could
happen.

  1) A subnet that is intended for a specific vlan will appear as an error
     on all of the other subnets, or
  2) an ip address request will get ignored altogether.

Filter the list of fixed_ips to find those that do not match any subnets.
Those are the ones that will cause a legitimate error.  We could just raise an
exception directly but the unit tests except different exception types for
different types of failures.  Rather than duplicate the exception raising code
just pass these erroneous fixed_ip entries to the _get_subnet_for_fixed_ip
function and let it raise the exceptions as they are expected by the unit
tests.

US102722: Disable provisioning of vlan subnets

CGTS-7450: vlan: match dhcp port device id without vlan

When a port update is received from the server it may have an empty fixed_ips
attribute.  When this happens it is not possible to determine which VLAN the
port belongs to.  For the purpose of determining whether the port is one that
is owned by the agent this is not necessary.  It is sufficient to prefix match
the device_id with the default vlan value of the device_id since all other
values simply have a "-" + "VLAN_ID" at the end.

CGTS-7450: vlan: combine old and new vlan id when updating ports

When a port is updated it can have an empty fixed_ips attribute.  This
can cause the port_update_end code to think that there are no vlans to
service on this port, but that ignores the fact that the port used to
have a valid vlan_id.  For this reason when we process a port update we
need to ensure that we combined the existing vlan_id list with the new
vlan_id list as a set in order to invoke the driver on the correct
network vlan instance.

Conflicts:
	neutron/agent/dhcp/agent.py
	neutron/agent/linux/dhcp.py
	neutron/agent/linux/interface.py
	neutron/agent/linux/ip_lib.py
	neutron/common/constants.py
	neutron/conf/common.py
	neutron/db/db_base_plugin_common.py
	neutron/db/db_base_plugin_v2.py
	neutron/db/ipam_pluggable_backend.py
	neutron/db/l3_db.py
	neutron/db/segments_db.py
	neutron/plugins/ml2/driver_context.py
	neutron/plugins/ml2/plugin.py
	neutron/plugins/ml2/rpc.py
	neutron/tests/unit/agent/dhcp/test_agent.py
	neutron/tests/unit/plugins/wrs/test_extension_pnet.py
---
 etc/policy.json                                    |   5 +
 neutron/agent/dhcp/agent.py                        | 171 ++++++++++++----
 neutron/agent/linux/dhcp.py                        | 228 ++++++++++++++++-----
 neutron/agent/linux/interface.py                   |   4 +-
 neutron/agent/linux/ip_lib.py                      |   8 +-
 neutron/api/rpc/handlers/dhcp_rpc.py               |   4 +-
 neutron/common/constants.py                        |   3 +
 neutron/common/utils.py                            |  14 +-
 neutron/conf/common.py                             |   7 +
 neutron/db/agentschedulers_db.py                   |   8 +-
 neutron/db/db_base_plugin_common.py                |   5 +-
 neutron/db/db_base_plugin_v2.py                    |  36 +++-
 neutron/db/ipam_backend_mixin.py                   |   4 +
 neutron/db/ipam_pluggable_backend.py               | 169 ++++++++++++++-
 neutron/db/l3_db.py                                |  15 +-
 ...018b7ad4223_ml2_subnet_segment_dynamic_field.py |  52 +++++
 .../wrs_kilo_shipped/expand/wrs_kilo_shipped.py    |   4 +-
 .../alembic_migrations/vswitch_init_ops.py         |  15 ++
 neutron/db/models_v2.py                            |   9 +
 neutron/db/segments_db.py                          |  84 +++++++-
 neutron/extensions/wrs_net.py                      |   3 +
 neutron/extensions/wrs_provider.py                 |  10 +
 neutron/plugins/ml2/db.py                          |  21 +-
 neutron/plugins/ml2/driver_context.py              |  37 ++++
 neutron/plugins/ml2/managers.py                    | 115 +++++++++++
 neutron/plugins/ml2/plugin.py                      |  90 +++++++-
 neutron/plugins/ml2/rpc.py                         |  23 +++
 neutron/tests/etc/policy.json                      |   5 +
 neutron/tests/unit/agent/dhcp/test_agent.py        |  36 +++-
 neutron/tests/unit/agent/linux/test_dhcp.py        |  43 +++-
 .../tests/unit/api/rpc/handlers/test_dhcp_rpc.py   |   8 +-
 neutron/tests/unit/db/test_db_base_plugin_v2.py    |  30 ++-
 .../tests/unit/plugins/wrs/test_extension_pnet.py  |  80 ++++++++
 33 files changed, 1191 insertions(+), 155 deletions(-)
 create mode 100644 neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/5018b7ad4223_ml2_subnet_segment_dynamic_field.py

diff --git a/etc/policy.json b/etc/policy.json
index 9e1caf9..8c36949 100644
--- a/etc/policy.json
+++ b/etc/policy.json
@@ -18,10 +18,15 @@
     "create_subnet": "rule:admin_or_network_owner",
     "create_subnet:segment_id": "rule:admin_only",
     "create_subnet:service_types": "rule:admin_only",
+    "create_subnet:wrs-provider:segmentation_id": "rule:admin_only",
     "get_subnet": "rule:admin_or_owner or rule:shared",
     "get_subnet:segment_id": "rule:admin_only",
+    "get_subnet:wrs-provider:network_type": "rule:admin_only",
+    "get_subnet:wrs-provider:physical_network": "rule:admin_only",
+    "get_subnet:wrs-provider:segmentation_id": "rule:admin_only",
     "update_subnet": "rule:admin_or_network_owner",
     "update_subnet:service_types": "rule:admin_only",
+    "update_subnet:wrs-provider:segmentation_id": "rule:admin_only",
     "delete_subnet": "rule:admin_or_network_owner",
 
     "create_subnetpool": "",
diff --git a/neutron/agent/dhcp/agent.py b/neutron/agent/dhcp/agent.py
index 6c09349..7b3e1b0 100644
--- a/neutron/agent/dhcp/agent.py
+++ b/neutron/agent/dhcp/agent.py
@@ -38,6 +38,7 @@ from neutron.common import constants as n_const
 from neutron.common import rpc as n_rpc
 from neutron.common import topics
 from neutron.common import utils
+from neutron.extensions import wrs_net
 from neutron import manager
 
 LOG = logging.getLogger(__name__)
@@ -109,9 +110,9 @@ class DhcpAgent(manager.Manager):
                 self.conf
             )
             for net_id in existing_networks:
-                net = dhcp.NetModel({"id": net_id, "subnets": [],
-                                     "non_local_subnets": [], "ports": []})
-                self.cache.put(net)
+                network = self.plugin_rpc.get_network_info(net_id)
+                if network:
+                    self.cache.put(network)
         except NotImplementedError:
             # just go ahead with an empty networks cache
             LOG.debug("The '%s' DHCP-driver does not support retrieving of a "
@@ -129,16 +130,19 @@ class DhcpAgent(manager.Manager):
 
     def call_driver(self, action, network, **action_kwargs):
         """Invoke an action on a DHCP driver instance."""
+        vlan_id = action_kwargs.pop('vlan_id', n_const.NONE_VLAN_TAG)
         LOG.debug('Calling driver for network: %(net)s action: %(action)s',
-                  {'net': network.id, 'action': action})
+                  {'net': network.id, 'vlan': vlan_id, 'action': action})
         try:
             # the Driver expects something that is duck typed similar to
             # the base models.
+            driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
             driver = self.dhcp_driver_cls(self.conf,
                                           network,
                                           self._process_monitor,
                                           self.dhcp_version,
-                                          self.plugin_rpc)
+                                          self.plugin_rpc,
+                                          **driver_kwargs)
             getattr(driver, action)(**action_kwargs)
             return True
         except exceptions.Conflict:
@@ -166,6 +170,9 @@ class DhcpAgent(manager.Manager):
             else:
                 LOG.exception('Unable to %(action)s dhcp for %(net_id)s.',
                               {'net_id': network.id, 'action': action})
+        finally:
+            # Update the network cache in case the driver updated the port list
+            self.cache.put(network)
 
     def schedule_resync(self, reason, network_id=None):
         """Schedule a resync for a given network and reason. If no network is
@@ -275,6 +282,16 @@ class DhcpAgent(manager.Manager):
         if network:
             self.configure_dhcp_for_network(network)
 
+    def _get_dhcp_enabled_vlans(self, network):
+        """
+        Return a vlan_id set for the network vlans with dhcp enabled.
+        The primary network with vlan None is also included in the set.
+        """
+        network_subnets = (network.subnets +
+                           getattr(network, 'non_local_subnets', []))
+        return set([getattr(s, wrs_net.VLAN, 0)
+                    for s in network_subnets if s.enable_dhcp])
+
     @utils.exception_logger()
     def safe_configure_dhcp_for_network(self, network):
         try:
@@ -291,16 +308,20 @@ class DhcpAgent(manager.Manager):
         if not network.admin_state_up:
             return
 
-        for subnet in network.subnets:
-            if subnet.enable_dhcp:
-                if self.call_driver('enable', network):
-                    self.update_isolated_metadata_proxy(network)
-                    self.cache.put(network)
-                    # After enabling dhcp for network, mark all existing
-                    # ports as ready. So that the status of ports which are
-                    # created before enabling dhcp can be updated.
-                    self.dhcp_ready_ports |= {p.id for p in network.ports}
-                break
+        enabled = False
+        vlan_ids = self._get_dhcp_enabled_vlans(network)
+        for vlan_id in vlan_ids:
+            driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+            if self.call_driver('enable', network, **driver_kwargs):
+                # After enabling dhcp for network, mark all existing
+                # ports as ready. So that the status of ports which are
+                # created before enabling dhcp can be updated.
+                self.dhcp_ready_ports |= {p.id for p in network.ports}
+                enabled = True
+
+        if enabled:
+            self.update_isolated_metadata_proxy(network)
+            self.cache.put(network)
 
     def disable_dhcp_helper(self, network_id):
         """Disable DHCP for a network known to the agent."""
@@ -314,13 +335,18 @@ class DhcpAgent(manager.Manager):
             # destroy_monitored_metadata_proxy() is a noop when
             # there is no process running.
             self.disable_isolated_metadata_proxy(network)
-            if self.call_driver('disable', network):
-                self.cache.remove(network)
+            vlan_ids = self._get_dhcp_enabled_vlans(network)
+            for vlan_id in vlan_ids:
+                driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+                self.call_driver('disable', network, **driver_kwargs)
+            self.cache.remove(network)
 
     def refresh_dhcp_helper(self, network_id):
         """Refresh or disable DHCP for a network depending on the current state
         of the network.
         """
+        # NOTE(jrichard): In next rebase, do not carry forward changes related
+        # to vlan subnets.
         old_network = self.cache.get_network_by_id(network_id)
         if not old_network:
             # DHCP current not running for network.
@@ -330,22 +356,47 @@ class DhcpAgent(manager.Manager):
         if not network:
             return
 
-        if not any(s for s in network.subnets if s.enable_dhcp):
-            self.disable_dhcp_helper(network.id)
-            return
         # NOTE(kevinbenton): we don't exclude dhcp disabled subnets because
         # they still change the indexes used for tags
         old_non_local_subnets = getattr(old_network, 'non_local_subnets', [])
         new_non_local_subnets = getattr(network, 'non_local_subnets', [])
-        old_cidrs = [s.cidr for s in (old_network.subnets +
-                                      old_non_local_subnets)]
-        new_cidrs = [s.cidr for s in (network.subnets +
-                                      new_non_local_subnets)]
-        if old_cidrs == new_cidrs:
-            self.call_driver('reload_allocations', network)
-            self.cache.put(network)
-        elif self.call_driver('restart', network):
-            self.cache.put(network)
+
+        old_network_subnets = old_network.subnets + old_non_local_subnets
+        new_network_subnets = network.subnets + new_non_local_subnets
+
+        new_vlans = self._get_dhcp_enabled_vlans(network)
+        if not new_vlans:
+            # DHCP not currently configured for network
+            return self.disable_dhcp_helper(network_id)
+
+        # DHCP is enabled, update each network vlan according
+        # to the current DHCP enabled state
+        old_vlans = self._get_dhcp_enabled_vlans(old_network)
+
+        added_vlans = new_vlans - old_vlans
+        for vlan_id in added_vlans:
+            driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+            self.call_driver('enable', network, **driver_kwargs)
+
+        removed_vlans = old_vlans - new_vlans
+        for vlan_id in removed_vlans:
+            driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+            self.call_driver('disable', network, **driver_kwargs)
+
+        changed_vlans = old_vlans & new_vlans
+        for vlan_id in changed_vlans:
+            old_cidrs = set(s.cidr for s in old_network_subnets
+                            if (getattr(s, wrs_net.VLAN, 0) == vlan_id))
+            new_cidrs = set(s.cidr for s in new_network_subnets
+                            if (getattr(s, wrs_net.VLAN, 0) == vlan_id))
+            driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+            if old_cidrs == new_cidrs:
+                self.call_driver('reload_allocations', network,
+                                 **driver_kwargs)
+            else:
+                self.call_driver('restart', network, **driver_kwargs)
+
+        self.cache.put(network)
         # mark all ports as active in case the sync included
         # new ports that we hadn't seen yet.
         self.dhcp_ready_ports |= {p.id for p in network.ports}
@@ -400,7 +451,24 @@ class DhcpAgent(manager.Manager):
                 return
             self.refresh_dhcp_helper(network.id)
 
+    def _get_port_subnet_ids(self, port):
+        """
+        Get the list of subnet id values that this port is servicing.
+        """
+        return [ip['subnet_id'] for ip in port['fixed_ips'] or []]
+
+    def _get_port_vlan_ids(self, port, network):
+        """
+        Get the list of DHCP enabled subnet VLAN id values this port is
+        servicing.
+        """
+        subnet_ids = self._get_port_subnet_ids(port)
+        return list(set([s.get(wrs_net.VLAN, n_const.NONE_VLAN_TAG)
+                         for s in network.subnets
+                         if s.id in subnet_ids and s.enable_dhcp]))
+
     @_wait_if_syncing
+    @utils.synchronized('dhcp-agent')
     def port_update_end(self, context, payload):
         """Handle the port.update.end notification event."""
         updated_port = dhcp.DictModel(payload['port'])
@@ -414,10 +482,9 @@ class DhcpAgent(manager.Manager):
             LOG.info("Trigger reload_allocations for port %s",
                      updated_port)
             driver_action = 'reload_allocations'
+            orig = self.cache.get_port_by_id(updated_port['id'])
+            orig = orig or {'fixed_ips': []}
             if self._is_port_on_this_agent(updated_port):
-                orig = self.cache.get_port_by_id(updated_port['id'])
-                # assume IP change if not in cache
-                orig = orig or {'fixed_ips': []}
                 old_ips = {i['ip_address'] for i in orig['fixed_ips'] or []}
                 new_ips = {i['ip_address'] for i in updated_port['fixed_ips']}
                 old_subs = {i['subnet_id'] for i in orig['fixed_ips'] or []}
@@ -436,13 +503,29 @@ class DhcpAgent(manager.Manager):
                               network.id, old_ips, new_ips)
                     driver_action = 'restart'
             self.cache.put_port(updated_port)
-            self.call_driver(driver_action, network)
-            self.dhcp_ready_ports.add(updated_port.id)
+            # NOTE(alegacy): the port may no longer have any IP addresses so if
+            # that's the case we need to use the old ones in order to determine
+            # which VLAN we are operating against.
+            orig_vlan_ids = self._get_port_vlan_ids(orig, network)
+            updated_vlan_ids = self._get_port_vlan_ids(updated_port, network)
+            for vlan_id in set(orig_vlan_ids) | set(updated_vlan_ids):
+                driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+                self.call_driver(driver_action, network, **driver_kwargs)
+                self.dhcp_ready_ports.add(updated_port.id)
 
     def _is_port_on_this_agent(self, port):
+        if port['device_owner'] != constants.DEVICE_OWNER_DHCP:
+            # This method is only used to determine whether a port which has
+            # been updated is the port used by the agent.  The name
+            # "_is_port_on_this_agent" is misleading.  A better name would have
+            # been "_is_agent_port".
+            return False
+        # Regardless of which VLAN the port is servicing the device_id will
+        # start with the same prefix so match against anything that starts with
+        # the default device_id for this agent.
         thishost = utils.get_dhcp_agent_device_id(
             port['network_id'], self.conf.host)
-        return port['device_id'] == thishost
+        return port['device_id'].startswith(thishost)
 
     # Use the update handler for the port create event.
     port_create_end = port_update_end
@@ -464,10 +547,14 @@ class DhcpAgent(manager.Manager):
                 # the agent's port has been deleted. disable the service
                 # and add the network to the resync list to create
                 # (or acquire a reserved) port.
-                self.call_driver('disable', network)
+                driver_action = 'disable'
                 self.schedule_resync("Agent port was deleted", port.network_id)
             else:
-                self.call_driver('reload_allocations', network)
+                driver_action = 'reload_allocations'
+
+            for vlan_id in self._get_port_vlan_ids(port, network):
+                driver_kwargs = {} if not vlan_id else {'vlan_id': vlan_id}
+                self.call_driver(driver_action, network, **driver_kwargs)
 
     def update_isolated_metadata_proxy(self, network):
         """Spawn or kill metadata proxy.
@@ -659,10 +746,10 @@ class NetworkCache(object):
 
         non_local_subnets = getattr(network, 'non_local_subnets', [])
         for subnet in (network.subnets + non_local_subnets):
-            del self.subnet_lookup[subnet.id]
+            self.subnet_lookup.pop(subnet.id, None)
 
         for port in network.ports:
-            del self.port_lookup[port.id]
+            self.port_lookup.pop(port.id, None)
 
     def put_port(self, port):
         network = self.get_network_by_id(port.network_id)
@@ -681,7 +768,7 @@ class NetworkCache(object):
         for index in range(len(network.ports)):
             if network.ports[index] == port:
                 del network.ports[index]
-                del self.port_lookup[port.id]
+                self.port_lookup.pop(port.id, None)
                 break
 
     def get_port_by_id(self, port_id):
@@ -748,6 +835,10 @@ class DhcpAgentWithStateReport(DhcpAgent):
             return
         except Exception:
             LOG.exception("Failed reporting state!")
+            # Ensure that we resync our state the next time we successfully
+            # connect to the server because it may have moved networks
+            # off of this agent.
+            self.needs_resync = True
             return
         if self.agent_state.pop('start_flag', None):
             self.run()
diff --git a/neutron/agent/linux/dhcp.py b/neutron/agent/linux/dhcp.py
index ba8cb23..b6fb2d9 100644
--- a/neutron/agent/linux/dhcp.py
+++ b/neutron/agent/linux/dhcp.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2016 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 import abc
 import collections
@@ -19,6 +26,7 @@ import os
 import re
 import shutil
 import time
+import uuid
 
 import netaddr
 from neutron_lib.api.definitions import extra_dhcp_opt as edo_ext
@@ -40,6 +48,8 @@ from neutron.agent.linux import iptables_manager
 from neutron.cmd import runtime_checks as checks
 from neutron.common import constants as n_const
 from neutron.common import utils as common_utils
+from neutron.extensions import wrs_net
+from neutron.extensions import wrs_provider
 from neutron.ipam import utils as ipam_utils
 
 LOG = logging.getLogger(__name__)
@@ -53,6 +63,7 @@ METADATA_DEFAULT_PREFIX = 16
 METADATA_DEFAULT_IP = '169.254.169.254'
 METADATA_DEFAULT_CIDR = '%s/%d' % (METADATA_DEFAULT_IP,
                                    METADATA_DEFAULT_PREFIX)
+METADATA_DEFAULT_IFNAME = 'lo'
 METADATA_PORT = 80
 WIN2k3_STATIC_DNS = 249
 NS_PREFIX = 'qdhcp-'
@@ -121,11 +132,12 @@ class NetModel(DictModel):
 class DhcpBase(object):
 
     def __init__(self, conf, network, process_monitor,
-                 version=None, plugin=None):
+                 version=None, plugin=None, **kwargs):
         self.conf = conf
         self.network = network
         self.process_monitor = process_monitor
-        self.device_manager = DeviceManager(self.conf, plugin)
+        self.vlan_id = kwargs.get('vlan_id', n_const.NONE_VLAN_TAG)
+        self.device_manager = DeviceManager(self.conf, plugin, **kwargs)
         self.version = version
 
     @abc.abstractmethod
@@ -177,11 +189,14 @@ class DhcpLocalProcess(DhcpBase):
     PORTS = []
 
     def __init__(self, conf, network, process_monitor, version=None,
-                 plugin=None):
-        super(DhcpLocalProcess, self).__init__(conf, network, process_monitor,
-                                               version, plugin)
+                 plugin=None, **kwargs):
+        super(DhcpLocalProcess, self).__init__(conf, network,
+                                               process_monitor,
+                                               version, plugin,
+                                               **kwargs)
         self.confs_dir = self.get_confs_dir(conf)
-        self.network_conf_dir = os.path.join(self.confs_dir, network.id)
+        self.network_conf_dir = os.path.join(self.confs_dir,
+                                             self._get_process_uuid())
         fileutils.ensure_tree(self.network_conf_dir, mode=0o755)
 
     @staticmethod
@@ -200,12 +215,34 @@ class DhcpLocalProcess(DhcpBase):
         non_local_subnets = getattr(network, 'non_local_subnets', [])
         return network.subnets + non_local_subnets
 
+    def _get_process_uuid(self):
+        """
+        Returns a UUID as a string that is unique for the current process
+        based on the current network and vlan.
+        """
+        if self.vlan_id:
+            return str(uuid.uuid5(uuid.UUID(self.network.id),
+                                  str(self.vlan_id)))
+        else:
+            return self.network.id
+
+    def _get_dhcp_enabled_subnets(self):
+        """
+        Return a list of DHCP enabled subnets on the current network and
+        vlan
+        """
+        return [s for s in self._get_all_subnets(self.network) if
+                s.enable_dhcp and getattr(s, wrs_net.VLAN, 0) == self.vlan_id]
+
     def _enable_dhcp(self):
         """check if there is a subnet within the network with dhcp enabled."""
-        for subnet in self.network.subnets:
-            if subnet.enable_dhcp:
-                return True
-        return False
+        return bool(self._get_dhcp_enabled_subnets())
+
+    def _output_network_file(self):
+        pass
+
+    def _output_vlan_file(self):
+        pass
 
     def enable(self):
         """Enables DHCP for this network by spawning a local process."""
@@ -215,12 +252,14 @@ class DhcpLocalProcess(DhcpBase):
             fileutils.ensure_tree(self.network_conf_dir, mode=0o755)
             interface_name = self.device_manager.setup(self.network)
             self.interface_name = interface_name
+            self._output_network_file()
+            self._output_vlan_file()
             self.spawn_process()
 
     def _get_process_manager(self, cmd_callback=None):
         return external_process.ProcessManager(
             conf=self.conf,
-            uuid=self.network.id,
+            uuid=self._get_process_uuid(),
             namespace=self.network.namespace,
             default_cmd_callback=cmd_callback,
             pid_file=self.get_conf_file_name('pid'),
@@ -228,7 +267,8 @@ class DhcpLocalProcess(DhcpBase):
 
     def disable(self, retain_port=False):
         """Disable DHCP for this network by killing the local process."""
-        self.process_monitor.unregister(self.network.id, DNSMASQ_SERVICE_NAME)
+        self.process_monitor.unregister(self._get_process_uuid(),
+                                        DNSMASQ_SERVICE_NAME)
         self._get_process_manager().disable()
         if not retain_port:
             self._destroy_namespace_and_port()
@@ -245,6 +285,11 @@ class DhcpLocalProcess(DhcpBase):
         if not ns_ip.netns.exists(self.network.namespace):
             LOG.debug("Namespace already deleted: %s", self.network.namespace)
             return
+        if not ns_ip.namespace_is_empty():
+            # Since the namespace is shared amongst all of the VLAN
+            # tagged subnets on the network do not delete it unless no
+            # other subnets are using it
+            return
         try:
             ns_ip.netns.delete(self.network.namespace)
         except RuntimeError:
@@ -254,6 +299,12 @@ class DhcpLocalProcess(DhcpBase):
     def _get_value_from_conf_file(self, kind, converter=None):
         """A helper function to read a value from one of the state files."""
         file_name = self.get_conf_file_name(kind)
+        return DhcpLocalProcess._read_value_from_conf_file(
+            file_name, kind, converter)
+
+    @classmethod
+    def _read_value_from_conf_file(cls, file_name, kind, converter=None):
+        """A helper function to read a value from one of the state files."""
         msg = _('Error while reading %s')
 
         try:
@@ -310,13 +361,18 @@ class Dnsmasq(DhcpLocalProcess):
     def existing_dhcp_networks(cls, conf):
         """Return a list of existing networks ids that we have configs for."""
         confs_dir = cls.get_confs_dir(conf)
-        try:
-            return [
-                c for c in os.listdir(confs_dir)
-                if uuidutils.is_uuid_like(c)
-            ]
-        except OSError:
-            return []
+        conf_uuids = [
+            c for c in os.listdir(confs_dir)
+            if uuidutils.is_uuid_like(c)
+        ]
+        networks = set()
+        for conf_uuid in conf_uuids:
+            conf_dir = os.path.join(confs_dir, conf_uuid)
+            filename = os.path.join(conf_dir, 'network')
+            network = DhcpLocalProcess._read_value_from_conf_file(
+                filename, 'network')
+            networks.add(network)
+        return networks
 
     def _build_cmdline_callback(self, pid_file):
         # We ignore local resolv.conf if dns servers are specified
@@ -336,6 +392,7 @@ class Dnsmasq(DhcpLocalProcess):
             '--dhcp-optsfile=%s' % self.get_conf_file_name('opts'),
             '--dhcp-leasefile=%s' % self.get_conf_file_name('leases'),
             '--dhcp-match=set:ipxe,175',
+            '--dhcp-authoritative',
         ]
         if self.device_manager.driver.bridged:
             cmd += [
@@ -351,7 +408,8 @@ class Dnsmasq(DhcpLocalProcess):
             ]
 
         possible_leases = 0
-        for i, subnet in enumerate(self._get_all_subnets(self.network)):
+        subnets = self._get_dhcp_enabled_subnets()
+        for i, subnet in enumerate(subnets):
             mode = None
             # if a subnet is specified to have dhcp disabled
             if not subnet.enable_dhcp:
@@ -451,7 +509,7 @@ class Dnsmasq(DhcpLocalProcess):
 
         pm.enable(reload_cfg=reload_with_HUP)
 
-        self.process_monitor.register(uuid=self.network.id,
+        self.process_monitor.register(uuid=self._get_process_uuid(),
                                       service_name=DNSMASQ_SERVICE_NAME,
                                       monitored_process=pm)
 
@@ -565,13 +623,16 @@ class Dnsmasq(DhcpLocalProcess):
             no_opts,  # A flag indication that options shouldn't be written
         )
         """
+        subnets = self._get_dhcp_enabled_subnets()
+        subnet_ids = [s.id for s in subnets]
+
         v6_nets = dict((subnet.id, subnet) for subnet in
-                       self._get_all_subnets(self.network)
-                       if subnet.ip_version == 6)
+                       subnets if subnet.ip_version == 6)
 
         for port in self.network.ports:
             fixed_ips = self._sort_fixed_ips_for_dnsmasq(port.fixed_ips,
                                                          v6_nets)
+            fixed_ips = [ip for ip in fixed_ips if ip.subnet_id in subnet_ids]
             # Confirm whether Neutron server supports dns_name attribute in the
             # ports API
             dns_assignment = getattr(port, 'dns_assignment', None)
@@ -631,8 +692,7 @@ class Dnsmasq(DhcpLocalProcess):
         else:
             timestamp = int(time.time()) + self.conf.dhcp_lease_duration
         dhcp_enabled_subnet_ids = [s.id for s in
-                                   self._get_all_subnets(self.network)
-                                   if s.enable_dhcp]
+                                   self._get_dhcp_enabled_subnets()]
         for host_tuple in self._iter_hosts():
             port, alloc, hostname, name, no_dhcp, no_opts = host_tuple
             # don't write ip address which belongs to a dhcp disabled subnet
@@ -660,6 +720,16 @@ class Dnsmasq(DhcpLocalProcess):
             return '[%s]' % address
         return address
 
+    def _output_network_file(self):
+        """Write the network id to the conf directory for process state"""
+        network_file_path = self.get_conf_file_name('network')
+        file_utils.replace_file(network_file_path, self.network.id)
+
+    def _output_vlan_file(self):
+        """Write the vlan id to the conf directory for process state"""
+        vlan_file_path = self.get_conf_file_name('vlan')
+        file_utils.replace_file(vlan_file_path, str(self.vlan_id))
+
     def _output_hosts_file(self):
         """Writes a dnsmasq compatible dhcp hosts file.
 
@@ -681,8 +751,7 @@ class Dnsmasq(DhcpLocalProcess):
 
         LOG.debug('Building host file: %s', filename)
         dhcp_enabled_subnet_ids = [s.id for s in
-                                   self._get_all_subnets(self.network)
-                                   if s.enable_dhcp]
+                                   self._get_dhcp_enabled_subnets()]
         # NOTE(ihrachyshka): the loop should not log anything inside it, to
         # avoid potential performance drop when lots of hosts are dumped
         for host_tuple in self._iter_hosts():
@@ -868,19 +937,30 @@ class Dnsmasq(DhcpLocalProcess):
         file_utils.replace_file(name, '\n'.join(options))
         return name
 
+    def _is_same_vlan(self, a, b):
+        return bool(getattr(a, wrs_net.VLAN, 0) == getattr(b, wrs_net.VLAN, 0))
+
     def _generate_opts_per_subnet(self):
+        # Obtain the provider MTU if available
+        mtu = getattr(self.network, wrs_provider.MTU, None)
         options = []
         subnet_index_map = {}
         if self.conf.enable_isolated_metadata or self.conf.force_metadata:
             subnet_to_interface_ip = self._make_subnet_interface_ip_map()
         isolated_subnets = self.get_isolated_subnets(self.network)
-        for i, subnet in enumerate(self._get_all_subnets(self.network)):
+        subnets = self._get_dhcp_enabled_subnets()
+        for i, subnet in enumerate(subnets):
             addr_mode = getattr(subnet, 'ipv6_address_mode', None)
             segment_id = getattr(subnet, 'segment_id', None)
             if (not subnet.enable_dhcp or
                 (subnet.ip_version == 6 and
                  addr_mode == constants.IPV6_SLAAC)):
                 continue
+            # Configure the MTU option if applicable
+            if mtu and subnet.ip_version == 4:
+                options.append(
+                    self._format_option(
+                        subnet.ip_version, i, 'mtu', str(mtu)))
             if subnet.dns_nameservers:
                 options.append(
                     self._format_option(
@@ -925,6 +1005,7 @@ class Dnsmasq(DhcpLocalProcess):
                 for s in self._get_all_subnets(self.network):
                     sub_segment_id = getattr(s, 'segment_id', None)
                     if (s.ip_version == 4 and
+                            self._is_same_vlan(s, subnet) and
                             s.cidr != subnet.cidr and
                             sub_segment_id == segment_id):
                         host_routes.append("%s,0.0.0.0" % s.cidr)
@@ -1113,9 +1194,10 @@ class Dnsmasq(DhcpLocalProcess):
 
 class DeviceManager(object):
 
-    def __init__(self, conf, plugin):
+    def __init__(self, conf, plugin, **kwargs):
         self.conf = conf
         self.plugin = plugin
+        self.vlan_id = kwargs.get('vlan_id', n_const.NONE_VLAN_TAG)
         self.driver = agent_common_utils.load_interface_driver(conf)
 
     def get_interface_name(self, network, port):
@@ -1126,8 +1208,17 @@ class DeviceManager(object):
         """Return a unique DHCP device ID for this host on the network."""
         # There could be more than one dhcp server per network, so create
         # a device id that combines host and network ids
+        kwargs = {}
+        if self.vlan_id:
+            kwargs['vlan_id'] = self.vlan_id
         return common_utils.get_dhcp_agent_device_id(network.id,
-                                                     self.conf.host)
+                                                     self.conf.host,
+                                                     **kwargs)
+
+    def _get_network_subnets(self, network):
+        """Return a list of subnets on the current network and vlan"""
+        return [s for s in network.subnets if getattr(s,
+                   wrs_net.VLAN, n_const.NONE_VLAN_TAG) == self.vlan_id]
 
     def _set_default_route_ip_version(self, network, device_name, ip_version):
         device = ip_lib.IPDevice(device_name, namespace=network.namespace)
@@ -1135,7 +1226,8 @@ class DeviceManager(object):
         if gateway:
             gateway = gateway.get('gateway')
 
-        for subnet in network.subnets:
+        subnets = self._get_network_subnets(network)
+        for subnet in subnets:
             skip_subnet = (
                 subnet.ip_version != ip_version
                 or not subnet.enable_dhcp
@@ -1254,23 +1346,58 @@ class DeviceManager(object):
             port = self.plugin.update_dhcp_port(
                 port.id,
                 {'port': {'network_id': network.id,
-                          'fixed_ips': wanted_fixed_ips}})
+                          'fixed_ips': wanted_fixed_ips,
+                          'device_id': device_id}})
             if not port:
                 raise exceptions.Conflict()
 
         return port
 
+    def _setup_same_reserved_dhcp_port(self, network, device_id, dhcp_subnets):
+        """Setup the same reserved DHCP port that was previously used for this
+        same network+subnets, if there is one.
+        """
+        LOG.debug('DHCP port %(device_id)s on network %(network_id)s'
+                  ' does not yet exist. Checking for a reserved port with'
+                  ' the same subnet(s).',
+                  {'device_id': device_id, 'network_id': network.id})
+        # Compare what the subnets should be against what is already
+        # on the port.
+        dhcp_enabled_subnet_ids = set(dhcp_subnets)
+
+        for port in network.ports:
+            port_device_id = getattr(port, 'device_id', None)
+            if port_device_id != n_const.DEVICE_ID_RESERVED_DHCP_PORT:
+                continue
+            port_subnet_ids = set(ip.subnet_id for ip in port.fixed_ips)
+            if set(port_subnet_ids) != set(dhcp_enabled_subnet_ids):
+                    continue
+            try:
+                port = self.plugin.update_dhcp_port(
+                    port.id, {'port': {'network_id': network.id,
+                                       'device_id': device_id}})
+            except oslo_messaging.RemoteError as e:
+                if e.exc_type == 'DhcpPortInUse':
+                    LOG.info("Skipping DHCP port %s as it is "
+                             "already in use", port.id)
+                    continue
+                raise
+            if port:
+                return port
+
     def _setup_reserved_dhcp_port(self, network, device_id, dhcp_subnets):
         """Setup the reserved DHCP port, if there is one."""
         LOG.debug('DHCP port %(device_id)s on network %(network_id)s'
                   ' does not yet exist. Checking for a reserved port.',
                   {'device_id': device_id, 'network_id': network.id})
+        unique_ip_subnets = [dict(subnet_id=s) for s in dhcp_subnets]
         for port in network.ports:
             port_device_id = getattr(port, 'device_id', None)
             if port_device_id == n_const.DEVICE_ID_RESERVED_DHCP_PORT:
                 try:
                     port = self.plugin.update_dhcp_port(
                         port.id, {'port': {'network_id': network.id,
+                                           'fixed_ips': unique_ip_subnets,
                                            'device_id': device_id}})
                 except oslo_messaging.RemoteError as e:
                     if e.exc_type == 'DhcpPortInUse':
@@ -1309,17 +1436,19 @@ class DeviceManager(object):
         # The ID that the DHCP port will have (or already has).
         device_id = self.get_device_id(network)
 
-        # Get the set of DHCP-enabled local subnets on this network.
-        dhcp_subnets = {subnet.id: subnet for subnet in network.subnets
+        # Get the set of DHCP-enabled subnets on this network.
+        subnets = self._get_network_subnets(network)
+        dhcp_subnets = {subnet.id: subnet for subnet in subnets
                         if subnet.enable_dhcp}
 
-        # There are 3 cases: either the DHCP port already exists (but
-        # might need to be updated for a changed set of subnets); or
-        # some other code has already prepared a 'reserved' DHCP port,
-        # and we just need to adopt that; or we need to create a new
-        # DHCP port.  Try each of those in turn until we have a DHCP
-        # port.
+        # There are 4 cases: either the DHCP port already exists (but might
+        # need to be updated for a changed set of subnets); or a previous agent
+        # released a port that is now reserved and can be reused as is; or some
+        # other code has already prepared a 'reserved' DHCP port, and we just
+        # need to adopt that; or we need to create a new DHCP port.  Try each
+        # of those in turn until we have a DHCP port.
         for setup_method in (self._setup_existing_dhcp_port,
+                             self._setup_same_reserved_dhcp_port,
                              self._setup_reserved_dhcp_port,
                              self._setup_new_dhcp_port):
             dhcp_port = setup_method(network, device_id, dhcp_subnets)
@@ -1456,19 +1585,27 @@ class DeviceManager(object):
                     ip_cidrs.append('%s/%s' % (gateway, net.prefixlen))
 
         if self.conf.force_metadata or self.conf.enable_isolated_metadata:
-            ip_cidrs.append(METADATA_DEFAULT_CIDR)
+            meta_cidrs = ["%s/32" % METADATA_DEFAULT_IP]
+            self.driver.init_l3(METADATA_DEFAULT_IFNAME, meta_cidrs,
+                                namespace=network.namespace)
 
         self.driver.init_l3(interface_name, ip_cidrs,
                             namespace=network.namespace)
 
-        self._set_default_route(network, interface_name)
-        self._cleanup_stale_devices(network, port)
+        if not self.vlan_id:
+            self._set_default_route(network, interface_name)
+        # NOTE(alegacy): the stale device cleanup has been commented out
+        # because with VLAN subnets there can be more than 1 port in a
+        # namespace and this is causing the agent to delete ports that we
+        # actually need.
+        #self._cleanup_stale_devices(network, port)
 
         return interface_name
 
     def update(self, network, device_name):
         """Update device settings for the network's DHCP on this host."""
-        self._set_default_route(network, device_name)
+        if not self.vlan_id:
+            self._set_default_route(network, device_name)
 
     def unplug(self, device_name, network):
         """Unplug device settings for the network's DHCP on this host."""
@@ -1481,8 +1618,7 @@ class DeviceManager(object):
         else:
             LOG.debug('No interface exists for network %s', network.id)
 
-        self.plugin.release_dhcp_port(network.id,
-                                      self.get_device_id(network))
+        self.plugin.release_dhcp_port(network.id, self.get_device_id(network))
 
     def fill_dhcp_udp_checksums(self, namespace):
         """Ensure DHCP reply packets always have correct UDP checksums."""
diff --git a/neutron/agent/linux/interface.py b/neutron/agent/linux/interface.py
index ade5667..2aa678c 100644
--- a/neutron/agent/linux/interface.py
+++ b/neutron/agent/linux/interface.py
@@ -123,7 +123,9 @@ class LinuxInterfaceDriver(object):
         # The LLA generated by the operating system is not known to
         # Neutron, so it would be deleted if we added it to the 'previous'
         # list here
-        default_ipv6_lla = ip_lib.get_ipv6_lladdr(device.link.address)
+        # wrs: In the case of link local, then skip
+        default_ipv6_lla = (ip_lib.get_ipv6_lladdr(device.link.address)
+                            if device.link.address else None)
 
         cidrs = set()
         remove_ips = set()
diff --git a/neutron/agent/linux/ip_lib.py b/neutron/agent/linux/ip_lib.py
index 18bb6a1..b34706f 100644
--- a/neutron/agent/linux/ip_lib.py
+++ b/neutron/agent/linux/ip_lib.py
@@ -45,6 +45,7 @@ IP_NONLOCAL_BIND = 'net.ipv4.ip_nonlocal_bind'
 
 LOOPBACK_DEVNAME = 'lo'
 GRE_TUNNEL_DEVICE_NAMES = ['gre0', 'gretap0']
+SIT_DEVNAME_PREFIX = 'sit'
 
 SYS_NET_PATH = '/sys/class/net'
 DEFAULT_GW_PATTERN = re.compile(r"via (\S+)")
@@ -123,7 +124,8 @@ class IPWrapper(SubProcessBase):
     def device(self, name):
         return IPDevice(name, namespace=self.namespace)
 
-    def get_devices(self, exclude_loopback=True, exclude_gre_devices=True):
+    def get_devices(self, exclude_loopback=True, exclude_gre_devices=True,
+                    exclude_sit=False):
         retval = []
         if self.namespace:
             # we call out manually because in order to avoid screen scraping
@@ -154,6 +156,8 @@ class IPWrapper(SubProcessBase):
             if (exclude_loopback and name == LOOPBACK_DEVNAME or
                     exclude_gre_devices and name in GRE_TUNNEL_DEVICE_NAMES):
                 continue
+            if exclude_sit and name.startswith(SIT_DEVNAME_PREFIX):
+                continue
             retval.append(IPDevice(name, namespace=self.namespace))
 
         return retval
@@ -216,7 +220,7 @@ class IPWrapper(SubProcessBase):
         return ip
 
     def namespace_is_empty(self):
-        return not self.get_devices()
+        return not self.get_devices(exclude_sit=True)
 
     def garbage_collect_namespace(self):
         """Conditionally destroy the namespace if it is empty."""
diff --git a/neutron/api/rpc/handlers/dhcp_rpc.py b/neutron/api/rpc/handlers/dhcp_rpc.py
index 493bf5a..aed76cc 100644
--- a/neutron/api/rpc/handlers/dhcp_rpc.py
+++ b/neutron/api/rpc/handlers/dhcp_rpc.py
@@ -287,9 +287,7 @@ class DhcpRpcCallback(object):
         try:
             old_port = plugin.get_port(context, port['id'])
             if (old_port['device_id'] != n_const.DEVICE_ID_RESERVED_DHCP_PORT
-                and old_port['device_id'] !=
-                utils.get_dhcp_agent_device_id(port['port']['network_id'],
-                                               host)):
+                and old_port['device_id'] != port['port']['device_id']):
                 raise n_exc.DhcpPortInUse(port_id=port['id'])
             LOG.debug('Update dhcp port %(port)s '
                       'from %(host)s.',
diff --git a/neutron/common/constants.py b/neutron/common/constants.py
index 76cf54f..c232764 100644
--- a/neutron/common/constants.py
+++ b/neutron/common/constants.py
@@ -258,3 +258,6 @@ TYPE_QOS_RATELIMIT = "ratelimit"
 TYPE_QOS_SCHEDULER = "scheduler"
 
 QOS_SCHEDULER_POLICY_WEIGHT = "weight"
+
+# VLAN Subnets
+NONE_VLAN_TAG = 0
diff --git a/neutron/common/utils.py b/neutron/common/utils.py
index 9a5612f..ec11652 100644
--- a/neutron/common/utils.py
+++ b/neutron/common/utils.py
@@ -13,6 +13,12 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 # Borrowed from nova code base, more utilities will be added/borrowed as and
 # when needed.
 
@@ -155,13 +161,17 @@ def get_random_mac(base_mac):
     return net.get_random_mac(base_mac)
 
 
-def get_dhcp_agent_device_id(network_id, host):
+def get_dhcp_agent_device_id(network_id, host, vlan_id=0):
     # Split host so as to always use only the hostname and
     # not the domain name. This will guarantee consistency
     # whether a local hostname or an fqdn is passed in.
     local_hostname = host.split('.')[0]
     host_uuid = uuid.uuid5(uuid.NAMESPACE_DNS, str(local_hostname))
-    return 'dhcp%s-%s' % (host_uuid, network_id)
+    device_id = 'dhcp%s-%s' % (host_uuid, network_id)
+    # append the vlan to the device name if one is specified
+    if vlan_id:
+        device_id = '%s-%s' % (device_id, vlan_id)
+    return device_id
 
 
 class exception_logger(object):
diff --git a/neutron/conf/common.py b/neutron/conf/common.py
index 0979761..fdfa3f0 100644
--- a/neutron/conf/common.py
+++ b/neutron/conf/common.py
@@ -72,6 +72,13 @@ core_opts = [
                help=_("Maximum number of DNS nameservers per subnet")),
     cfg.IntOpt('max_subnet_host_routes', default=20,
                help=_("Maximum number of host routes per subnet")),
+    cfg.IntOpt('max_fixed_ips_per_port', default=5,
+               deprecated_for_removal=True,
+               help=_("Maximum number of fixed ips per port. This option "
+                      "is deprecated and will be removed in the Ocata "
+                      "release.")),
+    cfg.IntOpt('max_vlan_ips_per_port', default=5,
+               help=_("Maximum number of vlan based ips per port.")),
     cfg.BoolOpt('ipv6_pd_enabled', default=False,
                 help=_("Enables IPv6 Prefix Delegation for automatic subnet "
                        "CIDR allocation. "
diff --git a/neutron/db/agentschedulers_db.py b/neutron/db/agentschedulers_db.py
index 6ba4fca..299bb6c 100644
--- a/neutron/db/agentschedulers_db.py
+++ b/neutron/db/agentschedulers_db.py
@@ -444,14 +444,16 @@ class DhcpAgentSchedulerDbMixin(dhcpagentscheduler
         # reserve the port, so the ip is reused on a subsequent add
         device_id = utils.get_dhcp_agent_device_id(network_id,
                                                    agent['host'])
-        filters = dict(device_id=[device_id])
+        filters = dict(network_id=[network_id],
+                       device_owner=[constants.DEVICE_OWNER_DHCP])
         ports = self.get_ports(context, filters=filters)
         # NOTE(kevinbenton): there should only ever be one port per
         # DHCP agent per network so we don't have to worry about one
         # update_port passing and another failing
         for port in ports:
-            port['device_id'] = n_const.DEVICE_ID_RESERVED_DHCP_PORT
-            self.update_port(context, port['id'], dict(port=port))
+            if port['device_id'].startswith(device_id):
+                port['device_id'] = n_const.DEVICE_ID_RESERVED_DHCP_PORT
+                self.update_port(context, port['id'], dict(port=port))
         with context.session.begin():
             context.session.delete(binding)
 
diff --git a/neutron/db/db_base_plugin_common.py b/neutron/db/db_base_plugin_common.py
index 3c6b980..c7bca8c 100644
--- a/neutron/db/db_base_plugin_common.py
+++ b/neutron/db/db_base_plugin_common.py
@@ -35,6 +35,7 @@ from neutron.db import _utils as db_utils
 from neutron.db import api as db_api
 from neutron.db import common_db_mixin
 from neutron.db import models_v2
+from neutron.extensions import wrs_net
 from neutron.objects import ports as port_obj
 from neutron.objects import subnet as subnet_obj
 from neutron.objects import subnetpool as subnetpool_obj
@@ -148,6 +149,7 @@ class DbBasePluginCommon(common_db_mixin.CommonDbMixin):
                'host_routes': [{'destination': route['destination'],
                                 'nexthop': route['nexthop']}
                                for route in subnet['routes']],
+               wrs_net.VLAN: subnet['vlan_id'],
                }
         # The shared attribute for a subnet is the same as its parent network
         res['shared'] = self._is_network_shared(context, subnet.rbac_entries)
@@ -299,7 +301,8 @@ class DbBasePluginCommon(common_db_mixin.CommonDbMixin):
                 'subnetpool_id': subnetpool_id,
                 'enable_dhcp': subnet['enable_dhcp'],
                 'gateway_ip': gateway_ip,
-                'description': subnet.get('description')}
+                'description': subnet.get('description'),
+                'vlan_id': subnet[wrs_net.VLAN]}
         if subnet['ip_version'] == 6 and subnet['enable_dhcp']:
             if validators.is_attr_set(subnet['ipv6_ra_mode']):
                 args['ipv6_ra_mode'] = subnet['ipv6_ra_mode']
diff --git a/neutron/db/db_base_plugin_v2.py b/neutron/db/db_base_plugin_v2.py
index e17fdb2..a4e8ca4 100644
--- a/neutron/db/db_base_plugin_v2.py
+++ b/neutron/db/db_base_plugin_v2.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2016 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 import functools
 
@@ -55,6 +62,7 @@ from neutron.db import rbac_db_models as rbac_db
 from neutron.db import standardattrdescription_db as stattr_db
 from neutron.extensions import ip_allocation as ipa
 from neutron.extensions import l3
+from neutron.extensions import wrs_net
 from neutron import ipam
 from neutron.ipam import exceptions as ipam_exc
 from neutron.ipam import subnet_alloc
@@ -604,6 +612,12 @@ class NeutronDbPluginV2(db_base_plugin_common.DbBasePluginCommon,
             for rt in s['host_routes']:
                 self._validate_host_route(rt, ip_ver)
 
+        if validators.is_attr_set(s.get(wrs_net.VLAN)):
+            if cur_subnet:
+                if cur_subnet[wrs_net.VLAN] != s.get(wrs_net.VLAN):
+                    error_message = _("VLAN of a subnet can not be modified")
+                    raise exc.InvalidInput(error_message=error_message)
+
         if ip_ver == 4:
             if validators.is_attr_set(s.get('ipv6_ra_mode')):
                 raise exc.InvalidInput(
@@ -682,6 +696,7 @@ class NeutronDbPluginV2(db_base_plugin_common.DbBasePluginCommon,
             self._update_router_gw_ports(context,
                                          network,
                                          result)
+        updated_ports = []
         # If this subnet supports auto-addressing, then update any
         # internal ports on the network with addresses for this subnet.
         if ipv6_utils.is_auto_address_subnet(result):
@@ -762,6 +777,8 @@ class NeutronDbPluginV2(db_base_plugin_common.DbBasePluginCommon,
             net = netaddr.IPNetwork(s['cidr'])
             subnet['subnet']['cidr'] = '%s/%s' % (net.network, net.prefixlen)
 
+        subnet['subnet'][wrs_net.VLAN] = n_const.NONE_VLAN_TAG
+
         subnetpool_id = self._get_subnetpool_id(context, s)
         if not subnetpool_id and not has_cidr:
             msg = _('a subnetpool must be specified in the absence of a cidr')
@@ -933,8 +950,23 @@ class NeutronDbPluginV2(db_base_plugin_common.DbBasePluginCommon,
     @db_api.context_manager.reader
     def _subnet_get_user_allocation(self, context, subnet_id):
         """Check if there are any user ports on subnet and return first."""
-        return port_obj.IPAllocation.get_alloc_by_subnet_id(
-            context, subnet_id, AUTO_DELETE_PORT_OWNERS)
+        # need to join with ports table as IPAllocation's port
+        # is not joined eagerly and thus producing query which yields
+        # incorrect results
+        # NOTE(alegacy): at this point an assumption is made that there are no
+        # router ports therefore they only exclude the auto delete ports (e.g.,
+        # DHCP) and assume that everything else is a compute port.  For our
+        # purpose we do not want to block the delete if any of those compute
+        # ports has any vlan IP allocations (i.e., we let those get auto
+        # deleted).
+        return (context.session.query(models_v2.IPAllocation).
+                filter_by(subnet_id=subnet_id).
+                join(models_v2.Port).
+                join(models_v2.Subnet).
+                filter(and_(~models_v2.Port.device_owner.
+                            in_(AUTO_DELETE_PORT_OWNERS),
+                            models_v2.Subnet.vlan_id == 0)).
+                first())
 
     @db_api.context_manager.reader
     def _subnet_check_ip_allocations_internal_router_ports(self, context,
diff --git a/neutron/db/ipam_backend_mixin.py b/neutron/db/ipam_backend_mixin.py
index e7009da..f1ec63b 100644
--- a/neutron/db/ipam_backend_mixin.py
+++ b/neutron/db/ipam_backend_mixin.py
@@ -583,6 +583,10 @@ class IpamBackendMixin(db_base_plugin_common.DbBasePluginCommon):
         query = model_query.get_collection_query(context, models_v2.Subnet)
         return query.filter(models_v2.Subnet.network_id == network_id)
 
+    def _query_subnets_count(self, context, filters=None):
+        return self._get_collection_count(context, models_v2.Subnet,
+                                          filters=filters)
+
     def _query_filter_service_subnets(self, query, service_type):
         # TODO(korzen) use SubnetServiceType OVO here
         Subnet = models_v2.Subnet
diff --git a/neutron/db/ipam_pluggable_backend.py b/neutron/db/ipam_pluggable_backend.py
index b71208b..80307d2 100644
--- a/neutron/db/ipam_pluggable_backend.py
+++ b/neutron/db/ipam_pluggable_backend.py
@@ -13,12 +13,17 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+import collections
 import copy
+import itertools
 
 import netaddr
+import six
+
 from neutron_lib.api.definitions import portbindings
 from neutron_lib import constants
 from neutron_lib import exceptions as n_exc
+from oslo_config import cfg
 from oslo_db import exception as db_exc
 from oslo_log import log as logging
 from oslo_utils import excutils
@@ -29,8 +34,10 @@ from neutron.common import ipv6_utils
 from neutron.db import api as db_api
 from neutron.db import ipam_backend_mixin
 from neutron.db import models_v2
+from neutron.extensions import wrs_net
 from neutron.ipam import driver
 from neutron.ipam import exceptions as ipam_exc
+from neutron.ipam import utils as ipam_utils
 from neutron.objects import ports as port_obj
 from neutron.objects import subnet as obj_subnet
 
@@ -192,28 +199,49 @@ class IpamPluggableBackend(ipam_backend_mixin.IpamBackendMixin):
                                         ipam_driver, port_copy['port'], ips,
                                         revert_on_fail=False)
 
-    def _allocate_ips_for_port(self, context, port):
-        """Allocate IP addresses for the port. IPAM version.
+    def _filter_ips_by_subnets(self, fixed_ips, subnets):
+        """Filter the list of fixed_ips to be those from subnets that exist
+        within the provided subnet list.  This is so that
+        _test_fixed_ips_for_port() does not think that entries from different
+        vlans are not invalid.
+        """
+        result = []
+
+        def get_matching_subnet(fixed_ip):
+            for subnet in subnets:
+                if 'subnet_id' in fixed_ip:
+                    if subnet['id'] == fixed_ip['subnet_id']:
+                        return subnet
+                elif 'ip_address' in fixed_ip:
+                    if ipam_utils.check_subnet_ip(subnet['cidr'],
+                                                  fixed_ip['ip_address']):
+                        return subnet
+
+        for fixed_ip in fixed_ips:
+            if not get_matching_subnet(fixed_ip):
+                # Could be invalid, but also could just be from a different
+                # vlan so exclude it.
+                continue
+            result.append(fixed_ip)
+        return result
+
+    def _allocate_ips_for_port_by_subnets(self, context, p, subnets):
+        """Allocate IP addresses for the port for a given vlan. IPAM version.
 
         If port['fixed_ips'] is set to 'ATTR_NOT_SPECIFIED', allocate IP
         addresses for the port. If port['fixed_ips'] contains an IP address or
         a subnet_id then allocate an IP address accordingly.
         """
-        p = port['port']
         fixed_configured = p['fixed_ips'] is not constants.ATTR_NOT_SPECIFIED
-        subnets = self._ipam_get_subnets(context,
-                                         network_id=p['network_id'],
-                                         host=p.get(portbindings.HOST_ID),
-                                         service_type=p.get('device_owner'),
-                                         fixed_configured=fixed_configured)
 
         v4, v6_stateful, v6_stateless = self._classify_subnets(
             context, subnets)
 
         if fixed_configured:
+            fixed_ips = self._filter_ips_by_subnets(p['fixed_ips'], subnets)
             ips = self._test_fixed_ips_for_port(context,
                                                 p["network_id"],
-                                                p['fixed_ips'],
+                                                fixed_ips,
                                                 p['device_owner'],
                                                 subnets)
         else:
@@ -246,6 +274,66 @@ class IpamPluggableBackend(ipam_backend_mixin.IpamBackendMixin):
                                 'mac': port['mac_address']})
         return ips
 
+    def _group_subnets_by_vlan(self, subnets):
+        """
+        Accept a list of subnets as an argument and triage that by vlan_id.
+        """
+        vlan_subnets = collections.defaultdict(list)
+        for s in subnets:
+            vlan_subnets[s.get(wrs_net.VLAN, 0)].append(s)
+        return vlan_subnets
+
+    def _diff_fixed_ip_list(self, a, b):
+        r = list(itertools.ifilterfalse(lambda x: x in a, b))
+        r += list(itertools.ifilterfalse(lambda x: x in b, a))
+        return r
+
+    def _allocate_ips_for_port(self, context, port):
+        """Allocate IP addresses for the port. IPAM version.
+
+        If port['fixed_ips'] is set to 'ATTR_NOT_SPECIFIED', allocate IP
+        addresses for the port. If port['fixed_ips'] contains an IP address or
+        a subnet_id then allocate an IP address accordingly.
+        """
+        p = port['port']
+        fixed_configured = p['fixed_ips'] is not constants.ATTR_NOT_SPECIFIED
+        subnets = self._ipam_get_subnets(context,
+                                         network_id=p['network_id'],
+                                         host=p.get(portbindings.HOST_ID),
+                                         service_type=p.get('device_owner'),
+                                         fixed_configured=fixed_configured)
+        if fixed_configured:
+            # NOTE(alegacy): Because of how we allocate ips seperately for each
+            # layer of vlan subnets we need to do some upfront validation.
+            # This is because one of 2 things could happen.  A subnet that is
+            # intended for a specific vlan will appear as an error on all of
+            # the other subnets, or an ip address request will get ignored
+            # altogether.  So filter the list of fixed_ips to find those that
+            # do not match any subnets.  Those are the ones that will cause an
+            # error.  We could just raise an exception directly here but the
+            # unit tests except different exception types for different types
+            # of failures.  Rather than duplicate the exception raising code
+            # just pass these erroneous fixed_ip entries to the _get_subnet
+            # function and let it raise the exceptions as they are expected by
+            # the unit tests.
+            fixed_ips = self._filter_ips_by_subnets(p['fixed_ips'], subnets)
+            if len(fixed_ips) != len(p['fixed_ips']):
+                delta = self._diff_fixed_ip_list(p['fixed_ips'], fixed_ips)
+                for fixed in delta:
+                    self._get_subnet_for_fixed_ip(context, fixed, subnets)
+        ips = []
+        grouped_subnets = self._group_subnets_by_vlan(subnets)
+        for vlan_id, subnets in six.iteritems(grouped_subnets):
+            if len(ips) >= cfg.CONF.max_vlan_ips_per_port:
+                # Automatically add IP addresses for the first X VLAN subnets.
+                # If there are more subnets in the system then the user needs
+                # to manage the IP address assignments manually.  This
+                # condition exist to limit the number of active VLAN interfaces
+                # in vswitch.
+                continue
+            ips += self._allocate_ips_for_port_by_subnets(context, p, subnets)
+        return ips
+
     def _test_fixed_ips_for_port(self, context, network_id, fixed_ips,
                                  device_owner, subnets):
         """Test fixed IPs for port.
@@ -456,6 +544,8 @@ class IpamPluggableBackend(ipam_backend_mixin.IpamBackendMixin):
             port_qry = context.session.query(models_v2.Port)
             ports = port_qry.filter(
                 and_(models_v2.Port.network_id == network_id,
+                     models_v2.Port.device_owner !=
+                     constants.DEVICE_OWNER_DHCP,
                      ~models_v2.Port.device_owner.in_(
                          constants.ROUTER_INTERFACE_OWNERS_SNAT)))
             updated_ports = []
@@ -495,6 +585,67 @@ class IpamPluggableBackend(ipam_backend_mixin.IpamBackendMixin):
                               port['id'])
             return updated_ports
 
+    def _is_deviceowner_compute(self, port):
+        return port['device_owner'].startswith(
+            constants.DEVICE_OWNER_COMPUTE_PREFIX)
+
+    def _allocate_ips_for_subnet(self, context, subnet, ipam_subnet):
+        """Allocates an IP from the subnet for each port in the same network"""
+        network_id = subnet['network_id']
+        ports = (context.session.query(models_v2.Port).
+                 filter(models_v2.Port.network_id == network_id))
+        compute_ports = [p for p in ports if self._is_deviceowner_compute(p)]
+        ipam_driver = driver.Pool.get_instance(None, context)
+        factory = ipam_driver.get_address_request_factory()
+        updated_ports = []
+        for port in compute_ports:
+            count = len(port.get('fixed_ips', []))
+            if count >= cfg.CONF.max_vlan_ips_per_port:
+                # Automatically add IP addresses for the first X VLAN subnets.
+                # If there are more subnets in the system then the user needs
+                # to manage the IP address assignments manually.
+                continue
+            ip = {'subnet_id': subnet['id']}
+            ip_request = factory.get_request(context, port, ip)
+            ip_address = ipam_subnet.allocate(ip_request)
+            allocated = models_v2.IPAllocation(network_id=network_id,
+                                               port_id=port['id'],
+                                               ip_address=ip_address,
+                                               subnet_id=subnet['id'])
+            try:
+                # Do the insertion of each IP allocation entry within
+                # the context of a nested transaction, so that the entry
+                # is rolled back independently of other entries whenever
+                # the corresponding port has been deleted.
+                with context.session.begin_nested():
+                    context.session.add(allocated)
+                updated_ports.append(port['id'])
+            except db_exc.DBReferenceError:
+                LOG.debug("Port %s was deleted while updating it with an "
+                          "VLAN subnet address. Ignoring.", port['id'])
+                LOG.debug("Reverting IP allocation for %s", ip_address)
+                # Do not fail if reverting allocation was unsuccessful
+                try:
+                    ipam_subnet.deallocate(ip_address)
+                except Exception:
+                    LOG.debug("Reverting IP allocation failed for %s",
+                              ip_address)
+        return updated_ports
+
+    def add_vlan_addrs_on_network_ports(self, context, subnet, ipam_subnet):
+        """For a vlan tagged subnet, add addrs for ports on the network."""
+        with context.session.begin(subtransactions=True):
+            filters = dict(network_id=[subnet['network_id']],
+                           vlan_id=[subnet.get(wrs_net.VLAN, 0)])
+            count = self._query_subnets_count(context, filters=filters)
+            if count == 1:
+                # This is the first subnet on this network for the given VLAN
+                # id value therefore allocate an IP address; we do not add
+                # multiple IP addresses on the same VLAN to the same port.
+                return self._allocate_ips_for_subnet(
+                    context, subnet, ipam_subnet)
+        return []
+
     def allocate_subnet(self, context, network, subnet, subnetpool_id):
         subnetpool = None
 
diff --git a/neutron/db/l3_db.py b/neutron/db/l3_db.py
index 081d012..69d7bee 100644
--- a/neutron/db/l3_db.py
+++ b/neutron/db/l3_db.py
@@ -58,6 +58,7 @@ from neutron.db import models_v2
 from neutron.db import standardattrdescription_db as st_attr
 from neutron.extensions import external_net
 from neutron.extensions import l3
+from neutron.extensions import wrs_net
 from neutron.objects import ports as port_obj
 from neutron.objects import router as l3_obj
 from neutron.plugins.common import utils as p_utils
@@ -774,7 +775,8 @@ class L3_NAT_dbonly_mixin(l3.RouterPluginBase,
             if netaddr.IPNetwork(fixed_ip['ip_address']).version == 6:
                 return True
 
-    def _find_ipv6_router_port_by_network(self, context, router, net_id):
+    def _find_ipv6_router_port_by_network(self, context, router, net_id,
+                                          vlan_id=0):
         router_dev_owner = self._get_device_owner(context, router)
         for port in router.attached_ports:
             p = port['port']
@@ -782,7 +784,12 @@ class L3_NAT_dbonly_mixin(l3.RouterPluginBase,
                 # we don't want any special purpose internal ports
                 continue
             if p['network_id'] == net_id and self._port_has_ipv6_address(p):
-                return port
+                # Each port can only be associated to subnets of the same VLAN
+                # so it is sufficient to query for the first subnet in the list
+                subnet_id = p['fixed_ips'][0]['subnet_id']
+                subnet = self._core_plugin._get_subnet(context, subnet_id)
+                if subnet['vlan_id'] == vlan_id:
+                    return port
 
     def _add_interface_by_subnet(self, context, router, subnet_id, owner):
         subnet = self._core_plugin.get_subnet(context, subnet_id)
@@ -804,8 +811,10 @@ class L3_NAT_dbonly_mixin(l3.RouterPluginBase,
             ipv6_utils.is_ipv6_pd_enabled(subnet)):
             # Add new prefix to an existing ipv6 port with the same network id
             # if one exists
+            vlan_id = subnet.get(wrs_net.VLAN, n_const.NONE_VLAN_TAG)
             port = self._find_ipv6_router_port_by_network(context, router,
-                                                          subnet['network_id'])
+                                                          subnet['network_id'],
+                                                          vlan_id)
             if port:
                 fixed_ips = list(map(dict, port['port']['fixed_ips']))
                 fixed_ips.append(fixed_ip)
diff --git a/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/5018b7ad4223_ml2_subnet_segment_dynamic_field.py b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/5018b7ad4223_ml2_subnet_segment_dynamic_field.py
new file mode 100644
index 0000000..f991892
--- /dev/null
+++ b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/5018b7ad4223_ml2_subnet_segment_dynamic_field.py
@@ -0,0 +1,52 @@
+# Copyright 2014 OpenStack Foundation
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+#
+# Copyright (c) 2015 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
+"""ml2 subnet segment new kilo fields
+
+Revision ID: 5018b7ad4223
+Revises: 52eb37bd3d77
+Create Date: 2015-07-02 12:15:51.341278
+
+"""
+
+# revision identifiers, used by Alembic.
+revision = '5018b7ad4223'
+down_revision = '230661bb0d02'
+
+from alembic import op
+import sqlalchemy as sa
+
+
+def upgrade():
+    op.add_column(
+        'ml2_subnet_segments',
+        sa.Column('is_dynamic',
+                  sa.Boolean(), server_default='false', nullable=False))
+    op.add_column(
+        'ml2_subnet_segments',
+        sa.Column('segment_index', sa.Integer(), nullable=False,
+                  server_default='0'))
+
+
+def downgrade():
+    op.drop_column('ml2_subnet_segments', 'is_dynamic')
+    op.drop_column('ml2_subnet_segments', 'segment_index')
diff --git a/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py
index d8ebec4..b6bb8e1 100644
--- a/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py
+++ b/neutron/db/migration/alembic_migrations/versions/wrs_kilo_shipped/expand/wrs_kilo_shipped.py
@@ -22,14 +22,14 @@
 """WRS Kilo Revision placeholder
 
 Revision ID: wrs_kilo_shipped
-Revises: kilo
+Revises: 5018b7ad4223
 Create Date: 2016-05-25 00:00:01.000000
 
 """
 
 # revision identifiers, used by Alembic.
 revision = 'wrs_kilo_shipped'
-down_revision = '230661bb0d02'
+down_revision = '5018b7ad4223'
 
 
 def upgrade():
diff --git a/neutron/db/migration/alembic_migrations/vswitch_init_ops.py b/neutron/db/migration/alembic_migrations/vswitch_init_ops.py
index 1dab67f..34cb0dd 100644
--- a/neutron/db/migration/alembic_migrations/vswitch_init_ops.py
+++ b/neutron/db/migration/alembic_migrations/vswitch_init_ops.py
@@ -147,3 +147,18 @@ def upgrade():
         sa.ForeignKeyConstraint(['port_id'], ['ports.id'], ondelete='CASCADE'),
         sa.ForeignKeyConstraint(['qos_id'], ['qoses.id'], ondelete='CASCADE'),
         sa.PrimaryKeyConstraint('port_id', 'qos_id'))
+
+    op.add_column(
+        'subnets',
+        sa.Column('vlan_id', sa.Integer(), nullable=True))
+
+    op.create_table(
+        'ml2_subnet_segments',
+        sa.Column('id', sa.String(length=36), nullable=False),
+        sa.Column('subnet_id', sa.String(length=36), nullable=False),
+        sa.Column('network_type', sa.String(length=32), nullable=False),
+        sa.Column('physical_network', sa.String(length=64), nullable=True),
+        sa.Column('segmentation_id', sa.Integer(), nullable=True),
+        sa.ForeignKeyConstraint(['subnet_id'], ['subnets.id'],
+                                ondelete='CASCADE'),
+        sa.PrimaryKeyConstraint('id'))
diff --git a/neutron/db/models_v2.py b/neutron/db/models_v2.py
index 6863cc3..ff9f2bd 100644
--- a/neutron/db/models_v2.py
+++ b/neutron/db/models_v2.py
@@ -12,6 +12,14 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
+
 
 from neutron_lib.api.definitions import network as net_def
 from neutron_lib.api.definitions import port as port_def
@@ -166,6 +174,7 @@ class Subnet(standard_attr.HasStandardAttributes, model_base.BASEV2,
         'SubnetPool', lazy='joined',
         foreign_keys='Subnet.subnetpool_id',
         primaryjoin='Subnet.subnetpool_id==SubnetPool.id')
+    vlan_id = sa.Column(sa.Integer, nullable=True)
     ip_version = sa.Column(sa.Integer, nullable=False)
     cidr = sa.Column(sa.String(64), nullable=False)
     gateway_ip = sa.Column(sa.String(64))
diff --git a/neutron/db/segments_db.py b/neutron/db/segments_db.py
index d6aed7f..a29dfed 100644
--- a/neutron/db/segments_db.py
+++ b/neutron/db/segments_db.py
@@ -13,8 +13,10 @@
 from neutron_lib.callbacks import events
 from neutron_lib.callbacks import registry
 from neutron_lib.callbacks import resources
+from neutron_lib.db import model_base
 from oslo_log import log as logging
 from oslo_utils import uuidutils
+import sqlalchemy as sa
 from sqlalchemy import and_
 
 from neutron.db import api as db_api
@@ -32,11 +34,35 @@ NETWORK_ID = segments_model.NetworkSegment.network_id.name
 
 def _make_segment_dict(obj):
     """Make a segment dictionary out of an object."""
+    #NOTE(jrichard) drop change in next rebase.
     return {'id': obj.id,
             NETWORK_TYPE: obj.network_type,
             PHYSICAL_NETWORK: obj.physical_network,
             SEGMENTATION_ID: obj.segmentation_id,
-            NETWORK_ID: obj.network_id}
+            NETWORK_ID: getattr(obj, 'network_id', None)}
+
+
+class SubnetSegment(model_base.BASEV2, model_base.HasId):
+    """Represent persistent state of a subnet segment.
+
+    A subnet segment is a portion of a neutron subnet with a
+    specific physical realization. A neutron subnet can consist of
+    one or more segments.
+    """
+
+    # TODO(alegacy): rename this similar to how the NetworkSegments table was
+    # renamed?
+    __tablename__ = 'ml2_subnet_segments'
+
+    subnet_id = sa.Column(sa.String(36),
+                          sa.ForeignKey('subnets.id', ondelete="CASCADE"),
+                          nullable=False)
+    network_type = sa.Column(sa.String(32), nullable=False)
+    physical_network = sa.Column(sa.String(64))
+    segmentation_id = sa.Column(sa.Integer)
+    is_dynamic = sa.Column(sa.Boolean, default=False, nullable=False,
+                           server_default=sa.sql.false())
+    segment_index = sa.Column(sa.Integer, nullable=False, server_default='0')
 
 
 def add_network_segment(context, network_id, segment, segment_index=0,
@@ -139,3 +165,59 @@ def network_segments_exist(session, network_type, physical_network,
                 segments_model.NetworkSegment.segmentation_id <= maximum_id
             )))
         return bool(query.count() > 0)
+
+
+def add_subnet_segment(session, subnet_id, segment, segment_index=0,
+                       is_dynamic=False):
+    with session.begin(subtransactions=True):
+        record = SubnetSegment(
+            id=uuidutils.generate_uuid(),
+            subnet_id=subnet_id,
+            network_type=segment.get(NETWORK_TYPE),
+            physical_network=segment.get(PHYSICAL_NETWORK),
+            segmentation_id=segment.get(SEGMENTATION_ID),
+            segment_index=segment_index,
+            is_dynamic=is_dynamic
+        )
+        session.add(record)
+    LOG.info("Added segment %(id)s of type %(network_type)s for subnet"
+             " %(subnet_id)s",
+             {'id': record.id,
+              'network_type': record.network_type,
+              'subnet_id': record.subnet_id})
+
+
+def get_subnet_segments(session, subnet_id, filter_dynamic=False):
+    return get_subnets_segments(
+        session, [subnet_id], filter_dynamic)[subnet_id]
+
+
+def get_subnets_segments(session, subnet_ids, filter_dynamic=False):
+    if not subnet_ids:
+        return {}
+    with session.begin(subtransactions=True):
+        query = (session.query(SubnetSegment).
+                 filter(SubnetSegment.subnet_id.in_(subnet_ids)).
+                 order_by(SubnetSegment.segment_index))
+        if filter_dynamic is not None:
+            query = query.filter_by(is_dynamic=filter_dynamic)
+        records = query.all()
+        result = {subnet_id: [] for subnet_id in subnet_ids}
+        for record in records:
+            result[record.subnet_id].append(_make_segment_dict(record))
+        return result
+
+
+def subnet_segments_exist(session, network_type, physical_network,
+                          segment_range=None):
+    with session.begin(subtransactions=True):
+        query = (session.query(SubnetSegment).
+                 filter_by(network_type=network_type,
+                           physical_network=physical_network))
+        if segment_range:
+            minimum_id = segment_range['minimum']
+            maximum_id = segment_range['maximum']
+            query = (query.filter(
+                and_(SubnetSegment.segmentation_id >= minimum_id,
+                     SubnetSegment.segmentation_id <= maximum_id)))
+        return bool(query.count() > 0)
diff --git a/neutron/extensions/wrs_net.py b/neutron/extensions/wrs_net.py
index 08d0692..a232381 100644
--- a/neutron/extensions/wrs_net.py
+++ b/neutron/extensions/wrs_net.py
@@ -27,8 +27,11 @@ from neutron_lib.api import extensions as api_extensions
 
 LOG = logging.getLogger(__name__)
 
+VLAN = 'wrs-net:vlan_id'
+
 HOST = 'wrs-net:host'
 
+
 EXTENDED_ATTRIBUTES_2_0 = {
     'routers': {
         HOST: {'allow_post': False, 'allow_put': False,
diff --git a/neutron/extensions/wrs_provider.py b/neutron/extensions/wrs_provider.py
index 6a6797e..abd5d20 100644
--- a/neutron/extensions/wrs_provider.py
+++ b/neutron/extensions/wrs_provider.py
@@ -61,6 +61,7 @@ validators.add_validator('type:ip_mcast_address', _validate_ip_mcast_address)
 NETWORK_TYPE = 'wrs-provider:network_type'
 PHYSICAL_NETWORK = 'wrs-provider:physical_network'
 SEGMENTATION_ID = 'wrs-provider:segmentation_id'
+ATTRIBUTES = [NETWORK_TYPE, PHYSICAL_NETWORK, SEGMENTATION_ID]
 MTU = 'wrs-provider:mtu'
 
 EXTENDED_ATTRIBUTES_2_0 = {
@@ -307,6 +308,15 @@ class ProviderNetVxlanIdOutOfRange(exc.NeutronException):
                 "%(maximum)s exceeds %(threshold)s")
 
 
+class ProviderNetMustBeSameForSubnetAndNetwork(exc.NeutronException):
+    message = _("Subnet physical network must match the parent "
+                "network physical network")
+
+
+class MultiSubnetProviderSegmentsNotSupported(exc.NeutronException):
+    message = _("Multi-segment provider networks for subnets is not supported")
+
+
 def _raise_if_updates_provider_attributes(attrs):
     """Raise exception if provider attributes are present.
 
diff --git a/neutron/plugins/ml2/db.py b/neutron/plugins/ml2/db.py
index c89c79c..0d814d9 100644
--- a/neutron/plugins/ml2/db.py
+++ b/neutron/plugins/ml2/db.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from debtcollector import removals
 from neutron_lib.api.definitions import portbindings
@@ -24,7 +31,7 @@ from oslo_db import exception as db_exc
 from oslo_log import log
 from oslo_utils import uuidutils
 import six
-from sqlalchemy import or_
+from sqlalchemy import and_, or_
 from sqlalchemy.orm import exc
 
 from neutron._i18n import _
@@ -341,6 +348,18 @@ def is_dhcp_active_on_any_subnet(context, subnet_ids):
                 filter(models_v2.Subnet.id.in_(subnet_ids)).count())
 
 
+def get_subnets_by_port(session, port_id):
+    try:
+        query = (session.query(models_v2.Subnet)
+                 .join(models_v2.Port,
+                       and_((models_v2.Port.network_id ==
+                             models_v2.Subnet.network_id),
+                            models_v2.Port.id == port_id)))
+        return query.all()
+    except exc.NoResultFound:
+        return None
+
+
 def _prevent_segment_delete_with_port_bound(resource, event, trigger,
                                             context, segment,
                                             for_net_delete=False):
diff --git a/neutron/plugins/ml2/driver_context.py b/neutron/plugins/ml2/driver_context.py
index 34df36d..5f38fb3 100644
--- a/neutron/plugins/ml2/driver_context.py
+++ b/neutron/plugins/ml2/driver_context.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2014 Wind River Systems, Inc.
+#
+# The right to copy, distribute, modify, or otherwise make use
+# of this software may be licensed only pursuant to the terms
+# of an applicable Wind River license agreement.
+#
 
 from neutron_lib.api.definitions import portbindings
 from neutron_lib import constants
@@ -21,6 +28,7 @@ from oslo_serialization import jsonutils
 import sqlalchemy
 
 from neutron.db import segments_db
+from neutron.plugins.ml2 import db
 from neutron.plugins.ml2 import driver_api as api
 
 LOG = log.getLogger(__name__)
@@ -61,6 +69,10 @@ class MechanismDriverContext(object):
         # method call of the plugin.
         self._plugin_context = plugin_context
 
+    @property
+    def context(self):
+        return self._plugin_context
+
 
 class NetworkContext(MechanismDriverContext, api.NetworkContext):
 
@@ -94,6 +106,8 @@ class SubnetContext(MechanismDriverContext, api.SubnetContext):
         self._original_subnet = original_subnet
         self._network_context = NetworkContext(plugin, plugin_context,
                                                network) if network else None
+        self._segments = segments_db.get_subnet_segments(
+            plugin_context.session, subnet['id'])
 
     @property
     def current(self):
@@ -112,6 +126,10 @@ class SubnetContext(MechanismDriverContext, api.SubnetContext):
                 self._plugin, self._plugin_context, network)
         return self._network_context
 
+    @property
+    def subnet_segments(self):
+        return self._segments
+
 
 class PortContext(MechanismDriverContext, api.PortContext):
 
@@ -313,3 +331,22 @@ class PortContext(MechanismDriverContext, api.PortContext):
     def release_dynamic_segment(self, segment_id):
         return self._plugin.type_manager.release_dynamic_segment(
                 self._plugin_context, segment_id)
+
+    def subnets(self):
+        return db.get_subnets_by_port(self._plugin_context.session,
+                                      self._port['id'])
+
+    def subnet_segments(self):
+        subnets = db.get_subnets_by_port(self._plugin_context.session,
+                                         self._port['id'])
+        results = []
+        for subnet in subnets:
+            segments = segments_db.get_subnet_segments(
+                self._plugin_context.session, subnet['id'])
+            for segment in segments or []:
+                results.append({
+                        'subnet': subnet,
+                        'network_type': segment[api.NETWORK_TYPE],
+                        'physical_network': segment[api.PHYSICAL_NETWORK],
+                        'segmentation_id': segment[api.SEGMENTATION_ID]})
+        return results
diff --git a/neutron/plugins/ml2/managers.py b/neutron/plugins/ml2/managers.py
index bcd2ab9..61b2970 100644
--- a/neutron/plugins/ml2/managers.py
+++ b/neutron/plugins/ml2/managers.py
@@ -37,6 +37,8 @@ from neutron.db import segments_db
 from neutron.extensions import external_net
 from neutron.extensions import multiprovidernet as mpnet
 from neutron.extensions import vlantransparent
+from neutron.extensions import wrs_net
+from neutron.extensions import wrs_provider
 from neutron.plugins.ml2.common import exceptions as ml2_exc
 from neutron.plugins.ml2 import driver_api as api
 from neutron.plugins.ml2 import models
@@ -189,6 +191,32 @@ class TypeManager(stevedore.named.NamedExtensionManager):
             network[provider.SEGMENTATION_ID] = segment[
                 ml2_api.SEGMENTATION_ID]
 
+    def extend_subnets_dict_provider(self, context, subnets):
+        ids = [subnet['id'] for subnet in subnets]
+        session = context.session
+        subnet_segments = segments_db.get_subnets_segments(session, ids)
+        for subnet in subnets:
+            segments = subnet_segments[subnet['id']]
+            self._extend_subnet_dict_provider(subnet, segments)
+
+    def _extend_subnet_dict_provider(self, subnet, segments):
+        if not segments:
+            LOG.error("Subnet %s has no segments", id)
+            for attr in wrs_provider.ATTRIBUTES:
+                subnet[attr] = None
+        elif len(segments) > 1:
+            subnet[mpnet.SEGMENTS] = [
+                {wrs_provider.NETWORK_TYPE: segment[api.NETWORK_TYPE],
+                 wrs_provider.PHYSICAL_NETWORK: segment[api.PHYSICAL_NETWORK],
+                 wrs_provider.SEGMENTATION_ID: segment[api.SEGMENTATION_ID]}
+                for segment in segments]
+        else:
+            segment = segments[0]
+            subnet[wrs_provider.NETWORK_TYPE] = segment[api.NETWORK_TYPE]
+            subnet[wrs_provider.PHYSICAL_NETWORK] = \
+                segment[api.PHYSICAL_NETWORK]
+            subnet[wrs_provider.SEGMENTATION_ID] = segment[api.SEGMENTATION_ID]
+
     def initialize(self):
         for network_type, driver in self.drivers.items():
             LOG.info("Initializing driver for type '%s'", network_type)
@@ -240,6 +268,75 @@ class TypeManager(stevedore.named.NamedExtensionManager):
             # TODO(alegacy): tenant_id filters
             return self.reserve_provider_segment(context, segment)
 
+    def _process_subnet_provider_create(self, context, subnet):
+        if validators.is_attr_set(subnet.get(mpnet.SEGMENTS)):
+            raise wrs_provider.MultiSubnetProviderSegmentsNotSupported()
+        if any(validators.is_attr_set(subnet.get(attr))
+               for attr in wrs_provider.ATTRIBUTES):
+            segment = {
+                api.NETWORK_TYPE: subnet[wrs_provider.NETWORK_TYPE],
+                api.PHYSICAL_NETWORK: subnet[wrs_provider.PHYSICAL_NETWORK],
+                api.SEGMENTATION_ID: subnet[wrs_provider.SEGMENTATION_ID]}
+            return segment
+        return {}
+
+    def _check_requested_subnet_providernet(self, segment, requested_segment):
+        physical_network = requested_segment.get(api.PHYSICAL_NETWORK)
+        if (physical_network and
+            (physical_network != segment[api.PHYSICAL_NETWORK])):
+            raise wrs_provider.ProviderNetMustBeSameForSubnetAndNetwork()
+
+    def create_subnet_segments(self, context, subnet, peer_subnets, tenant_id):
+        """Call type drivers to create subnet segments."""
+        requested_segment = self._process_subnet_provider_create(
+            context, subnet)
+        network_id = subnet['network_id']
+        vlan_id = subnet[wrs_net.VLAN]
+        session = context.session
+        if vlan_id:
+            if len(peer_subnets) > 0:
+                vlan_subnet_id = peer_subnets[0]['id']
+                vlan_segments = segments_db.get_subnet_segments(session,
+                                                                vlan_subnet_id)
+            else:
+                vlan_segments = []
+
+            if len(vlan_segments) > 0:
+                # There is at least one existing subnet on this vlan,
+                # therefore use the same network provider segments for
+                # additional subnets
+                for segment_index, segment in enumerate(vlan_segments):
+                    segments_db.add_subnet_segment(session, subnet['id'],
+                                                   segment, segment_index)
+            else:
+                # Allocate a new segmentation id from the parent
+                # network's provider network.  This has to be the same in
+                # order to guarantee that the provider network of the
+                # subnet is also hosted on the same compute nodes as the
+                # parent network.  It is also likely to be more
+                # convenient for the operator to have all of the
+                # network's subnets on the same provider network.
+                segments = segments_db.get_network_segments(
+                    context, network_id)
+                for segment_index, segment in enumerate(segments):
+                    self._check_requested_subnet_providernet(
+                        segment, requested_segment)
+                    segment[api.SEGMENTATION_ID] = (
+                        requested_segment.get(api.SEGMENTATION_ID, None))
+                    filters = {'tenant_id': tenant_id}
+                    segment = self.reserve_provider_segment(
+                        context, segment, **filters)
+                    segments_db.add_subnet_segment(session, subnet['id'],
+                                                   segment, segment_index)
+        else:
+            # subnet has no vlan, use the primary network attachment (see
+            # note above about co-locating the subnet and network on the
+            # same provider network).
+            segments = segments_db.get_network_segments(context, network_id)
+            for segment_index, segment in enumerate(segments):
+                segments_db.add_subnet_segment(session, subnet['id'], segment,
+                                               segment_index)
+
     def is_partial_segment(self, segment):
         network_type = segment[ml2_api.NETWORK_TYPE]
         driver = self.drivers.get(network_type)
@@ -309,6 +406,19 @@ class TypeManager(stevedore.named.NamedExtensionManager):
             LOG.error("Failed to release segment '%s' because "
                       "network type is not supported.", segment)
 
+    def release_subnet_segments(self, session, subnet_id):
+        segments = segments_db.get_subnet_segments(session, subnet_id,
+                                                   filter_dynamic=None)
+
+        for segment in segments:
+            network_type = segment.get(api.NETWORK_TYPE)
+            driver = self.drivers.get(network_type)
+            if driver:
+                driver.obj.release_segment(session, segment)
+            else:
+                LOG.error("Failed to release segment '%s' because "
+                          "network type is not supported.", segment)
+
     def allocate_dynamic_segment(self, context, network_id, segment):
         """Allocate a dynamic segment using a partial or full segment dict."""
         dynamic_segment = segments_db.get_dynamic_segment(
@@ -355,6 +465,11 @@ class TypeManager(stevedore.named.NamedExtensionManager):
         return segments_db.network_segments_exist(
             context.session, network_type, physical_network, segment_range)
 
+    def subnet_segments_exist(self, context, network_type, physical_network,
+                          segment_range=None):
+        return segments_db.subnet_segments_exist(
+            context.session, network_type, physical_network, segment_range)
+
 
 class MechanismManager(stevedore.named.NamedExtensionManager):
     """Manage networking mechanisms using drivers."""
diff --git a/neutron/plugins/ml2/plugin.py b/neutron/plugins/ml2/plugin.py
index cacb603..32209cb 100644
--- a/neutron/plugins/ml2/plugin.py
+++ b/neutron/plugins/ml2/plugin.py
@@ -20,6 +20,7 @@
 # of an applicable Wind River license agreement.
 #
 
+import copy
 import re
 
 from sqlalchemy import and_
@@ -98,6 +99,7 @@ from neutron.extensions import netmtu_writable as mtu_ext
 from neutron.extensions import providernet as provider
 from neutron.extensions import vlantransparent
 from neutron.extensions import wrs_binding
+from neutron.extensions import wrs_net
 from neutron.extensions import wrs_provider
 from neutron.plugins.common import utils as p_utils
 from neutron.plugins.ml2.common import exceptions as ml2_exc
@@ -178,7 +180,7 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
                                     "default-subnetpools",
                                     "subnet-service-types",
                                     "host", "wrs-provider", "wrs-tenant",
-                                    "wrs-binding", "wrs-tm"]
+                                    "wrs-binding", "wrs-tm", "wrs-net"]
 
     @property
     def supported_extension_aliases(self):
@@ -747,6 +749,27 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
                                   segment[api.SEGMENTATION_ID],
                                   segment[api.PHYSICAL_NETWORK])
 
+    def wrs_update_subnet(self, subnet):
+        subnet = copy.deepcopy(subnet)
+        if wrs_net.VLAN in subnet:
+            subnet["vlan_id"] = subnet[wrs_net.VLAN]
+        return subnet
+
+    def _notify_subnet_created(self, mech_context):
+        subnet = mech_context.current
+        subnet = self.wrs_update_subnet(subnet)
+        # currently a subnet only supports one bound segment
+        segment = mech_context.subnet_segments[0]
+        self.notifier.subnet_create(mech_context.context, subnet,
+                                    segment[api.NETWORK_TYPE],
+                                    segment[api.SEGMENTATION_ID],
+                                    segment[api.PHYSICAL_NETWORK])
+
+    def _notify_subnet_deleted(self, mech_context):
+        subnet = mech_context.current
+        subnet = self.wrs_update_subnet(subnet)
+        self.notifier.subnet_delete(mech_context.context, subnet)
+
     def _delete_objects(self, context, resource, objects):
         delete_op = getattr(self, 'delete_%s' % resource)
         for obj in objects:
@@ -1096,8 +1119,23 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
         # TODO(kevinbenton): BEFORE notification should be added here
         pass
 
+    def _get_other_subnets_on_vlan(self, context, subnet):
+        """Find all other subnets that share the same network_id and vlan_id.
+        """
+        network_id = subnet['network_id']
+        if not validators.is_attr_set(subnet.get(wrs_net.VLAN)):
+            vlan_id = n_const.NONE_VLAN_TAG
+        else:
+            vlan_id = subnet[wrs_net.VLAN]
+        filters = dict(network_id=[network_id], vlan_id=[vlan_id])
+        return self.get_subnets(context, filters=filters)
+
     def _create_subnet_db(self, context, subnet):
+        subnet_data = subnet[subnet_def.RESOURCE_NAME]
+        tenant_id = subnet_data['tenant_id']
         with db_api.context_manager.writer.using(context):
+            peer_subnets = self._get_other_subnets_on_vlan(context,
+                                                           subnet_data)
             result, net_db, ipam_sub = self._create_subnet_precommit(
                 context, subnet)
 
@@ -1109,6 +1147,13 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
 
             self.extension_manager.process_create_subnet(
                 context, subnet[subnet_def.RESOURCE_NAME], result)
+
+            # NOTE(jrichard) drop these changes next rebase
+            subnet_data['id'] = result['id']
+            self.type_manager.create_subnet_segments(context, subnet_data,
+                                                     peer_subnets, tenant_id)
+            self.type_manager.extend_subnets_dict_provider(context, [result])
+
             network = self._make_network_dict(net_db, context=context)
             self.type_manager.extend_network_dict_provider(context, network)
             mech_context = driver_context.SubnetContext(self, context,
@@ -1138,6 +1183,7 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
                 LOG.error("mechanism_manager.create_subnet_postcommit "
                           "failed, deleting subnet '%s'", result['id'])
                 self.delete_subnet(context, result['id'])
+        self._notify_subnet_created(mech_context)
         return result
 
     @utils.transaction_guard
@@ -1155,6 +1201,9 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
                 context, id, subnet)
             self.extension_manager.process_update_subnet(
                 context, subnet[subnet_def.RESOURCE_NAME], updated_subnet)
+            # NOTE(jrichard) drop this change next rebase
+            self.type_manager.extend_subnets_dict_provider(context,
+                                                           [updated_subnet])
             updated_subnet = self.get_subnet(context, id)
             mech_context = driver_context.SubnetContext(
                 self, context, updated_subnet, network=None,
@@ -1170,6 +1219,25 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
         self.mechanism_manager.update_subnet_postcommit(mech_context)
         return updated_subnet
 
+    def get_subnet(self, context, id, fields=None):
+        session = context.session
+        with session.begin(subtransactions=True):
+            result = super(Ml2Plugin, self).get_subnet(context, id, None)
+            self.type_manager.extend_subnets_dict_provider(context, [result])
+
+        return self._fields(result, fields)
+
+    def get_subnets(self, context, filters=None, fields=None,
+                    sorts=None, limit=None, marker=None, page_reverse=False):
+        session = context.session
+        with session.begin(subtransactions=True):
+            nets = super(Ml2Plugin,
+                         self).get_subnets(context, filters, None, sorts,
+                                           limit, marker, page_reverse)
+            self.type_manager.extend_subnets_dict_provider(context, nets)
+
+        return [self._fields(net, fields) for net in nets]
+
     @utils.transaction_guard
     def delete_subnet(self, context, id):
         # the only purpose of this override is to protect this from being
@@ -1189,6 +1257,17 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
         setattr(context, '_mech_context', mech_context)
         self.mechanism_manager.delete_subnet_precommit(mech_context)
 
+        vlan_id = subnet.get(wrs_net.VLAN)
+        if vlan_id:
+            network_id = subnet.get("network_id")
+            filters = dict(network_id=[network_id], vlan_id=[vlan_id])
+            subnets = self.get_subnets(context, filters=filters)
+            if len(subnets) == 1:
+                # If this is the last subnet in this vlan then release the
+                # segment that is associated to this subnet.
+                self.type_manager.release_subnet_segments(
+                    context.session, subnet['id'])
+
     @registry.receives(resources.SUBNET, [events.AFTER_DELETE])
     def _subnet_delete_after_delete_handler(self, rtype, event, trigger,
                                             context, subnet, **kwargs):
@@ -1200,6 +1279,7 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
             # delete the subnet.  Ideally we'd notify the caller of
             # the fact that an error occurred.
             LOG.error("mechanism_manager.delete_subnet_postcommit failed")
+        self._notify_subnet_deleted(context._mech_context)
 
     # TODO(yalei) - will be simplified after security group and address pair be
     # converted to ext driver too.
@@ -2027,8 +2107,12 @@ class Ml2Plugin(db_base_plugin_v2.NeutronDbPluginV2,
 
     def _is_providernet_referenced(self, context, providernet,
                                    segment_range=None):
-        return self.type_manager.network_segments_exist(
-            context, providernet['type'], providernet['name'], segment_range)
+        return (self.type_manager.network_segments_exist(
+            context, providernet['type'], providernet['name'],
+            segment_range) or
+                self.type_manager.subnet_segments_exist(
+            context, providernet['type'], providernet['name'],
+            segment_range))
 
     def _is_providernet_type_supported(self, network_type):
         if not self.type_manager.network_type_supported(network_type):
diff --git a/neutron/plugins/ml2/rpc.py b/neutron/plugins/ml2/rpc.py
index 9cc0845..7344c7d 100644
--- a/neutron/plugins/ml2/rpc.py
+++ b/neutron/plugins/ml2/rpc.py
@@ -171,6 +171,9 @@ class RpcCallbacks(type_tunnel.TunnelRpcCallbackMixin):
                          'vif_type': port_context.vif_type})
             return {'device': device}
 
+        # obtain WRS subnet bindings
+        subnets = port_context.subnet_segments()
+
         # obtain WRS qos policies
         plugin = directory.get_plugin()
         port_qos_id = plugin.get_qos_by_port(rpc_context, port['id'])
@@ -201,6 +204,7 @@ class RpcCallbacks(type_tunnel.TunnelRpcCallbackMixin):
                  'segmentation_id': segment[api.SEGMENTATION_ID],
                  'physical_network': segment[api.PHYSICAL_NETWORK],
                  'mtu': port_context.network._network.get('mtu'),
+                 'subnets': subnets,
                  'port_qos_policy': port_qos_policy,
                  'network_qos_policy': network_qos_policy,
                  'fixed_ips': port['fixed_ips'],
@@ -463,6 +467,12 @@ class AgentNotifierApi(dvr_rpc.DVRAgentRpcApiMixin,
         self.topic_network_update = topics.get_topic_name(topic,
                                                           topics.NETWORK,
                                                           topics.UPDATE)
+        self.topic_subnet_create = topics.get_topic_name(topic,
+                                                         topics.SUBNET,
+                                                         topics.CREATE)
+        self.topic_subnet_delete = topics.get_topic_name(topic,
+                                                         topics.SUBNET,
+                                                         topics.DELETE)
 
         target = oslo_messaging.Target(topic=topic, version='1.0')
         self.client = n_rpc.get_client(target)
@@ -489,3 +499,16 @@ class AgentNotifierApi(dvr_rpc.DVRAgentRpcApiMixin,
         cctxt = self.client.prepare(topic=self.topic_network_update,
                                     fanout=True, version='1.4')
         cctxt.cast(context, 'network_update', network=network)
+
+    def subnet_create(self, context, subnet, network_type, segmentation_id,
+                      physical_network):
+        cctxt = self.client.prepare(topic=self.topic_subnet_create,
+                                    fanout=True)
+        cctxt.cast(context, 'subnet_create', subnet=subnet,
+                   network_type=network_type, segmentation_id=segmentation_id,
+                   physical_network=physical_network)
+
+    def subnet_delete(self, context, subnet):
+        cctxt = self.client.prepare(topic=self.topic_subnet_delete,
+                                    fanout=True)
+        cctxt.cast(context, 'subnet_delete', subnet=subnet)
diff --git a/neutron/tests/etc/policy.json b/neutron/tests/etc/policy.json
index 9e1caf9..8c36949 100644
--- a/neutron/tests/etc/policy.json
+++ b/neutron/tests/etc/policy.json
@@ -18,10 +18,15 @@
     "create_subnet": "rule:admin_or_network_owner",
     "create_subnet:segment_id": "rule:admin_only",
     "create_subnet:service_types": "rule:admin_only",
+    "create_subnet:wrs-provider:segmentation_id": "rule:admin_only",
     "get_subnet": "rule:admin_or_owner or rule:shared",
     "get_subnet:segment_id": "rule:admin_only",
+    "get_subnet:wrs-provider:network_type": "rule:admin_only",
+    "get_subnet:wrs-provider:physical_network": "rule:admin_only",
+    "get_subnet:wrs-provider:segmentation_id": "rule:admin_only",
     "update_subnet": "rule:admin_or_network_owner",
     "update_subnet:service_types": "rule:admin_only",
+    "update_subnet:wrs-provider:segmentation_id": "rule:admin_only",
     "delete_subnet": "rule:admin_or_network_owner",
 
     "create_subnetpool": "",
diff --git a/neutron/tests/unit/agent/dhcp/test_agent.py b/neutron/tests/unit/agent/dhcp/test_agent.py
index e4d197c..428da95 100644
--- a/neutron/tests/unit/agent/dhcp/test_agent.py
+++ b/neutron/tests/unit/agent/dhcp/test_agent.py
@@ -102,7 +102,7 @@ fake_allocation_pool_subnet1 = dhcp.DictModel(dict(id='', start='172.9.9.2',
 
 fake_port1 = dhcp.DictModel(dict(id='12345678-1234-aaaa-1234567890ab',
                             device_id='dhcp-12345678-1234-aaaa-1234567890ab',
-                            device_owner='',
+                            device_owner=const.DEVICE_OWNER_DHCP,
                             allocation_pools=fake_subnet1_allocation_pools,
                             mac_address='aa:bb:cc:dd:ee:ff',
                             network_id=FAKE_NETWORK_UUID,
@@ -287,7 +287,7 @@ class TestDhcpAgent(base.BaseTestCase):
                 mocks['start_ready_ports_loop'].assert_called_once_with()
 
     def test_call_driver(self):
-        network = mock.Mock()
+        network = copy.deepcopy(fake_network)
         network.id = '1'
         dhcp = dhcp_agent.DhcpAgent(cfg.CONF)
         self.assertTrue(dhcp.call_driver('foo', network))
@@ -299,7 +299,7 @@ class TestDhcpAgent(base.BaseTestCase):
 
     def _test_call_driver_failure(self, exc=None,
                                   trace_level='exception', expected_sync=True):
-        network = mock.Mock()
+        network = copy.deepcopy(fake_network)
         network.id = '1'
         self.driver.return_value.foo.side_effect = exc or Exception
         dhcp = dhcp_agent.DhcpAgent(HOSTNAME)
@@ -511,17 +511,25 @@ class TestDhcpAgent(base.BaseTestCase):
             self.assertFalse(dhcp.cache.get_network_ids())
             self.assertTrue(log.called)
 
+    def _get_network_info(self, network_id):
+        return dhcp.NetModel(dict(id=network_id, subnets=[], ports=[]))
+
     def test_populate_cache_on_start(self):
         networks = ['aaa', 'bbb']
         self.driver.existing_dhcp_networks.return_value = networks
 
-        dhcp = dhcp_agent.DhcpAgent(HOSTNAME)
+        with mock.patch(DHCP_PLUGIN) as plug:
+            mock_plugin = mock.Mock()
+            mock_plugin.get_network_info.side_effect = self._get_network_info
+            plug.return_value = mock_plugin
 
-        self.driver.existing_dhcp_networks.assert_called_once_with(
-            dhcp.conf,
-        )
+            dhcp = dhcp_agent.DhcpAgent(HOSTNAME)
+
+            self.driver.existing_dhcp_networks.assert_called_once_with(
+                dhcp.conf,
+            )
 
-        self.assertEqual(set(networks), set(dhcp.cache.get_network_ids()))
+            self.assertEqual(set(networks), set(dhcp.cache.get_network_ids()))
 
     def test_none_interface_driver(self):
         cfg.CONF.set_override('interface_driver', None)
@@ -1016,6 +1024,7 @@ class TestDhcpAgentEventHandler(base.BaseTestCase):
         self.dhcp.port_update_end(None, payload)
         self.cache.assert_has_calls(
             [mock.call.get_network_by_id(fake_port2.network_id),
+             mock.call.get_port_by_id(fake_port2.id),
              mock.call.put_port(mock.ANY)])
         self.call_driver.assert_called_once_with('reload_allocations',
                                                  fake_network)
@@ -1037,6 +1046,7 @@ class TestDhcpAgentEventHandler(base.BaseTestCase):
         self.dhcp.port_update_end(None, payload)
         self.cache.assert_has_calls(
             [mock.call.get_network_by_id(fake_port1.network_id),
+             mock.call.get_port_by_id(fake_port1.id),
              mock.call.put_port(mock.ANY)])
         self.call_driver.assert_has_calls(
             [mock.call.call_driver('reload_allocations', fake_network)])
@@ -1487,14 +1497,17 @@ class TestDeviceManager(base.BaseTestCase):
                           'device_id': mock.ANY}})])
 
         if port == fake_ipv6_port:
-            expected_ips = ['2001:db8::a8bb:ccff:fedd:ee99/64',
-                            '169.254.169.254/16']
+            expected_ips = ['2001:db8::a8bb:ccff:fedd:ee99/64']
         else:
-            expected_ips = ['172.9.9.9/24', '169.254.169.254/16']
+            expected_ips = ['172.9.9.9/24']
         expected = [
             mock.call.get_device_name(port),
             mock.call.configure_ipv6_ra(net.namespace, 'default', 0),
             mock.call.init_l3(
+                'lo',
+                ['169.254.169.254/32'],
+                namespace=net.namespace),
+            mock.call.init_l3(
                 'tap12345678-12',
                 expected_ips,
                 namespace=net.namespace)]
@@ -1628,6 +1641,7 @@ class TestDeviceManager(base.BaseTestCase):
         dh.setup_dhcp_port(fake_network_copy)
         port_body = {'port': {
                      'network_id': fake_network.id,
+                     'device_id': mock.ANY,
                      'fixed_ips': [{'subnet_id': fake_fixed_ip1.subnet_id,
                                     'ip_address': fake_fixed_ip1.ip_address},
                                    {'subnet_id': fake_subnet2.id}]}}
diff --git a/neutron/tests/unit/agent/linux/test_dhcp.py b/neutron/tests/unit/agent/linux/test_dhcp.py
index d35ed95..f823ac3 100644
--- a/neutron/tests/unit/agent/linux/test_dhcp.py
+++ b/neutron/tests/unit/agent/linux/test_dhcp.py
@@ -1158,6 +1158,7 @@ class TestDnsmasq(TestBase):
             '--dhcp-optsfile=/dhcp/%s/opts' % network.id,
             '--dhcp-leasefile=/dhcp/%s/leases' % network.id,
             '--dhcp-match=set:ipxe,175',
+            '--dhcp-authoritative',
             '--bind-interfaces',
             '--interface=tap0',
         ]
@@ -2156,20 +2157,30 @@ class TestDnsmasq(TestBase):
             'not_uuid_like_name': True
         }
 
+        def read_value_from_conf_file_fake(file_name, kind,
+                                           converter=None):
+            dirname = os.path.dirname(file_name)
+            return os.path.basename(dirname)
+
         def active_fake(self, instance, cls):
             return cases[instance.network.id]
 
         with mock.patch('os.listdir') as mock_listdir:
             with mock.patch.object(dhcp.Dnsmasq, 'active') as mock_active:
-                mock_active.__get__ = active_fake
-                mock_listdir.return_value = list(cases)
+                with mock.patch.object(
+                    dhcp.DhcpLocalProcess,
+                    '_read_value_from_conf_file') as mock_read:
+                    mock_active.__get__ = active_fake
+                    mock_listdir.return_value = list(cases)
+                    mock_read.side_effect = read_value_from_conf_file_fake
 
-                result = dhcp.Dnsmasq.existing_dhcp_networks(self.conf)
+                    result = dhcp.Dnsmasq.existing_dhcp_networks(self.conf)
 
-                mock_listdir.assert_called_once_with(path)
-                self.assertItemsEqual(['aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa',
-                                       'bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb'],
-                                      result)
+                    mock_listdir.assert_called_once_with(path)
+                    self.assertItemsEqual(
+                        ['aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa',
+                         'bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb'],
+                        result)
 
     def test__output_hosts_file_log_only_twice(self):
         dm = self._get_dnsmasq(FakeDualStackNetworkSingleDHCP())
@@ -2468,11 +2479,21 @@ class TestDeviceManager(TestConfBase):
                                                        mock.ANY)
 
             expect_ips = ['192.168.0.6/24', 'fdca:3ba5:a17a:4ba3::2/64']
+            expected_calls = [mock.call.get_device_name(mock.ANY)]
+            expected_calls.append(mock.call.configure_ipv6_ra(
+                'qdhcp-ns',
+                'default',
+                n_const.ACCEPT_RA_DISABLED))
             if enable_isolated_metadata or force_metadata:
-                expect_ips.append(dhcp.METADATA_DEFAULT_CIDR)
-            mgr.driver.init_l3.assert_called_with('ns-XXX',
-                                                  expect_ips,
-                                                  namespace='qdhcp-ns')
+                expected_calls.append(mock.call.init_l3(
+                    'lo',
+                    ['%s/32' % dhcp.METADATA_DEFAULT_IP],
+                    namespace='qdhcp-ns'))
+            expected_calls.append(mock.call.init_l3(
+                'ns-XXX',
+                expect_ips,
+                namespace='qdhcp-ns'))
+            mgr.driver.assert_has_calls(expected_calls)
 
     def test_setup_reserved_and_disable_metadata(self):
         """Test reserved port case of DeviceManager's DHCP port setup
diff --git a/neutron/tests/unit/api/rpc/handlers/test_dhcp_rpc.py b/neutron/tests/unit/api/rpc/handlers/test_dhcp_rpc.py
index 9345c52..fc5a138 100644
--- a/neutron/tests/unit/api/rpc/handlers/test_dhcp_rpc.py
+++ b/neutron/tests/unit/api/rpc/handlers/test_dhcp_rpc.py
@@ -259,11 +259,15 @@ class TestDhcpRpcCallback(base.BaseTestCase):
                                         port=port)
 
     def test_update_reserved_dhcp_port(self):
+        device_id = utils.get_dhcp_agent_device_id('foo_network_id',
+                                                   'foo_host')
         port = {'port': {'network_id': 'foo_network_id',
+                         'device_id': device_id,
                          'device_owner': constants.DEVICE_OWNER_DHCP,
                          'fixed_ips': [{'subnet_id': 'foo_subnet_id'}]}
                 }
         expected_port = {'port': {'network_id': 'foo_network_id',
+                                  'device_id': device_id,
                                   'device_owner': constants.DEVICE_OWNER_DHCP,
                                   portbindings.HOST_ID: 'foo_host',
                                   'fixed_ips': [{'subnet_id': 'foo_subnet_id'}]
@@ -274,9 +278,7 @@ class TestDhcpRpcCallback(base.BaseTestCase):
         def _fake_port_action(plugin, context, port, action):
             self.assertEqual(expected_port, port)
 
-        self.plugin.get_port.return_value = {
-            'device_id': utils.get_dhcp_agent_device_id('foo_network_id',
-                                                        'foo_host')}
+        self.plugin.get_port.return_value = {'device_id': device_id}
         self.callbacks._port_action = _fake_port_action
         self.callbacks.update_dhcp_port(
             mock.Mock(), host='foo_host', port_id='foo_port_id', port=port)
diff --git a/neutron/tests/unit/db/test_db_base_plugin_v2.py b/neutron/tests/unit/db/test_db_base_plugin_v2.py
index 9d9f943..7e3bdc8 100644
--- a/neutron/tests/unit/db/test_db_base_plugin_v2.py
+++ b/neutron/tests/unit/db/test_db_base_plugin_v2.py
@@ -326,20 +326,23 @@ class NeutronDbPluginV2TestCase(testlib_api.WebTestCase):
         return self._create_bulk(fmt, number, 'network', base_data, **kwargs)
 
     def _create_subnet(self, fmt, net_id, cidr,
-                       expected_res_status=None, **kwargs):
+                       expected_res_status=None, arg_list=None, **kwargs):
         data = {'subnet': {'network_id': net_id,
                            'ip_version': 4,
                            'tenant_id': self._tenant_id}}
         if cidr:
             data['subnet']['cidr'] = cidr
-        for arg in ('ip_version', 'tenant_id', 'subnetpool_id', 'prefixlen',
-                    'enable_dhcp', 'allocation_pools', 'segment_id',
-                    'dns_nameservers', 'host_routes',
-                    'shared', 'ipv6_ra_mode', 'ipv6_address_mode',
-                    'service_types'):
+        for arg in (('ip_version', 'tenant_id', 'subnetpool_id', 'prefixlen',
+                     'enable_dhcp', 'allocation_pools', 'segment_id',
+                     'dns_nameservers', 'host_routes',
+                     'shared', 'ipv6_ra_mode', 'ipv6_address_mode',
+                     'service_types') +
+                    (arg_list or ())):
             # Arg must be present and not null (but can be false)
             if kwargs.get(arg) is not None:
-                data['subnet'][arg] = kwargs[arg]
+                new_arg = arg.replace('___', '-')
+                new_arg = new_arg.replace('__', ':')
+                data['subnet'][new_arg] = kwargs[arg]
 
         if ('gateway_ip' in kwargs and
             kwargs['gateway_ip'] is not constants.ATTR_NOT_SPECIFIED):
@@ -459,7 +462,8 @@ class NeutronDbPluginV2TestCase(testlib_api.WebTestCase):
                      allocation_pools=None, ip_version=4, enable_dhcp=True,
                      dns_nameservers=None, host_routes=None, shared=None,
                      ipv6_ra_mode=None, ipv6_address_mode=None,
-                     tenant_id=None, set_context=False, segment_id=None):
+                     tenant_id=None, set_context=False, segment_id=None,
+                     **kwargs):
         res = self._create_subnet(fmt,
                                   net_id=network['network']['id'],
                                   cidr=cidr,
@@ -476,7 +480,8 @@ class NeutronDbPluginV2TestCase(testlib_api.WebTestCase):
                                   shared=shared,
                                   ipv6_ra_mode=ipv6_ra_mode,
                                   ipv6_address_mode=ipv6_address_mode,
-                                  set_context=set_context)
+                                  set_context=set_context,
+                                  **kwargs)
         # Things can go wrong - raise HTTP exc with res code only
         # so it can be caught by unit tests
         if res.status_int >= webob.exc.HTTPClientError.code:
@@ -630,7 +635,8 @@ class NeutronDbPluginV2TestCase(testlib_api.WebTestCase):
                ipv6_address_mode=None,
                tenant_id=None,
                service_types=None,
-               set_context=False):
+               set_context=False,
+               **kwargs):
         with optional_ctx(network, self.network,
                           set_context=set_context,
                           tenant_id=tenant_id) as network_to_use:
@@ -649,7 +655,8 @@ class NeutronDbPluginV2TestCase(testlib_api.WebTestCase):
                                        ipv6_ra_mode=ipv6_ra_mode,
                                        ipv6_address_mode=ipv6_address_mode,
                                        tenant_id=tenant_id,
-                                       set_context=set_context)
+                                       set_context=set_context,
+                                       **kwargs)
             yield subnet
 
     @contextlib.contextmanager
@@ -4315,6 +4322,7 @@ class TestSubnetsV2(NeutronDbPluginV2TestCase):
                                               ipv6_address_mode=addr_mode)
             if (insert_db_reference_error or insert_address_allocated
                 or device_owner == constants.DEVICE_OWNER_ROUTER_SNAT
+                or device_owner == constants.DEVICE_OWNER_DHCP
                 or device_owner in constants.ROUTER_INTERFACE_OWNERS):
                 # DVR SNAT, router interfaces and DHCP ports should not have
                 # been updated with addresses from the new auto-address subnet
diff --git a/neutron/tests/unit/plugins/wrs/test_extension_pnet.py b/neutron/tests/unit/plugins/wrs/test_extension_pnet.py
index 8ef1e53..844795a 100644
--- a/neutron/tests/unit/plugins/wrs/test_extension_pnet.py
+++ b/neutron/tests/unit/plugins/wrs/test_extension_pnet.py
@@ -28,6 +28,7 @@ import webob.exc
 from oslo_log import log as logging
 
 from neutron.common import constants as n_const
+from neutron.extensions import wrs_provider as ext_pnet
 from neutron.plugins.ml2 import config
 from neutron.tests.unit.db import test_db_base_plugin_v2
 from neutron.tests.unit.plugins.wrs import test_wrs_plugin
@@ -333,6 +334,27 @@ class ProvidernetTestCase(ProvidernetTestCaseMixin,
                     self.assertIsNotNone(data['provider:segmentation_id'])
                     self.assertEqual(data['mtu'], pnet_data['mtu'])
 
+    def test_create_vlan_tenant_subnet(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1):
+                with self.network() as net:
+                    data = net['network']
+                    self.assertEqual(data['provider:physical_network'],
+                                     VLAN_PNET1['name'])
+                    self.assertEqual(data['provider:network_type'],
+                                     VLAN_PNET1['type'])
+                    self.assertIsNotNone(data['provider:segmentation_id'])
+                    self.assertEqual(data['mtu'], pnet_data['mtu'])
+                    with self.subnet(net, cidr='1.2.3.0/24') as subnet:
+                        sdata = subnet['subnet']
+                        self.assertEqual(sdata[ext_pnet.PHYSICAL_NETWORK],
+                                         data['provider:physical_network'])
+                        self.assertEqual(sdata[ext_pnet.NETWORK_TYPE],
+                                         data['provider:network_type'])
+                        self.assertEqual(sdata[ext_pnet.SEGMENTATION_ID],
+                                         data['provider:segmentation_id'])
+
     def test_create_vxlan_tenant_net(self):
         with self.pnet(VXLAN_PNET1) as pnet:
             pnet_data = pnet['providernet']
@@ -346,6 +368,27 @@ class ProvidernetTestCase(ProvidernetTestCaseMixin,
                     self.assertIsNotNone(data['provider:segmentation_id'])
                     self.assertEqual(data['mtu'], pnet_data['mtu'])
 
+    def test_create_vxlan_tenant_subnet(self):
+        with self.pnet(VXLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VXLAN_PNET1_RANGE1):
+                with self.network() as net:
+                    data = net['network']
+                    self.assertEqual(data['provider:physical_network'],
+                                     VXLAN_PNET1['name'])
+                    self.assertEqual(data['provider:network_type'],
+                                     VXLAN_PNET1['type'])
+                    self.assertIsNotNone(data['provider:segmentation_id'])
+                    self.assertEqual(data['mtu'], pnet_data['mtu'])
+                    with self.subnet(net, cidr='1.2.3.0/24') as subnet:
+                        sdata = subnet['subnet']
+                        self.assertEqual(sdata[ext_pnet.PHYSICAL_NETWORK],
+                                         data['provider:physical_network'])
+                        self.assertEqual(sdata[ext_pnet.NETWORK_TYPE],
+                                         data['provider:network_type'])
+                        self.assertEqual(sdata[ext_pnet.SEGMENTATION_ID],
+                                         data['provider:segmentation_id'])
+
     def test_create_flat_tenant_net(self):
         with self.pnet(FLAT_PNET1) as pnet:
             pnet_data = pnet['providernet']
@@ -361,6 +404,43 @@ class ProvidernetTestCase(ProvidernetTestCaseMixin,
                 self.assertIsNone(data['provider:segmentation_id'])
                 self.assertEqual(data['mtu'], pnet_data['mtu'])
 
+    def test_create_flat_tenant_subnet(self):
+        with self.pnet(FLAT_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.network(arg_list=('provider__physical_network',
+                                        'provider__network_type',),
+                              provider__physical_network=pnet_data['name'],
+                              provider__network_type='flat') as net:
+                data = net['network']
+                self.assertEqual(data['provider:physical_network'],
+                                 FLAT_PNET1['name'])
+                self.assertEqual(data['provider:network_type'],
+                                 FLAT_PNET1['type'])
+                self.assertIsNone(data['provider:segmentation_id'])
+                self.assertEqual(data['mtu'], pnet_data['mtu'])
+                with self.subnet(net, cidr='1.2.3.0/24') as subnet:
+                    sdata = subnet['subnet']
+                    self.assertEqual(sdata[ext_pnet.PHYSICAL_NETWORK],
+                                     data['provider:physical_network'])
+                    self.assertEqual(sdata[ext_pnet.NETWORK_TYPE],
+                                     data['provider:network_type'])
+                    self.assertEqual(sdata[ext_pnet.SEGMENTATION_ID],
+                                     data['provider:segmentation_id'])
+
+    def test_create_vlan_tenant_net_with_out_of_range_vlan(self):
+        with self.pnet(VLAN_PNET1) as pnet:
+            pnet_data = pnet['providernet']
+            with self.pnet_range(pnet_data, VLAN_PNET1_RANGE1):
+                self.assertRaises(webob.exc.HTTPClientError,
+                                  self._make_network,
+                                  self.fmt, 'net1', True,
+                                  arg_list=('provider__physical_network',
+                                            'provider__network_type',
+                                            'provider__segmentation_id'),
+                                  provider__physical_network=pnet_data['name'],
+                                  provider__network_type='vlan',
+                                  provider__segmentation_id='999')
+
     def test_create_vxlan_tenant_net_with_out_of_range_vxlan(self):
         with self.pnet(VXLAN_PNET1) as pnet:
             pnet_data = pnet['providernet']
-- 
2.7.4

